<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kubernetes – Production-Grade Container Orchestration</title>
    <link>https://kubernetes.io/</link>
    <description>The Kubernetes project blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <image>
      <url>https://raw.githubusercontent.com/kubernetes/kubernetes/master/logo/logo.png</url>
      <title>Kubernetes.io</title>
      <link>https://kubernetes.io/</link>
    </image>
    
	<atom:link href="https://kubernetes.io/feed.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Blog: Join SIG Scalability and Learn Kubernetes the Hard Way</title>
      <link>https://kubernetes.io/blog/2020/03/19/join-sig-scalability/</link>
      <pubDate>Thu, 19 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2020/03/19/join-sig-scalability/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Alex Handy&lt;/p&gt;

&lt;p&gt;Contributing to SIG Scalability is a great way to learn Kubernetes in all its depth and breadth, and the team would love to have you &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-scalability#scalability-special-interest-group&#34; target=&#34;_blank&#34;&gt;join as a contributor&lt;/a&gt;. I took a look at the value of learning the hard way and interviewed the current SIG chairs to give you an idea of what contribution feels like.&lt;/p&gt;

&lt;h2 id=&#34;the-value-of-learning-the-hard-way&#34;&gt;The value of Learning The Hard Way&lt;/h2&gt;

&lt;p&gt;There is a belief in the software development community that pushes for the most challenging and rigorous possible method of learning a new language or system. These tend to go by the moniker of &amp;ldquo;Learn __ the Hard Way.&amp;rdquo; Examples abound: Learn Code the Hard Way, Learn Python the Hard Way, and many others originating with Zed Shaw&amp;rsquo;s courses in the topic.&lt;/p&gt;

&lt;p&gt;While there are folks out there who offer you a &amp;ldquo;Learn Kubernetes the Hard Way&amp;rdquo; type experience (most notably &lt;a href=&#34;https://github.com/kelseyhightower/kubernetes-the-hard-way&#34; target=&#34;_blank&#34;&gt;Kelsey Hightower&amp;rsquo;s&lt;/a&gt;), any &amp;ldquo;Hard Way&amp;rdquo; project should attempt to cover every aspect of the core topic&amp;rsquo;s principles.&lt;/p&gt;

&lt;p&gt;Therefore, the real way to &amp;ldquo;Learn Kubernetes the Hard Way,&amp;rdquo; is to join the CNCF and get involved in the project itself. And there is only one SIG that could genuinely offer a full-stack learning experience for Kubernetes: SIG Scalability.&lt;/p&gt;

&lt;p&gt;The team behind SIG Scalability is responsible for detecting and dealing with issues that arise when Kubernetes clusters are working with upwards of a thousand nodes. Said &lt;a href=&#34;https://github.com/wojtek-t&#34; target=&#34;_blank&#34;&gt;Wojiciech Tyczynski&lt;/a&gt;, a staff software engineer at Google and a member of SIG Scalability, the standard size for a test cluster for this SIG is over 5,000 nodes.&lt;/p&gt;

&lt;p&gt;And yet, this SIG is not composed of Ph.D.&amp;rsquo;s in highly scalable systems designs. Many of the folks working with Tyczynski, for example, joined the SIG knowing very little about these types of issues, and often, very little about Kubernetes.&lt;/p&gt;

&lt;p&gt;Working on SIG Scalability is like jumping into the deep end of the pool to learn to swim, and the SIG is inherently concerned with the entire Kubernetes project. SIG Scalability focuses on how Kubernetes functions as a whole and at scale. The SIG Scalability team members have an impetus to learn about every system and to understand how all systems interact with one another.&lt;/p&gt;

&lt;h2 id=&#34;a-complex-and-rewarding-contributor-experience&#34;&gt;A complex and rewarding contributor experience&lt;/h2&gt;

&lt;p&gt;While that may sound complicated (and it is!), that doesn&amp;rsquo;t mean it&amp;rsquo;s outside the reach of an average developer, tester, or administrator. Google software developer Matt Matejczyk has only been on the team since the beginning of 2019, and he&amp;rsquo;s been a valued member of the team since then, ferreting out bugs.&lt;/p&gt;

&lt;p&gt;&amp;ldquo;I am new here,&amp;rdquo; said Matejczyk. &amp;ldquo;I joined the team in January [2019]. Before that, I worked on AdWords at Google in New York. Why did I join? I knew some people there, so that was one of the decisions for me to move. I thought at that time that Kubernetes is a unique, cutting edge technology. I thought it&amp;rsquo;d be cool to work on that.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;Matejczyk was correct about the coolness. &amp;ldquo;It&amp;rsquo;s cool,&amp;rdquo; he said. &amp;ldquo;So actually, ramping up on scalability is not easy. There are many things you need to understand. You need to understand Kubernetes very well. It can use every part of Kubernetes. I am still ramping up after these 8 months. I think it took me maybe 3 months to get up to decent speed.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;When Matejczyk spoke to what he had worked on during those 8 months, he answered, &amp;ldquo;An interesting example is a regression I have been working on recently. We noticed the overall slowness of Kubernetes control plane in specific scenarios, and we couldn&amp;rsquo;t attribute it to any particular component. In the end, we realized that everything boiled down to the memory allocation on the golang level. It was very counterintuitive to have two completely separate pieces of code (running as a part of the same binary) affecting the performance of each other only because one of them was allocating memory too fast. But connecting all the dots and getting to the bottom of regression like this gives great satisfaction.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;Tyczynski said that &amp;ldquo;It&amp;rsquo;s not only debugging regressions, but it&amp;rsquo;s also debugging and finding bottlenecks. In general, those can be regressions, but those can be things we can improve. The other significant area is extending what we want to guarantee to users. Extending SLA and SLO coverage of the system so users can rely on what they can expect from the system in terms of performance and scalability. Matt is doing much work in extending our tests to be more representative and cover more Kubernetes concepts.&amp;rdquo;&lt;/p&gt;

&lt;h2 id=&#34;give-sig-scalability-a-try&#34;&gt;Give SIG Scalability a try&lt;/h2&gt;

&lt;p&gt;The SIG Scalability team is always in need of new members, and if you&amp;rsquo;re the sort of developer or tester who loves taking on new complex challenges, and perhaps loves learning things the hard way, consider joining this SIG. As the team points out, adding Kubernetes expertise to your resume is never a bad idea, and this is the one SIG where you can learn it all from top to bottom.&lt;/p&gt;

&lt;p&gt;See &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-scalability#scalability-special-interest-group&#34; target=&#34;_blank&#34;&gt;the SIG&amp;rsquo;s documentation&lt;/a&gt; to learn about upcoming meetings, its charter, and more. You can also join the &lt;a href=&#34;https://kubernetes.slack.com/archives/C09QZTRH7&#34; target=&#34;_blank&#34;&gt;#sig-scalability Slack channel&lt;/a&gt; to see what it&amp;rsquo;s like. We hope to see you join in to take advantage of this great opportunity to learn Kubernetes and contribute back at the same time.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kong Ingress Controller and Service Mesh: Setting up Ingress to Istio on Kubernetes</title>
      <link>https://kubernetes.io/blog/2020/03/18/kong-ingress-controller-and-istio-service-mesh/</link>
      <pubDate>Wed, 18 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2020/03/18/kong-ingress-controller-and-istio-service-mesh/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author:&lt;/strong&gt; Kevin Chen, Kong&lt;/p&gt;

&lt;p&gt;Kubernetes has become the de facto way to orchestrate containers and the services within services. But how do we give services outside our cluster access to what is within? Kubernetes comes with the Ingress API object that manages external access to services within a cluster.&lt;/p&gt;

&lt;p&gt;Ingress is a group of rules that will proxy inbound connections to endpoints defined by a backend. However, Kubernetes does not know what to do with Ingress resources without an Ingress controller, which is where an open source controller can come into play. In this post, we are going to use one option for this: the Kong Ingress Controller. The Kong Ingress Controller was open-sourced a year ago and recently reached one million downloads. In the recent 0.7 release, service mesh support was also added. Other features of this release include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Built-In Kubernetes Admission Controller&lt;/strong&gt;, which validates Custom Resource Definitions (CRD) as they are created or updated and rejects any invalid configurations.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;In-memory Mode&lt;/strong&gt; - Each pod’s controller actively configures the Kong container in its pod, which limits the blast radius of failure of a single container of Kong or controller container to that pod only.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Native gRPC Routing&lt;/strong&gt; - gRPC traffic can now be routed via Kong Ingress Controller natively with support for method-based routing.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/Kong-Ingress-Controller-and-Service-Mesh/KIC-gRPC.png&#34; alt=&#34;K4K-gRPC&#34; /&gt;&lt;/p&gt;

&lt;p&gt;If you would like a deeper dive into Kong Ingress Controller 0.7, please check out the &lt;a href=&#34;https://github.com/Kong/kubernetes-ingress-controller&#34; target=&#34;_blank&#34;&gt;GitHub repository&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;But let’s get back to the service mesh support since that will be the main focal point of this blog post. Service mesh allows organizations to address microservices challenges related to security, reliability, and observability by abstracting inter-service communication into a mesh layer. But what if our mesh layer sits within Kubernetes and we still need to expose certain services beyond our cluster? Then you need an Ingress controller such as the Kong Ingress Controller. In this blog post, we’ll cover how to deploy Kong Ingress Controller as your Ingress layer to an Istio mesh. Let’s dive right in:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/Kong-Ingress-Controller-and-Service-Mesh/k4k8s.png&#34; alt=&#34;Kong Kubernetes Ingress Controller&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;part-0-set-up-istio-on-kubernetes&#34;&gt;Part 0: Set up Istio on Kubernetes&lt;/h3&gt;

&lt;p&gt;This blog will assume you have Istio set up on Kubernetes. If you need to catch up to this point, please check out the &lt;a href=&#34;https://istio.io/docs/setup/&#34; target=&#34;_blank&#34;&gt;Istio documentation&lt;/a&gt;. It will walk you through setting up Istio on Kubernetes.&lt;/p&gt;

&lt;h3 id=&#34;1-install-the-bookinfo-application&#34;&gt;1. Install the Bookinfo Application&lt;/h3&gt;

&lt;p&gt;First, we need to label the namespaces that will host our application and Kong proxy. To label our default namespace where the bookinfo app sits, run this command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl label namespace default istio-injection=enabled
namespace/default labeled
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then create a new namespace that will be hosting our Kong gateway and the Ingress controller:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create namespace kong
namespace/kong created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Because Kong will be sitting outside the default namespace, be sure you also label the Kong namespace with istio-injection enabled as well:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl label namespace kong istio-injection=enabled
namespace/kong labeled
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Having both namespaces labeled &lt;code&gt;istio-injection=enabled&lt;/code&gt; is necessary. Or else the default configuration will not inject a sidecar container into the pods of your namespaces.&lt;/p&gt;

&lt;p&gt;Now deploy your BookInfo application with the following command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl apply -f http://bit.ly/bookinfoapp
service/details created
serviceaccount/bookinfo-details created
deployment.apps/details-v1 created
service/ratings created
serviceaccount/bookinfo-ratings created
deployment.apps/ratings-v1 created
service/reviews created
serviceaccount/bookinfo-reviews created
deployment.apps/reviews-v1 created
deployment.apps/reviews-v2 created
deployment.apps/reviews-v3 created
service/productpage created
serviceaccount/bookinfo-productpage created
deployment.apps/productpage-v1 created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let’s double-check our Services and Pods to make sure that we have it all set up correctly:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get services
NAME          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
details       ClusterIP   10.97.125.254    &amp;lt;none&amp;gt;        9080/TCP   29s
kubernetes    ClusterIP   10.96.0.1        &amp;lt;none&amp;gt;        443/TCP    29h
productpage   ClusterIP   10.97.62.68      &amp;lt;none&amp;gt;        9080/TCP   28s
ratings       ClusterIP   10.96.15.180     &amp;lt;none&amp;gt;        9080/TCP   28s
reviews       ClusterIP   10.104.207.136   &amp;lt;none&amp;gt;        9080/TCP   28s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should see four new services: details, productpage, ratings, and reviews. None of them have an external IP so we will use the &lt;a href=&#34;https://github.com/Kong/kong&#34; target=&#34;_blank&#34;&gt;Kong gateway&lt;/a&gt; to expose the necessary services. And to check pods, run the following command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pods
NAME                              READY   STATUS    RESTARTS   AGE
details-v1-c5b5f496d-9wm29        2/2     Running   0          101s
productpage-v1-7d6cfb7dfd-5mc96   2/2     Running   0          100s
ratings-v1-f745cf57b-hmkwf        2/2     Running   0          101s
reviews-v1-85c474d9b8-kqcpt       2/2     Running   0          101s
reviews-v2-ccffdd984-9jnsj        2/2     Running   0          101s
reviews-v3-98dc67b68-nzw97        2/2     Running   0          101s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This command outputs useful data, so let’s take a second to understand it. If you examine the READY column, each pod has two containers running: the service and an Envoy sidecar injected alongside it. Another thing to highlight is that there are three review pods but only 1 review service. The Envoy sidecar will load balance the traffic to three different review pods that contain different versions, giving us the ability to A/B test our changes. With that said, you should now be able to access your product page!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl exec -it $(kubectl get pod -l app=ratings -o jsonpath=&#39;{.items[0].metadata.name}&#39;) -c ratings -- curl productpage:9080/productpage | grep -o &amp;quot;&amp;lt;title&amp;gt;.*&amp;lt;/title&amp;gt;&amp;quot;
&amp;lt;title&amp;gt;Simple Bookstore App&amp;lt;/title&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;2-kong-kubernetes-ingress-controller-without-database&#34;&gt;2. Kong Kubernetes Ingress Controller Without Database&lt;/h3&gt;

&lt;p&gt;To expose your services to the world, we will deploy Kong as the north-south traffic gateway. &lt;a href=&#34;https://github.com/Kong/kong/releases/tag/1.1.2&#34; target=&#34;_blank&#34;&gt;Kong 1.1&lt;/a&gt; released with declarative configuration and DB-less mode. Declarative configuration allows you to specify the desired system state through a YAML or JSON file instead of a sequence of API calls. Using declarative config provides several key benefits to reduce complexity, increase automation and enhance system performance. And with the Kong Ingress Controller, any Ingress rules you apply to the cluster will automatically be configured on the Kong proxy. Let’s set up the Kong Ingress Controller and the actual Kong proxy first like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl apply -f https://bit.ly/k4k8s
namespace/kong configured
customresourcedefinition.apiextensions.k8s.io/kongconsumers.configuration.konghq.com created
customresourcedefinition.apiextensions.k8s.io/kongcredentials.configuration.konghq.com created
customresourcedefinition.apiextensions.k8s.io/kongingresses.configuration.konghq.com created
customresourcedefinition.apiextensions.k8s.io/kongplugins.configuration.konghq.com created
serviceaccount/kong-serviceaccount created
clusterrole.rbac.authorization.k8s.io/kong-ingress-clusterrole created
clusterrolebinding.rbac.authorization.k8s.io/kong-ingress-clusterrole-nisa-binding created
configmap/kong-server-blocks created
service/kong-proxy created
service/kong-validation-webhook created
deployment.apps/ingress-kong created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To check if the Kong pod is up and running, run:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pods -n kong
NAME                               READY   STATUS    RESTARTS   AGE
pod/ingress-kong-8b44c9856-9s42v   3/3     Running   0          2m26s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There will be three containers within this pod. The first container is the Kong Gateway that will be the Ingress point to your cluster. The second container is the Ingress controller. It uses Ingress resources and updates the proxy to follow rules defined in the resource. And lastly, the third container is the Envoy proxy injected by Istio. Kong will route traffic through the Envoy sidecar proxy to the appropriate service. To send requests into the cluster via our newly deployed Kong Gateway, setup an environment variable with the a URL based on the IP address at which Kong is accessible.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ export PROXY_URL=&amp;quot;$(minikube service -n kong kong-proxy --url | head -1)&amp;quot;
$ echo $PROXY_URL
http://192.168.99.100:32728
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, we need to change some configuration so that the side-car Envoy process can route the request correctly based on the host/authority header of the request. Run the following to stop the route from preserving host:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ echo &amp;quot;
apiVersion: configuration.konghq.com/v1
kind: KongIngress
metadata:
    name: do-not-preserve-host
route:
  preserve_host: false
&amp;quot; | kubectl apply -f -
kongingress.configuration.konghq.com/do-not-preserve-host created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And annotate the existing productpage service to set service-upstream as true:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl annotate svc productpage Ingress.kubernetes.io/service-upstream=&amp;quot;true&amp;quot;
service/productpage annotated
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now that we have everything set up, we can look at how to use the Ingress resource to help route external traffic to the services within your Istio mesh. We’ll create an Ingress rule that routes all traffic with the path of &lt;code&gt;/&lt;/code&gt; to our productpage service:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ echo &amp;quot;
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: productpage
  annotations:
    configuration.konghq.com: do-not-preserve-host
spec:
  rules:
  - http:
      paths:
      - path: /
        backend:
          serviceName: productpage
          servicePort: 9080
&amp;quot; | kubectl apply -f -
ingress.extensions/productpage created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And just like that, the Kong Ingress Controller is able to understand the rules you defined in the Ingress resource and routes it to the productpage service! To view the product page service’s GUI, go to &lt;code&gt;$PROXY_URL/productpage&lt;/code&gt; in your browser. Or to test it in your command line, try:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl $PROXY_URL/productpage
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That is all I have for this walk-through. If you enjoyed the technologies used in this post, please check out their repositories since they are all open source and would love to have more contributors! Here are their links for your convenience:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Kong: [&lt;a href=&#34;https://github.com/Kong/kubernetes-ingress-controller&#34; target=&#34;_blank&#34;&gt;GitHub&lt;/a&gt;] [&lt;a href=&#34;https://twitter.com/thekonginc&#34; target=&#34;_blank&#34;&gt;Twitter&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Kubernetes: [&lt;a href=&#34;https://github.com/kubernetes/kubernetes&#34; target=&#34;_blank&#34;&gt;GitHub&lt;/a&gt;] [&lt;a href=&#34;https://twitter.com/kubernetesio&#34; target=&#34;_blank&#34;&gt;Twitter&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Istio: [&lt;a href=&#34;https://github.com/istio/istio&#34; target=&#34;_blank&#34;&gt;GitHub&lt;/a&gt;] [&lt;a href=&#34;https://twitter.com/IstioMesh&#34; target=&#34;_blank&#34;&gt;Twitter&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Envoy: [&lt;a href=&#34;https://github.com/envoyproxy/envoy&#34; target=&#34;_blank&#34;&gt;GitHub&lt;/a&gt;] [&lt;a href=&#34;https://twitter.com/EnvoyProxy&#34; target=&#34;_blank&#34;&gt;Twitter&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thank you for following along!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Contributor Summit Amsterdam Postponed</title>
      <link>https://kubernetes.io/blog/2020/03/04/contributor-summit-delayed/</link>
      <pubDate>Wed, 04 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2020/03/04/contributor-summit-delayed/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Dawn Foster (VMware), Jorge Castro (VMware)&lt;/p&gt;

&lt;p&gt;The CNCF has announced that &lt;a href=&#34;https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/attend/novel-coronavirus-update/&#34; target=&#34;_blank&#34;&gt;KubeCon + CloudNativeCon EU has been delayed&lt;/a&gt; until July/August of 2020. As a result the Contributor Summit planning team is weighing options for how to proceed. Here’s the current plan:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;There will be an in-person Contributor Summit as planned when KubeCon + CloudNativeCon is rescheduled.&lt;/li&gt;
&lt;li&gt;We are looking at options for having additional virtual contributor activities in the meantime.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We will communicate via this blog and the usual communications channels on the final plan. Please bear with us as we adapt when we get more information. Thank you for being patient as the team pivots to bring you a great Contributor Summit!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Bring your ideas to the world with kubectl plugins</title>
      <link>https://kubernetes.io/blog/2020/02/28/bring-your-ideas-to-the-world-with-kubectl-plugins/</link>
      <pubDate>Fri, 28 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2020/02/28/bring-your-ideas-to-the-world-with-kubectl-plugins/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author:&lt;/strong&gt; Cornelius Weig (TNG Technology Consulting GmbH)&lt;/p&gt;

&lt;p&gt;&lt;code&gt;kubectl&lt;/code&gt; is the most critical tool to interact with Kubernetes and has to address multiple user personas, each with their own needs and opinions.
One way to make &lt;code&gt;kubectl&lt;/code&gt; do what you need is to build new functionality into &lt;code&gt;kubectl&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;challenges-with-building-commands-into-kubectl&#34;&gt;Challenges with building commands into &lt;code&gt;kubectl&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;However, that&amp;rsquo;s easier said than done. Being such an important cornerstone of
Kubernetes, any meaningful change to &lt;code&gt;kubectl&lt;/code&gt; needs to undergo a Kubernetes
Enhancement Proposal (KEP) where the intended change is discussed beforehand.&lt;/p&gt;

&lt;p&gt;When it comes to implementation, you&amp;rsquo;ll find that &lt;code&gt;kubectl&lt;/code&gt; is an ingenious and
complex piece of engineering. It might take a long time to get used to
the processes and style of the codebase to get done what you want to achieve. Next
comes the review process which may go through several rounds until it meets all
the requirements of the Kubernetes maintainers &amp;ndash; after all, they need to take
over ownership of this feature and maintain it from the day it&amp;rsquo;s merged.&lt;/p&gt;

&lt;p&gt;When everything goes well, you can finally rejoice. Your code will be shipped
with the next Kubernetes release. Well, that could mean you need to wait
another 3 months to ship your idea in &lt;code&gt;kubectl&lt;/code&gt; if you are unlucky.&lt;/p&gt;

&lt;p&gt;So this was the happy path where everything goes well. But there are good
reasons why your new functionality may never make it into &lt;code&gt;kubectl&lt;/code&gt;. For one,
&lt;code&gt;kubectl&lt;/code&gt; has a particular look and feel and violating that style will not be
acceptable by the maintainers. For example, an interactive command that
produces output with colors would be inconsistent with the rest of &lt;code&gt;kubectl&lt;/code&gt;.
Also, when it comes to tools or commands useful only to a minuscule proportion
of users, the maintainers may simply reject your proposal as &lt;code&gt;kubectl&lt;/code&gt; needs to
address common needs.&lt;/p&gt;

&lt;p&gt;But this doesn’t mean you can’t ship your ideas to &lt;code&gt;kubectl&lt;/code&gt; users.&lt;/p&gt;

&lt;h2 id=&#34;what-if-you-didn-t-have-to-change-kubectl-to-add-functionality&#34;&gt;What if you didn’t have to change &lt;code&gt;kubectl&lt;/code&gt; to add functionality?&lt;/h2&gt;

&lt;p&gt;This is where &lt;code&gt;kubectl&lt;/code&gt; &lt;a href=&#34;https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/&#34; target=&#34;_blank&#34;&gt;plugins&lt;/a&gt; shine.
Since &lt;code&gt;kubectl&lt;/code&gt; v1.12, you can simply
drop executables into your &lt;code&gt;PATH&lt;/code&gt;, which follows the naming pattern
&lt;code&gt;kubectl-myplugin&lt;/code&gt;. Then you can execute this plugin as &lt;code&gt;kubectl myplugin&lt;/code&gt;, and
it will just feel like a normal sub-command of &lt;code&gt;kubectl&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Plugins give you the opportunity to try out new experiences like terminal UIs,
colorful output, specialized functionality, or other innovative ideas. You can
go creative, as you’re the owner of your own plugin.&lt;/p&gt;

&lt;p&gt;Further, plugins offer safe experimentation space for commands you’d like to
propose to &lt;code&gt;kubectl&lt;/code&gt;. By pre-releasing as a plugin, you can push your
functionality faster to the end-users and quickly gather feedback. For example,
the &lt;a href=&#34;https://github.com/verb/kubectl-debug&#34; target=&#34;_blank&#34;&gt;kubectl-debug&lt;/a&gt; plugin is proposed
to become a built-in command in &lt;code&gt;kubectl&lt;/code&gt; in a
&lt;a href=&#34;https://github.com/kubernetes/enhancements/blob/master/keps/sig-cli/20190805-kubectl-debug.md&#34; target=&#34;_blank&#34;&gt;KEP&lt;/a&gt;).
In the meanwhile, the plugin author can ship the functionality and collect
feedback using the plugin mechanism.&lt;/p&gt;

&lt;h2 id=&#34;how-to-get-started-with-developing-plugins&#34;&gt;How to get started with developing plugins&lt;/h2&gt;

&lt;p&gt;If you already have an idea for a plugin, how do you best make it happen?
First you have to ask yourself if you can implement it as a wrapper around
existing &lt;code&gt;kubectl&lt;/code&gt; functionality. If so, writing the plugin as a shell script
is often the best way forward, because the resulting plugin will be small,
works cross-platform, and has a high level of trust because it is not
compiled.&lt;/p&gt;

&lt;p&gt;On the other hand, if the plugin logic is complex, a general-purpose language
is usually better. The canonical choice here is Go, because you can use the
excellent &lt;code&gt;client-go&lt;/code&gt; library to interact with the Kubernetes API. The Kubernetes
maintained &lt;a href=&#34;https://github.com/kubernetes/sample-cli-plugin&#34; target=&#34;_blank&#34;&gt;sample-cli-plugin&lt;/a&gt;
demonstrates some best practices and can be used as a template for new plugin
projects.&lt;/p&gt;

&lt;p&gt;When the development is done, you just need to ship your plugin to the
Kubernetes users. For the best plugin installation experience and discoverability,
you should consider doing so via the
&lt;a href=&#34;https://github.com/kubernetes-sigs/krew&#34; target=&#34;_blank&#34;&gt;krew&lt;/a&gt; plugin manager. For an in-depth
discussion about the technical details around &lt;code&gt;kubectl&lt;/code&gt; plugins, refer to the
documentation on &lt;a href=&#34;https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/&#34; target=&#34;_blank&#34;&gt;kubernetes.io&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Contributor Summit Amsterdam Schedule Announced</title>
      <link>https://kubernetes.io/blog/2020/02/18/contributor-summit-amsterdam-schedule-announced/</link>
      <pubDate>Tue, 18 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2020/02/18/contributor-summit-amsterdam-schedule-announced/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jeffrey Sica (Red Hat), Amanda Katona (VMware)&lt;/p&gt;

&lt;p&gt;tl;dr &lt;a href=&#34;https://events.linuxfoundation.org/kubernetes-contributor-summit-europe/&#34; target=&#34;_blank&#34;&gt;Registration is open&lt;/a&gt; and the &lt;a href=&#34;https://kcseu2020.sched.com/&#34; target=&#34;_blank&#34;&gt;schedule is live&lt;/a&gt; so register now and we’ll see you in Amsterdam!&lt;/p&gt;

&lt;h2 id=&#34;kubernetes-contributor-summit&#34;&gt;Kubernetes Contributor Summit&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Sunday, March 29, 2020&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Evening Contributor Celebration:
&lt;a href=&#34;https://www.zuid-pool.nl/en/&#34; target=&#34;_blank&#34;&gt;ZuidPool&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Address: &lt;a href=&#34;https://www.google.com/search?q=KubeCon+Amsterdam+2020&amp;amp;ie=UTF-8&amp;amp;ibp=htl;events&amp;amp;rciv=evn&amp;amp;sa=X&amp;amp;ved=2ahUKEwiZoLvQ0dvnAhVST6wKHScBBZ8Q5bwDMAB6BAgSEAE#&#34; target=&#34;_blank&#34;&gt;Europaplein 22, 1078 GZ Amsterdam, Netherlands&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Time: 18:00 - 21:00&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Monday, March 30, 2020&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;All Day Contributor Summit:&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.rai.nl/en/&#34; target=&#34;_blank&#34;&gt;Amsterdam RAI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Address: &lt;a href=&#34;https://www.google.com/search?q=kubecon+amsterdam+2020&amp;amp;oq=kubecon+amste&amp;amp;aqs=chrome.0.35i39j69i57j0l4j69i61l2.3957j1j4&amp;amp;sourceid=chrome&amp;amp;ie=UTF-8&amp;amp;ibp=htl;events&amp;amp;rciv=evn&amp;amp;sa=X&amp;amp;ved=2ahUKEwiZoLvQ0dvnAhVST6wKHScBBZ8Q5bwDMAB6BAgSEAE#&#34; target=&#34;_blank&#34;&gt;Europaplein 24, 1078 GZ Amsterdam, Netherlands&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Time:  09:00 - 17:00 (Breakfast at 08:00)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2020-02-18-Contributor-Summit-Amsterdam-Schedule-Announced/contribsummit.jpg&#34; alt=&#34;Contributor Summit&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Hello everyone and Happy 2020! It’s hard to believe that KubeCon EU 2020 is less than six weeks away, and with that another contributor summit! This year we have the pleasure of being in Amsterdam in early spring, so be sure to pack some warmer clothing. This summit looks to be exciting with a lot of fantastic community-driven content. We received &lt;strong&gt;26&lt;/strong&gt; submissions from the CFP. From that, the events team selected &lt;strong&gt;12&lt;/strong&gt; sessions. Each of the sessions falls into one of four categories:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Community&lt;/li&gt;
&lt;li&gt;Contributor Improvement&lt;/li&gt;
&lt;li&gt;Sustainability&lt;/li&gt;
&lt;li&gt;In-depth Technical&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;On top of the presentations, there will be a dedicated Docs Sprint as well as the New Contributor Workshop 101 and 201 Sessions. All told, we will have five separate rooms of content throughout the day on Monday. Please &lt;strong&gt;&lt;a href=&#34;https://kcseu2020.sched.com/&#34; target=&#34;_blank&#34;&gt;see the full schedule&lt;/a&gt;&lt;/strong&gt; to see what sessions you’d be interested in. We hope between the content provided and the inevitable hallway track, everyone has a fun and enriching experience.&lt;/p&gt;

&lt;p&gt;Speaking of fun, the social Sunday night should be a blast! We’re hosting this summit’s social close to the conference center, at &lt;a href=&#34;https://www.zuid-pool.nl/en/&#34; target=&#34;_blank&#34;&gt;ZuidPool&lt;/a&gt;. There will be games, bingo, and unconference sign-up throughout the evening. It should be a relaxed way to kick off the week.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://events.linuxfoundation.org/kubernetes-contributor-summit-europe/&#34; target=&#34;_blank&#34;&gt;Registration is open&lt;/a&gt;! Space is limited so it’s always a good idea to register early.&lt;/p&gt;

&lt;p&gt;If you have any questions, reach out to the &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/events/2020/03-contributor-summit#team&#34; target=&#34;_blank&#34;&gt;Amsterdam Team&lt;/a&gt; on Slack in the &lt;a href=&#34;https://kubernetes.slack.com/archives/C7J893413&#34; target=&#34;_blank&#34;&gt;#contributor-summit&lt;/a&gt; channel.&lt;/p&gt;

&lt;p&gt;Hope to see you there!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Deploying External OpenStack Cloud Provider with Kubeadm</title>
      <link>https://kubernetes.io/blog/2020/02/07/deploying-external-openstack-cloud-provider-with-kubeadm/</link>
      <pubDate>Fri, 07 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2020/02/07/deploying-external-openstack-cloud-provider-with-kubeadm/</guid>
      <description>
        
        
        

&lt;p&gt;This document describes how to install a single control-plane Kubernetes cluster v1.15 with kubeadm on CentOS, and then deploy an external OpenStack cloud provider and Cinder CSI plugin to use Cinder volumes as persistent volumes in Kubernetes.&lt;/p&gt;

&lt;h3 id=&#34;preparation-in-openstack&#34;&gt;Preparation in OpenStack&lt;/h3&gt;

&lt;p&gt;This cluster runs on OpenStack VMs, so let&amp;rsquo;s create a few things in OpenStack first.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A project/tenant for this Kubernetes cluster&lt;/li&gt;
&lt;li&gt;A user in this project for Kubernetes, to query node information and attach volumes etc&lt;/li&gt;
&lt;li&gt;A private network and subnet&lt;/li&gt;
&lt;li&gt;A router for this private network and connect it to a public network for floating IPs&lt;/li&gt;
&lt;li&gt;A security group for all Kubernetes VMs&lt;/li&gt;
&lt;li&gt;A VM as a control-plane node and a few VMs as worker nodes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The security group will have the following rules to open ports for Kubernetes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Control-Plane Node&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Protocol&lt;/th&gt;
&lt;th&gt;Port Number&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;TCP&lt;/td&gt;
&lt;td&gt;6443&lt;/td&gt;
&lt;td&gt;Kubernetes API Server&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;TCP&lt;/td&gt;
&lt;td&gt;2379-2380&lt;/td&gt;
&lt;td&gt;etcd server client API&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;TCP&lt;/td&gt;
&lt;td&gt;10250&lt;/td&gt;
&lt;td&gt;Kubelet API&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;TCP&lt;/td&gt;
&lt;td&gt;10251&lt;/td&gt;
&lt;td&gt;kube-scheduler&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;TCP&lt;/td&gt;
&lt;td&gt;10252&lt;/td&gt;
&lt;td&gt;kube-controller-manager&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;TCP&lt;/td&gt;
&lt;td&gt;10255&lt;/td&gt;
&lt;td&gt;Read-only Kubelet API&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Worker Nodes&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Protocol&lt;/th&gt;
&lt;th&gt;Port Number&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;TCP&lt;/td&gt;
&lt;td&gt;10250&lt;/td&gt;
&lt;td&gt;Kubelet API&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;TCP&lt;/td&gt;
&lt;td&gt;10255&lt;/td&gt;
&lt;td&gt;Read-only Kubelet API&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;TCP&lt;/td&gt;
&lt;td&gt;30000-32767&lt;/td&gt;
&lt;td&gt;NodePort Services&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;CNI ports on both control-plane and worker nodes&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Protocol&lt;/th&gt;
&lt;th&gt;Port Number&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;TCP&lt;/td&gt;
&lt;td&gt;179&lt;/td&gt;
&lt;td&gt;Calico BGP network&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;TCP&lt;/td&gt;
&lt;td&gt;9099&lt;/td&gt;
&lt;td&gt;Calico felix (health check)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;UDP&lt;/td&gt;
&lt;td&gt;8285&lt;/td&gt;
&lt;td&gt;Flannel&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;UDP&lt;/td&gt;
&lt;td&gt;8472&lt;/td&gt;
&lt;td&gt;Flannel&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;TCP&lt;/td&gt;
&lt;td&gt;6781-6784&lt;/td&gt;
&lt;td&gt;Weave Net&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;UDP&lt;/td&gt;
&lt;td&gt;6783-6784&lt;/td&gt;
&lt;td&gt;Weave Net&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;CNI specific ports are only required to be opened when that particular CNI plugin is used. In this guide, we will use Weave Net. Only the Weave Net ports (TCP 6781-6784 and UDP 6783-6784), will need to be opened in the security group.&lt;/p&gt;

&lt;p&gt;The control-plane node needs at least 2 cores and 4GB RAM. After the VM is launched, verify its hostname and make sure it is the same as the node name in Nova.
If the hostname is not resolvable, add it to &lt;code&gt;/etc/hosts&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;For example, if the VM is called master1, and it has an internal IP 192.168.1.4. Add that to &lt;code&gt;/etc/hosts&lt;/code&gt; and set hostname to master1.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;color:#a2f&#34;&gt;echo&lt;/span&gt; &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;192.168.1.4 master1&amp;#34;&lt;/span&gt; &amp;gt;&amp;gt; /etc/hosts

hostnamectl set-hostname master1&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id=&#34;install-docker-and-kubernetes&#34;&gt;Install Docker and Kubernetes&lt;/h3&gt;

&lt;p&gt;Next, we&amp;rsquo;ll follow the official documents to install docker and Kubernetes using kubeadm.&lt;/p&gt;

&lt;p&gt;Install Docker following the steps from the &lt;a href=&#34;https://kubernetes.io/docs/setup/production-environment/container-runtimes/&#34;&gt;container runtime&lt;/a&gt; documentation.&lt;/p&gt;

&lt;p&gt;Note that it is a &lt;a href=&#34;https://kubernetes.io/docs/setup/production-environment/container-runtimes/#cgroup-drivers&#34;&gt;best practice to use systemd as the cgroup driver&lt;/a&gt; for Kubernetes.
If you use an internal container registry, add them to the docker config.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# Install Docker CE&lt;/span&gt;
&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;## Set up the repository&lt;/span&gt;
&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;### Install required packages.&lt;/span&gt;

yum install yum-utils device-mapper-persistent-data lvm2

&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;### Add Docker repository.&lt;/span&gt;

yum-config-manager &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;  --add-repo &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;  https://download.docker.com/linux/centos/docker-ce.repo

&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;## Install Docker CE.&lt;/span&gt;

yum update &lt;span style=&#34;color:#666&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; yum install docker-ce-18.06.2.ce

&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;## Create /etc/docker directory.&lt;/span&gt;

mkdir /etc/docker

&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# Configure the Docker daemon&lt;/span&gt;

cat &amp;gt; /etc/docker/daemon.json &lt;span style=&#34;color:#b44&#34;&gt;&amp;lt;&amp;lt;EOF
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;{
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;  &amp;#34;exec-opts&amp;#34;: [&amp;#34;native.cgroupdriver=systemd&amp;#34;],
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;  &amp;#34;log-driver&amp;#34;: &amp;#34;json-file&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;  &amp;#34;log-opts&amp;#34;: {
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;    &amp;#34;max-size&amp;#34;: &amp;#34;100m&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;  },
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;  &amp;#34;storage-driver&amp;#34;: &amp;#34;overlay2&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;  &amp;#34;storage-opts&amp;#34;: [
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;    &amp;#34;overlay2.override_kernel_check=true&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;  ]
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;}
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;EOF&lt;/span&gt;

mkdir -p /etc/systemd/system/docker.service.d

&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# Restart Docker&lt;/span&gt;
systemctl daemon-reload
systemctl restart docker
systemctl &lt;span style=&#34;color:#a2f&#34;&gt;enable&lt;/span&gt; docker&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Install kubeadm following the steps from the &lt;a href=&#34;https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/&#34;&gt;Installing Kubeadm&lt;/a&gt; documentation.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;cat &lt;span style=&#34;color:#b44&#34;&gt;&amp;lt;&amp;lt;EOF &amp;gt; /etc/yum.repos.d/kubernetes.repo
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;[kubernetes]
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;name=Kubernetes
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;enabled=1
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;gpgcheck=1
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;repo_gpgcheck=1
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;EOF&lt;/span&gt;

&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# Set SELinux in permissive mode (effectively disabling it)&lt;/span&gt;
&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# Caveat: In a production environment you may not want to disable SELinux, please refer to Kubernetes documents about SELinux&lt;/span&gt;
setenforce &lt;span style=&#34;color:#666&#34;&gt;0&lt;/span&gt;
sed -i &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;s/^SELINUX=enforcing$/SELINUX=permissive/&amp;#39;&lt;/span&gt; /etc/selinux/config

yum install -y kubelet kubeadm kubectl --disableexcludes&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;kubernetes

systemctl &lt;span style=&#34;color:#a2f&#34;&gt;enable&lt;/span&gt; --now kubelet

cat &lt;span style=&#34;color:#b44&#34;&gt;&amp;lt;&amp;lt;EOF &amp;gt;  /etc/sysctl.d/k8s.conf
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;net.bridge.bridge-nf-call-ip6tables = 1
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;net.bridge.bridge-nf-call-iptables = 1
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;EOF&lt;/span&gt;
sysctl --system

&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# check if br_netfilter module is loaded&lt;/span&gt;
lsmod | grep br_netfilter

&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# if not, load it explicitly with&lt;/span&gt; 
modprobe br_netfilter&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The official document about how to create a single control-plane cluster can be found from the &lt;a href=&#34;https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/&#34;&gt;Creating a single control-plane cluster with kubeadm&lt;/a&gt; documentation.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ll largely follow that document but also add additional things for the cloud provider.
To make things more clear, we&amp;rsquo;ll use a &lt;code&gt;kubeadm-config.yml&lt;/code&gt; for the control-plane node.
In this config we specify to use an external OpenStack cloud provider, and where to find its config.
We also enable storage API in API server&amp;rsquo;s runtime config so we can use OpenStack volumes as persistent volumes in Kubernetes.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;kubeadm.k8s.io/v1beta1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;InitConfiguration&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;nodeRegistration:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;kubeletExtraArgs:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;cloud-provider:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;external&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;---&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;kubeadm.k8s.io/v1beta2&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;ClusterConfiguration&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kubernetesVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;v1.15.1&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;apiServer:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;extraArgs:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;enable-admission-plugins:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;NodeRestriction&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;runtime-config:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;storage.k8s.io/v1=true&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;controllerManager:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;extraArgs:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;external-cloud-volume-plugin:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;openstack&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;extraVolumes:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;cloud-config&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;hostPath:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;/etc/kubernetes/cloud-config&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;mountPath:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;/etc/kubernetes/cloud-config&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;readOnly:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;true&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;pathType:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;File&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;networking:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;serviceSubnet:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;10.96.0.0/12&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;podSubnet:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;10.224.0.0/16&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;dnsDomain:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;cluster.local&amp;#34;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now we&amp;rsquo;ll create the cloud config, &lt;code&gt;/etc/kubernetes/cloud-config&lt;/code&gt;, for OpenStack.
Note that the tenant here is the one we created for all Kubernetes VMs in the beginning.
All VMs should be launched in this project/tenant.
In addition you need to create a user in this tenant for Kubernetes to do queries.
The ca-file is the CA root certificate for OpenStack&amp;rsquo;s API endpoint, for example &lt;code&gt;https://openstack.cloud:5000/v3&lt;/code&gt;
At the time of writing the cloud provider doesn&amp;rsquo;t allow insecure connections (skip CA check).&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-ini&#34; data-lang=&#34;ini&#34;&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;[Global]&lt;/span&gt;
&lt;span style=&#34;color:#b44&#34;&gt;region&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;RegionOne&lt;/span&gt;
&lt;span style=&#34;color:#b44&#34;&gt;username&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;username&lt;/span&gt;
&lt;span style=&#34;color:#b44&#34;&gt;password&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;password&lt;/span&gt;
&lt;span style=&#34;color:#b44&#34;&gt;auth-url&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;https://openstack.cloud:5000/v3&lt;/span&gt;
&lt;span style=&#34;color:#b44&#34;&gt;tenant-id&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;14ba698c0aec4fd6b7dc8c310f664009&lt;/span&gt;
&lt;span style=&#34;color:#b44&#34;&gt;domain-id&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;default&lt;/span&gt;
&lt;span style=&#34;color:#b44&#34;&gt;ca-file&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;/etc/kubernetes/ca.pem&lt;/span&gt;

&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;[LoadBalancer]&lt;/span&gt;
&lt;span style=&#34;color:#b44&#34;&gt;subnet-id&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;b4a9a292-ea48-4125-9fb2-8be2628cb7a1&lt;/span&gt;
&lt;span style=&#34;color:#b44&#34;&gt;floating-network-id&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;bc8a590a-5d65-4525-98f3-f7ef29c727d5&lt;/span&gt;

&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;[BlockStorage]&lt;/span&gt;
&lt;span style=&#34;color:#b44&#34;&gt;bs-version&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;v2&lt;/span&gt;

&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;[Networking]&lt;/span&gt;
&lt;span style=&#34;color:#b44&#34;&gt;public-network-name&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;public&lt;/span&gt;
&lt;span style=&#34;color:#b44&#34;&gt;ipv6-support-disabled&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;false&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Next run kubeadm to initiate the control-plane node&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubeadm init --config&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;kubeadm-config.yml&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;With the initialization completed, copy admin config to .kube&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;  mkdir -p &lt;span style=&#34;color:#b8860b&#34;&gt;$HOME&lt;/span&gt;/.kube
  sudo cp -i /etc/kubernetes/admin.conf &lt;span style=&#34;color:#b8860b&#34;&gt;$HOME&lt;/span&gt;/.kube/config
  sudo chown &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;$(&lt;/span&gt;id -u&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;)&lt;/span&gt;:&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;$(&lt;/span&gt;id -g&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;)&lt;/span&gt; &lt;span style=&#34;color:#b8860b&#34;&gt;$HOME&lt;/span&gt;/.kube/config&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;At this stage, the control-plane node is created but not ready. All the nodes have the taint &lt;code&gt;node.cloudprovider.kubernetes.io/uninitialized=true:NoSchedule&lt;/code&gt; and are waiting to be initialized by the cloud-controller-manager.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;# kubectl describe no master1
Name:               master1
Roles:              master
......
Taints:             node-role.kubernetes.io/master:NoSchedule
                    node.cloudprovider.kubernetes.io/uninitialized=true:NoSchedule
                    node.kubernetes.io/not-ready:NoSchedule
......&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now deploy the OpenStack cloud controller manager into the cluster, following &lt;a href=&#34;https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/using-controller-manager-with-kubeadm.md&#34; target=&#34;_blank&#34;&gt;using controller manager with kubeadm&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Create a secret with the cloud-config for the openstack cloud provider.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl create secret -n kube-system generic cloud-config --from-literal&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;cloud.conf&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;$(&lt;/span&gt;cat /etc/kubernetes/cloud-config&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;)&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;&lt;/span&gt; --dry-run -o yaml &amp;gt; cloud-config-secret.yaml
kubectl apply -f cloud-config-secret.yaml &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Get the CA certificate for OpenStack API endpoints and put that into &lt;code&gt;/etc/kubernetes/ca.pem&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Create RBAC resources.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl apply -f https://github.com/kubernetes/cloud-provider-openstack/raw/release-1.15/cluster/addons/rbac/cloud-controller-manager-roles.yaml
kubectl apply -f https://github.com/kubernetes/cloud-provider-openstack/raw/release-1.15/cluster/addons/rbac/cloud-controller-manager-role-bindings.yaml&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We&amp;rsquo;ll run the OpenStack cloud controller manager as a DaemonSet rather than a pod.
The manager will only run on the control-plane node, so if there are multiple control-plane nodes, multiple pods will be run for high availability.
Create &lt;code&gt;openstack-cloud-controller-manager-ds.yaml&lt;/code&gt; containing the following manifests, then apply it.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;---&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;v1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;ServiceAccount&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;cloud-controller-manager&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;namespace:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;kube-system&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;---&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;apps/v1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;DaemonSet&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;openstack-cloud-controller-manager&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;namespace:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;kube-system&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;labels:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;k8s-app:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;openstack-cloud-controller-manager&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;selector:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;matchLabels:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;k8s-app:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;openstack-cloud-controller-manager&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;updateStrategy:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;RollingUpdate&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;template:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;labels:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;k8s-app:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;openstack-cloud-controller-manager&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;nodeSelector:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;node-role.kubernetes.io/master:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;securityContext:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;runAsUser:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;1001&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;tolerations:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;key:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;node.cloudprovider.kubernetes.io/uninitialized&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;value:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;true&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;effect:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;NoSchedule&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;key:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;node-role.kubernetes.io/master&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;effect:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;NoSchedule&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;effect:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;NoSchedule&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;key:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;node.kubernetes.io/not-ready&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;serviceAccountName:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;cloud-controller-manager&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;containers:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;openstack-cloud-controller-manager&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;image:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;docker.io/k8scloudprovider/openstack-cloud-controller-manager:v1&lt;span style=&#34;color:#666&#34;&gt;.15.0&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;args:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/bin/openstack-cloud-controller-manager&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;--v=&lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;--cloud-config=$(CLOUD_CONFIG)&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;--cloud-provider=openstack&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;--use-service-account-credentials=&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;true&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;--address=&lt;span style=&#34;color:#666&#34;&gt;127.0.0.1&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;volumeMounts:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;mountPath:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/etc/kubernetes/pki&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;k8s-certs&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;readOnly:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;true&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;mountPath:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/etc/ssl/certs&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;ca-certs&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;readOnly:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;true&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;mountPath:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/etc/config&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;cloud-config-volume&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;readOnly:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;true&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;mountPath:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/usr/libexec/kubernetes/kubelet-plugins/volume/exec&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;flexvolume-dir&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;mountPath:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/etc/kubernetes&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;ca-cert&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;readOnly:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;true&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;resources:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;requests:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;cpu:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;200m&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;env:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;CLOUD_CONFIG&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;value:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/etc/config/cloud.conf&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;hostNetwork:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;true&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;volumes:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;hostPath:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;path:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/usr/libexec/kubernetes/kubelet-plugins/volume/exec&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;DirectoryOrCreate&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;flexvolume-dir&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;hostPath:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;path:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/etc/kubernetes/pki&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;DirectoryOrCreate&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;k8s-certs&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;hostPath:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;path:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/etc/ssl/certs&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;DirectoryOrCreate&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;ca-certs&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;cloud-config-volume&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;secret:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;secretName:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;cloud-config&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;ca-cert&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;secret:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;secretName:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;openstack-ca-cert&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;When the controller manager is running, it will query OpenStack to get information about the nodes and remove the taint. In the node info you&amp;rsquo;ll see the VM&amp;rsquo;s UUID in OpenStack.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;# kubectl describe no master1
Name:               master1
Roles:              master
......
Taints:             node-role.kubernetes.io/master:NoSchedule
                    node.kubernetes.io/not-ready:NoSchedule
......
sage:docker: network plugin is not ready: cni config uninitialized
......
PodCIDR:                     10.224.0.0/24
ProviderID:                  openstack:///548e3c46-2477-4ce2-968b-3de1314560a5&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now install your favourite CNI and the control-plane node will become ready.&lt;/p&gt;

&lt;p&gt;For example, to install Weave Net, run this command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl apply -f &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;https://cloud.weave.works/k8s/net?k8s-version=&lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;$(&lt;/span&gt;kubectl version | base64 | tr -d &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;\n&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;)&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Next we&amp;rsquo;ll set up worker nodes.&lt;/p&gt;

&lt;p&gt;Firstly, install docker and kubeadm in the same way as how they were installed in the control-plane node.
To join them to the cluster we need a token and ca cert hash from the output of control-plane node installation.
If it is expired or lost we can recreate it using these commands.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# check if token is expired&lt;/span&gt;
kubeadm token list

&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# re-create token and show join command&lt;/span&gt;
kubeadm token create --print-join-command&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Create &lt;code&gt;kubeadm-config.yml&lt;/code&gt; for worker nodes with the above token and ca cert hash.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;kubeadm.k8s.io/v1beta2&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;discovery:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;bootstrapToken:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;apiServerEndpoint:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;192.168.1.7&lt;/span&gt;:&lt;span style=&#34;color:#666&#34;&gt;6443&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;token:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;0c0z4p.dnafh6vnmouus569&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;caCertHashes:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;sha256:fcb3e956a6880c05fc9d09714424b827f57a6fdc8afc44497180905946527adf&amp;#34;&lt;/span&gt;]&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;JoinConfiguration&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;nodeRegistration:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;kubeletExtraArgs:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;cloud-provider:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;external&amp;#34;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;apiServerEndpoint is the control-plane node, token and caCertHashes can be taken from the join command printed in the output of &amp;lsquo;kubeadm token create&amp;rsquo; command.&lt;/p&gt;

&lt;p&gt;Run kubeadm and the worker nodes will be joined to the cluster.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubeadm join  --config kubeadm-config.yml &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;At this stage we&amp;rsquo;ll have a working Kubernetes cluster with an external OpenStack cloud provider.
The provider tells Kubernetes about the mapping between Kubernetes nodes and OpenStack VMs.
If Kubernetes wants to attach a persistent volume to a pod, it can find out which OpenStack VM the pod is running on from the mapping, and attach the underlying OpenStack volume to the VM accordingly.&lt;/p&gt;

&lt;h3 id=&#34;deploy-cinder-csi&#34;&gt;Deploy Cinder CSI&lt;/h3&gt;

&lt;p&gt;The integration with Cinder is provided by an external Cinder CSI plugin, as described in the &lt;a href=&#34;https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/using-cinder-csi-plugin.md&#34; target=&#34;_blank&#34;&gt;Cinder CSI&lt;/a&gt; documentation.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ll perform the following steps to install the Cinder CSI plugin.
Firstly, create a secret with CA certs for OpenStack&amp;rsquo;s API endpoints. It is the same cert file as what we use in cloud provider above.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl create secret -n kube-system generic openstack-ca-cert --from-literal&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;ca.pem&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;$(&lt;/span&gt;cat /etc/kubernetes/ca.pem&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;)&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;&lt;/span&gt; --dry-run -o yaml &amp;gt; openstack-ca-cert.yaml
kubectl apply -f openstack-ca-cert.yaml&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then create RBAC resources.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl apply -f https://raw.githubusercontent.com/kubernetes/cloud-provider-openstack/release-1.15/manifests/cinder-csi-plugin/cinder-csi-controllerplugin-rbac.yaml
kubectl apply -f https://github.com/kubernetes/cloud-provider-openstack/raw/release-1.15/manifests/cinder-csi-plugin/cinder-csi-nodeplugin-rbac.yaml&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The Cinder CSI plugin includes a controller plugin and a node plugin.
The controller communicates with Kubernetes APIs and Cinder APIs to create/attach/detach/delete Cinder volumes. The node plugin in-turn runs on each worker node to bind a storage device (attached volume) to a pod, and unbind it during deletion.
Create &lt;code&gt;cinder-csi-controllerplugin.yaml&lt;/code&gt; and apply it to create csi controller.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Service&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;v1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;csi-cinder-controller-service&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;namespace:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;kube-system&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;labels:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;app:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;csi-cinder-controllerplugin&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;selector:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;app:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;csi-cinder-controllerplugin&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;ports:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;dummy&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;port:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;12345&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;---&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;StatefulSet&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;apps/v1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;csi-cinder-controllerplugin&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;namespace:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;kube-system&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;serviceName:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;csi-cinder-controller-service&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;replicas:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;selector:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;matchLabels:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;app:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;csi-cinder-controllerplugin&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;template:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;labels:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;app:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;csi-cinder-controllerplugin&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;serviceAccount:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;csi-cinder-controller-sa&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;containers:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;csi-attacher&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;image:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;quay.io/k8scsi/csi-attacher:v1&lt;span style=&#34;color:#666&#34;&gt;.0.1&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;args:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;--v=5&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;--csi-address=$(ADDRESS)&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;env:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;ADDRESS&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;value:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/var/lib/csi/sockets/pluginproxy/csi.sock&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;imagePullPolicy:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;IfNotPresent&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;volumeMounts:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;socket-dir&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;mountPath:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/var/lib/csi/sockets/pluginproxy/&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;csi-provisioner&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;image:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;quay.io/k8scsi/csi-provisioner:v1&lt;span style=&#34;color:#666&#34;&gt;.0.1&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;args:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;--provisioner=csi-cinderplugin&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;--csi-address=$(ADDRESS)&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;env:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;ADDRESS&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;value:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/var/lib/csi/sockets/pluginproxy/csi.sock&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;imagePullPolicy:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;IfNotPresent&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;volumeMounts:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;socket-dir&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;mountPath:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/var/lib/csi/sockets/pluginproxy/&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;csi-snapshotter&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;image:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;quay.io/k8scsi/csi-snapshotter:v1&lt;span style=&#34;color:#666&#34;&gt;.0.1&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;args:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;--connection-timeout=15s&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;--csi-address=$(ADDRESS)&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;env:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;ADDRESS&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;value:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/var/lib/csi/sockets/pluginproxy/csi.sock&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;imagePullPolicy:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Always&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;volumeMounts:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;mountPath:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/var/lib/csi/sockets/pluginproxy/&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;socket-dir&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;cinder-csi-plugin&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;image:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;docker.io/k8scloudprovider/cinder-csi-plugin:v1&lt;span style=&#34;color:#666&#34;&gt;.15.0&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;args&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/bin/cinder-csi-plugin&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;--v=5&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;--nodeid=$(NODE_ID)&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;--endpoint=$(CSI_ENDPOINT)&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;--cloud-config=$(CLOUD_CONFIG)&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;--cluster=$(CLUSTER_NAME)&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;env:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;NODE_ID&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;valueFrom:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;                &lt;/span&gt;fieldRef:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;                  &lt;/span&gt;fieldPath:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;spec.nodeName&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;CSI_ENDPOINT&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;value:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;unix://csi/csi.sock&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;CLOUD_CONFIG&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;value:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/etc/config/cloud.conf&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;CLUSTER_NAME&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;value:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;kubernetes&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;imagePullPolicy:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;IfNotPresent&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;volumeMounts:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;socket-dir&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;mountPath:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/csi&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;secret-cinderplugin&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;mountPath:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/etc/config&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;readOnly:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;true&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;mountPath:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/etc/kubernetes&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;ca-cert&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;readOnly:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;true&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;volumes:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;socket-dir&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;hostPath:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;path:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/var/lib/csi/sockets/pluginproxy/&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;DirectoryOrCreate&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;secret-cinderplugin&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;secret:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;secretName:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;cloud-config&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;ca-cert&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;secret:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;secretName:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;openstack-ca-cert&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Create &lt;code&gt;cinder-csi-nodeplugin.yaml&lt;/code&gt; and apply it to create csi node.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;DaemonSet&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;apps/v1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;csi-cinder-nodeplugin&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;namespace:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;kube-system&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;selector:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;matchLabels:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;app:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;csi-cinder-nodeplugin&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;template:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;labels:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;app:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;csi-cinder-nodeplugin&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;serviceAccount:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;csi-cinder-node-sa&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;hostNetwork:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;true&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;containers:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;node-driver-registrar&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;image:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;quay.io/k8scsi/csi-node-driver-registrar:v1&lt;span style=&#34;color:#666&#34;&gt;.1.0&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;args:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;--v=5&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;--csi-address=$(ADDRESS)&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;--kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;lifecycle:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;preStop:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;exec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;                &lt;/span&gt;command:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;/bin/sh&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;-c&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;rm -rf /registration/cinder.csi.openstack.org /registration/cinder.csi.openstack.org-reg.sock&amp;#34;&lt;/span&gt;]&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;env:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;ADDRESS&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;value:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/csi/csi.sock&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;DRIVER_REG_SOCK_PATH&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;value:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/var/lib/kubelet/plugins/cinder.csi.openstack.org/csi.sock&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;KUBE_NODE_NAME&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;valueFrom:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;                &lt;/span&gt;fieldRef:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;                  &lt;/span&gt;fieldPath:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;spec.nodeName&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;imagePullPolicy:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;IfNotPresent&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;volumeMounts:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;socket-dir&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;mountPath:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/csi&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;registration-dir&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;mountPath:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/registration&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;cinder-csi-plugin&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;securityContext:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;privileged:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;true&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;capabilities:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;add:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;SYS_ADMIN&amp;#34;&lt;/span&gt;]&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;allowPrivilegeEscalation:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;true&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;image:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;docker.io/k8scloudprovider/cinder-csi-plugin:v1&lt;span style=&#34;color:#666&#34;&gt;.15.0&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;args&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/bin/cinder-csi-plugin&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;--nodeid=$(NODE_ID)&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;--endpoint=$(CSI_ENDPOINT)&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;--cloud-config=$(CLOUD_CONFIG)&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;env:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;NODE_ID&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;valueFrom:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;                &lt;/span&gt;fieldRef:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;                  &lt;/span&gt;fieldPath:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;spec.nodeName&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;CSI_ENDPOINT&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;value:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;unix://csi/csi.sock&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;CLOUD_CONFIG&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;value:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/etc/config/cloud.conf&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;imagePullPolicy:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;IfNotPresent&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;volumeMounts:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;socket-dir&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;mountPath:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/csi&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;pods-mount-dir&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;mountPath:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/var/lib/kubelet/pods&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;mountPropagation:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;Bidirectional&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;kubelet-dir&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;mountPath:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/var/lib/kubelet&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;mountPropagation:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;Bidirectional&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;pods-cloud-data&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;mountPath:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/var/lib/cloud/data&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;readOnly:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;true&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;pods-probe-dir&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;mountPath:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/dev&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;mountPropagation:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;HostToContainer&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;secret-cinderplugin&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;mountPath:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/etc/config&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;readOnly:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;true&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;mountPath:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/etc/kubernetes&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;ca-cert&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;readOnly:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;true&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;volumes:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;socket-dir&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;hostPath:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;path:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/var/lib/kubelet/plugins/cinder.csi.openstack.org&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;DirectoryOrCreate&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;registration-dir&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;hostPath:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;path:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/var/lib/kubelet/plugins_registry/&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Directory&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;kubelet-dir&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;hostPath:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;path:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/var/lib/kubelet&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Directory&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;pods-mount-dir&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;hostPath:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;path:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/var/lib/kubelet/pods&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Directory&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;pods-cloud-data&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;hostPath:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;path:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/var/lib/cloud/data&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Directory&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;pods-probe-dir&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;hostPath:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;path:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/dev&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Directory&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;secret-cinderplugin&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;secret:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;secretName:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;cloud-config&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;ca-cert&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;secret:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;secretName:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;openstack-ca-cert&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;When they are both running, create a storage class for Cinder.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;storage.k8s.io/v1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;StorageClass&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;csi-sc-cinderplugin&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;provisioner:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;csi-cinderplugin&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then we can create a PVC with this class.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;v1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;PersistentVolumeClaim&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;myvol&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;accessModes:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;ReadWriteOnce&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;resources:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;requests:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;storage:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;1Gi&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;storageClassName:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;csi-sc-cinderplugin&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;When the PVC is created, a Cinder volume is created correspondingly.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;# kubectl get pvc
NAME    STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS          AGE
myvol   Bound    pvc-14b8bc68-6c4c-4dc6-ad79-4cb29a81faad   1Gi        RWO            csi-sc-cinderplugin   3s&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In OpenStack the volume name will match the Kubernetes persistent volume generated name. In this example it would be: &lt;em&gt;pvc-14b8bc68-6c4c-4dc6-ad79-4cb29a81faad&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Now we can create a pod with the PVC.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;v1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Pod&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;web&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;containers:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;web&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;image:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;nginx&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;ports:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;web&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;containerPort:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;80&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;hostPort:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;8081&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;protocol:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;TCP&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;volumeMounts:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;mountPath:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;/usr/share/nginx/html&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;mypd&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;volumes:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;mypd&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;persistentVolumeClaim:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;claimName:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;myvol&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;When the pod is running, the volume will be attached to the pod.
If we go back to OpenStack, we can see the Cinder volume is mounted to the worker node where the pod is running on.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;# openstack volume show 6b5f3296-b0eb-40cd-bd4f-2067a0d6287f
+--------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Field                          | Value                                                                                                                                                                                                                                                                                                                          |
+--------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| attachments                    | [{u&amp;#39;server_id&amp;#39;: u&amp;#39;1c5e1439-edfa-40ed-91fe-2a0e12bc7eb4&amp;#39;, u&amp;#39;attachment_id&amp;#39;: u&amp;#39;11a15b30-5c24-41d4-86d9-d92823983a32&amp;#39;, u&amp;#39;attached_at&amp;#39;: u&amp;#39;2019-07-24T05:02:34.000000&amp;#39;, u&amp;#39;host_name&amp;#39;: u&amp;#39;compute-6&amp;#39;, u&amp;#39;volume_id&amp;#39;: u&amp;#39;6b5f3296-b0eb-40cd-bd4f-2067a0d6287f&amp;#39;, u&amp;#39;device&amp;#39;: u&amp;#39;/dev/vdb&amp;#39;, u&amp;#39;id&amp;#39;: u&amp;#39;6b5f3296-b0eb-40cd-bd4f-2067a0d6287f&amp;#39;}] |
| availability_zone              | nova                                                                                                                                                                                                                                                                                                                           |
| bootable                       | false                                                                                                                                                                                                                                                                                                                          |
| consistencygroup_id            | None                                                                                                                                                                                                                                                                                                                           |
| created_at                     | 2019-07-24T05:02:18.000000                                                                                                                                                                                                                                                                                                     |
| description                    | Created by OpenStack Cinder CSI driver                                                                                                                                                                                                                                                                                         |
| encrypted                      | False                                                                                                                                                                                                                                                                                                                          |
| id                             | 6b5f3296-b0eb-40cd-bd4f-2067a0d6287f                                                                                                                                                                                                                                                                                           |
| migration_status               | None                                                                                                                                                                                                                                                                                                                           |
| multiattach                    | False                                                                                                                                                                                                                                                                                                                          |
| name                           | pvc-14b8bc68-6c4c-4dc6-ad79-4cb29a81faad                                                                                                                                                                                                                                                                                       |
| os-vol-host-attr:host          | rbd:volumes@rbd#rbd                                                                                                                                                                                                                                                                                                            |
| os-vol-mig-status-attr:migstat | None                                                                                                                                                                                                                                                                                                                           |
| os-vol-mig-status-attr:name_id | None                                                                                                                                                                                                                                                                                                                           |
| os-vol-tenant-attr:tenant_id   | 14ba698c0aec4fd6b7dc8c310f664009                                                                                                                                                                                                                                                                                               |
| properties                     | attached_mode=&amp;#39;rw&amp;#39;, cinder.csi.openstack.org/cluster=&amp;#39;kubernetes&amp;#39;                                                                                                                                                                                                                                                              |
| replication_status             | None                                                                                                                                                                                                                                                                                                                           |
| size                           | 1                                                                                                                                                                                                                                                                                                                              |
| snapshot_id                    | None                                                                                                                                                                                                                                                                                                                           |
| source_volid                   | None                                                                                                                                                                                                                                                                                                                           |
| status                         | in-use                                                                                                                                                                                                                                                                                                                         |
| type                           | rbd                                                                                                                                                                                                                                                                                                                            |
| updated_at                     | 2019-07-24T05:02:35.000000                                                                                                                                                                                                                                                                                                     |
| user_id                        | 5f6a7a06f4e3456c890130d56babf591                                                                                                                                                                                                                                                                                               |
+--------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id=&#34;summary&#34;&gt;Summary&lt;/h3&gt;

&lt;p&gt;In this walk-through, we deployed a Kubernetes cluster on OpenStack VMs and integrated it with OpenStack using an external OpenStack cloud provider. Then on this Kubernetes cluster we deployed Cinder CSI plugin which can create Cinder volumes and expose them in Kubernetes as persistent volumes.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: KubeInvaders - Gamified Chaos Engineering Tool for Kubernetes</title>
      <link>https://kubernetes.io/blog/2020/01/22/kubeinvaders-gamified-chaos-engineering-tool-for-kubernetes/</link>
      <pubDate>Wed, 22 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2020/01/22/kubeinvaders-gamified-chaos-engineering-tool-for-kubernetes/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt; Eugenio Marzo, Sourcesense&lt;/p&gt;

&lt;p&gt;Some months ago, I released my latest project called KubeInvaders. The
first time I shared it with the community was during an Openshift
Commons Briefing session. Kubenvaders is a Gamified Chaos Engineering
tool for Kubernetes and Openshift and helps test how resilient your
Kubernetes cluster is, in a fun way.&lt;/p&gt;

&lt;p&gt;It is like Space Invaders, but the aliens are pods.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/lucky-sideburn/KubeInvaders-kubernetes-post/raw/master/img1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;During my presentation at Codemotion Milan 2019, I started saying &amp;ldquo;of
course you can do it with few lines of Bash, but it is boring.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/lucky-sideburn/KubeInvaders-kubernetes-post/raw/master/img2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Using the code above you can kill random pods across a Kubernetes cluster, but I
think it is much more fun with the spaceship of KubeInvaders.&lt;/p&gt;

&lt;p&gt;I published the code at
&lt;a href=&#34;https://github.com/lucky-sideburn/KubeInvaders&#34; target=&#34;_blank&#34;&gt;https://github.com/lucky-sideburn/KubeInvaders&lt;/a&gt;
and there is a little community that is growing gradually. Some people
love to use it for demo sessions killing pods on a big screen.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/lucky-sideburn/KubeInvaders-kubernetes-post/raw/master/img3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;how-to-install-kubeinvaders&#34;&gt;How to install KubeInvaders&lt;/h2&gt;

&lt;p&gt;I defined multiples modes to install it:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Helm Chart
&lt;a href=&#34;https://github.com/lucky-sideburn/KubeInvaders/tree/master/helm-charts/kubeinvaders&#34; target=&#34;_blank&#34;&gt;https://github.com/lucky-sideburn/KubeInvaders/tree/master/helm-charts/kubeinvaders&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Manual Installation for Openshift using a template
&lt;a href=&#34;https://github.com/lucky-sideburn/KubeInvaders#install-kubeinvaders-on-openshift&#34; target=&#34;_blank&#34;&gt;https://github.com/lucky-sideburn/KubeInvaders#install-kubeinvaders-on-openshift&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Manual Installation for Kubernetes
&lt;a href=&#34;https://github.com/lucky-sideburn/KubeInvaders#install-kubeinvaders-on-kubernetes&#34; target=&#34;_blank&#34;&gt;https://github.com/lucky-sideburn/KubeInvaders#install-kubeinvaders-on-kubernetes&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The preferred way, of course, is with a Helm chart:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  # Please set target_namespace to set your target namespace!
  helm install --set-string target_namespace=&amp;quot;namespace1,namespace2&amp;quot; \
  --name kubeinvaders --namespace kubeinvaders ./helm-charts/kubeinvaders
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;how-to-use-kubeinvaders&#34;&gt;How to use KubeInvaders&lt;/h2&gt;

&lt;p&gt;Once it is installed on your cluster you can use the following
functionalities:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Key &amp;lsquo;a&amp;rsquo; — Switch to automatic pilot&lt;/li&gt;
&lt;li&gt;Key &amp;rsquo;m&amp;rsquo; — Switch to manual pilot&lt;/li&gt;
&lt;li&gt;Key &amp;lsquo;i&amp;rsquo; — Show pod&amp;rsquo;s name. Move the ship towards an alien&lt;/li&gt;
&lt;li&gt;Key &amp;lsquo;h&amp;rsquo; — Print help&lt;/li&gt;
&lt;li&gt;Key &amp;lsquo;n&amp;rsquo; — Jump between different namespaces (my favorite feature!)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;tuning-kubeinvaders&#34;&gt;Tuning KubeInvaders&lt;/h2&gt;

&lt;p&gt;At Codemotion Milan 2019, my colleagues and I organized a desk with a
game station for playing KubeInvaders. People had to fight with Kubernetes to
win a t-shirt.&lt;/p&gt;

&lt;p&gt;If you have pods that require a few seconds to start, you may lose. It
is possible to set the complexity of the game with these parameters as
environmment variables in the Kubernetes deployment:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ALIENPROXIMITY — Reduce this value to increase the distance between aliens;&lt;/li&gt;
&lt;li&gt;HITSLIMIT — Seconds of CPU time to wait before shooting;&lt;/li&gt;
&lt;li&gt;UPDATETIME — Seconds to wait before updating pod status (you can set also 0.x Es: 0.5);&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The result is a harder game experience against the machine.&lt;/p&gt;

&lt;h2 id=&#34;use-cases&#34;&gt;Use cases&lt;/h2&gt;

&lt;p&gt;Adopting chaos engineering strategies for your production environment is
really useful, because it is the only way to test if a system supports
unexpected destructive events.&lt;/p&gt;

&lt;p&gt;KubeInvaders is a game — so please do not take it too seriously! — but it demonstrates
some important use cases:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Test how resilient Kubernetes clusters are on unexpected pod deletion&lt;/li&gt;
&lt;li&gt;Collect metrics like pod restart time&lt;/li&gt;
&lt;li&gt;Tune readiness probes&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;next-steps&#34;&gt;Next steps&lt;/h2&gt;

&lt;p&gt;I want to continue to add some cool features and integrate it into a
Kubernetes dashboard because I am planning to transform it into a
&amp;ldquo;Gamified Chaos Engineering and Development Tool for Kubernetes&amp;rdquo;, to help
developer to interact with deployments in a Kubernetes environment. For
example:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Point to the aliens to get pod logs&lt;/li&gt;
&lt;li&gt;Deploy Helm charts by shooting some particular objects&lt;/li&gt;
&lt;li&gt;Read messages stored in a specific label present in a deployment&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Please feel free to contribute to
&lt;a href=&#34;https://github.com/lucky-sideburn/KubeInvaders&#34; target=&#34;_blank&#34;&gt;https://github.com/lucky-sideburn/KubeInvaders&lt;/a&gt;
and stay updated following #kubeinvaders news &lt;a href=&#34;https://twitter.com/luckysideburn&#34; target=&#34;_blank&#34;&gt;on Twitter&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: CSI Ephemeral Inline Volumes</title>
      <link>https://kubernetes.io/blog/2020/01/21/csi-ephemeral-inline-volumes/</link>
      <pubDate>Tue, 21 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2020/01/21/csi-ephemeral-inline-volumes/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author:&lt;/strong&gt; Patrick Ohly (Intel)&lt;/p&gt;

&lt;p&gt;Typically, volumes provided by an external storage driver in
Kubernetes are &lt;em&gt;persistent&lt;/em&gt;, with a lifecycle that is completely
independent of pods or (as a special case) loosely coupled to the
first pod which uses a volume (&lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode&#34; target=&#34;_blank&#34;&gt;late binding
mode&lt;/a&gt;).
The mechanism for requesting and defining such volumes in Kubernetes
are &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/persistent-volumes/&#34; target=&#34;_blank&#34;&gt;Persistent Volume Claim (PVC) and Persistent Volume
(PV)&lt;/a&gt;
objects. Originally, volumes that are backed by a Container Storage Interface
(CSI) driver could only be used via this PVC/PV mechanism.&lt;/p&gt;

&lt;p&gt;But there are also use cases for data volumes whose content and
lifecycle is tied to a pod. For example, a driver might populate a
volume with dynamically created secrets that are specific to the
application running in the pod. Such volumes need to be created
together with a pod and can be deleted as part of pod termination
(&lt;em&gt;ephemeral&lt;/em&gt;). They get defined as part of the pod spec (&lt;em&gt;inline&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;Since Kubernetes 1.15, CSI drivers can also be used for such
&lt;em&gt;ephemeral inline&lt;/em&gt; volumes. The &lt;a href=&#34;https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/&#34; target=&#34;_blank&#34;&gt;CSIInlineVolume feature
gate&lt;/a&gt;
had to be set to enable it in 1.15 because support was still in alpha
state. In 1.16, the feature reached beta state, which typically means
that it is enabled in clusters by default.&lt;/p&gt;

&lt;p&gt;CSI drivers have to be adapted to support this because although two
existing CSI gRPC calls are used (&lt;code&gt;NodePublishVolume&lt;/code&gt; and &lt;code&gt;NodeUnpublishVolume&lt;/code&gt;),
the way how they are
used is different and not covered by the CSI spec: for ephemeral
volumes, only &lt;code&gt;NodePublishVolume&lt;/code&gt; is invoked by &lt;code&gt;kubelet&lt;/code&gt; when asking
the CSI driver for a volume. All other calls
(like &lt;code&gt;CreateVolume&lt;/code&gt;, &lt;code&gt;NodeStageVolume&lt;/code&gt;, etc.) are skipped. The volume
parameters are provided in the pod spec and from there copied into the
&lt;code&gt;NodePublishVolumeRequest.volume_context&lt;/code&gt; field. There are currently
no standardized parameters; even common ones like size must be
provided in a format that is defined by the CSI driver. Likewise, only
&lt;code&gt;NodeUnpublishVolume&lt;/code&gt; gets called after the pod has terminated and the
volume needs to be removed.&lt;/p&gt;

&lt;p&gt;Initially, the assumption was that CSI drivers would be specifically
written to provide either persistent or ephemeral volumes. But there
are also drivers which provide storage that is useful in both modes:
for example, &lt;a href=&#34;https://github.com/intel/pmem-csi&#34; target=&#34;_blank&#34;&gt;PMEM-CSI&lt;/a&gt; manages
persistent memory (PMEM), a new kind of local storage that is provided
by &lt;a href=&#34;https://www.intel.com/content/www/us/en/architecture-and-technology/optane-dc-persistent-memory.html&#34; target=&#34;_blank&#34;&gt;Intel® Optane™ DC Persistent
Memory&lt;/a&gt;. Such
memory is useful both as persistent data storage (faster than normal SSDs)
and as ephemeral scratch space (higher capacity than DRAM).&lt;/p&gt;

&lt;p&gt;Therefore the support in Kubernetes 1.16 was extended:
* Kubernetes and users can determine which kind of volumes a driver
  supports via the &lt;code&gt;volumeLifecycleModes&lt;/code&gt; field in the &lt;a href=&#34;https://kubernetes-csi.github.io/docs/csi-driver-object.html#what-fields-does-the-csidriver-object-have&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;CSIDriver&lt;/code&gt;
  object&lt;/a&gt;.
* Drivers can get information about the volume mode by enabling the
  &lt;a href=&#34;https://kubernetes-csi.github.io/docs/pod-info.html&#34; target=&#34;_blank&#34;&gt;&amp;ldquo;pod info on
  mount&amp;rdquo;&lt;/a&gt; feature
  which then will add the new &lt;code&gt;csi.storage.k8s.io/ephemeral&lt;/code&gt; entry to
  the &lt;code&gt;NodePublishRequest.volume_context&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;For more information about implementing support of ephemeral inline
volumes in a CSI driver, see the &lt;a href=&#34;https://kubernetes-csi.github.io/docs/ephemeral-local-volumes.html&#34; target=&#34;_blank&#34;&gt;Kubernetes-CSI
documentation&lt;/a&gt;
and the &lt;a href=&#34;https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/20190122-csi-inline-volumes.md&#34; target=&#34;_blank&#34;&gt;original design
document&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;What follows in this blog post are usage examples based on real drivers
and a summary at the end.&lt;/p&gt;

&lt;h1 id=&#34;examples&#34;&gt;Examples&lt;/h1&gt;

&lt;h2 id=&#34;pmem-csi-https-github-com-intel-pmem-csi&#34;&gt;&lt;a href=&#34;https://github.com/intel/pmem-csi&#34; target=&#34;_blank&#34;&gt;PMEM-CSI&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Support for ephemeral inline volumes was added in &lt;a href=&#34;https://github.com/intel/pmem-csi/releases/tag/v0.6.0&#34; target=&#34;_blank&#34;&gt;release
v0.6.0&lt;/a&gt;. The
driver can be used on hosts with real Intel® Optane™ DC Persistent
Memory, on &lt;a href=&#34;https://github.com/intel/pmem-csi/blob/v0.6.0/examples/gce.md&#34; target=&#34;_blank&#34;&gt;special machines in
GCE&lt;/a&gt; or
with hardware emulated by QEMU. The latter is fully &lt;a href=&#34;https://github.com/intel/pmem-csi/tree/v0.6.0#qemu-and-kubernetes&#34; target=&#34;_blank&#34;&gt;integrated into
the
makefile&lt;/a&gt;
and only needs Go, Docker and KVM, so that approach was used for this
example:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;git clone --branch release-0.6 https://github.com/intel/pmem-csi
&lt;span style=&#34;color:#a2f&#34;&gt;cd&lt;/span&gt; pmem-csi
&lt;span style=&#34;color:#b8860b&#34;&gt;TEST_DISTRO&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;clear &lt;span style=&#34;color:#b8860b&#34;&gt;TEST_DISTRO_VERSION&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;32080&lt;/span&gt; &lt;span style=&#34;color:#b8860b&#34;&gt;TEST_PMEM_REGISTRY&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;intel make start&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Bringing up the four-node cluster can take a while but eventually should end with:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;The test cluster is ready. Log in with /work/pmem-csi/_work/pmem-govm/ssh-pmem-govm, run kubectl once logged in.
Alternatively, KUBECONFIG=/work/pmem-csi/_work/pmem-govm/kube.config can also be used directly.

To try out the pmem-csi driver persistent volumes:
...

To try out the pmem-csi driver ephemeral volumes:
   cat deploy/kubernetes-1.17/pmem-app-ephemeral.yaml | /work/pmem-csi/_work/pmem-govm/ssh-pmem-govm kubectl create -f -
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;deploy/kubernetes-1.17/pmem-app-ephemeral.yaml&lt;/code&gt; specifies one volume:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kind: Pod
apiVersion: v1
metadata:
  name: my-csi-app-inline-volume
spec:
  containers:
    - name: my-frontend
      image: busybox
      command: [ &amp;quot;sleep&amp;quot;, &amp;quot;100000&amp;quot; ]
      volumeMounts:
      - mountPath: &amp;quot;/data&amp;quot;
        name: my-csi-volume
  volumes:
  - name: my-csi-volume
    csi:
      driver: pmem-csi.intel.com
      fsType: &amp;quot;xfs&amp;quot;
      volumeAttributes:
        size: &amp;quot;2Gi&amp;quot;
        nsmode: &amp;quot;fsdax&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once we have created that pod, we can inspect the result:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl describe pods/my-csi-app-inline-volume&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;Name:         my-csi-app-inline-volume
...
Volumes:
  my-csi-volume:
    Type:              CSI (a Container Storage Interface (CSI) volume source)
    Driver:            pmem-csi.intel.com
    FSType:            xfs
    ReadOnly:          false
    VolumeAttributes:      nsmode=fsdax
                           size=2Gi
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl &lt;span style=&#34;color:#a2f&#34;&gt;exec&lt;/span&gt; my-csi-app-inline-volume -- df -h /data&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;Filesystem                Size      Used Available Use% Mounted on
/dev/ndbus0region0fsdax/d7eb073f2ab1937b88531fce28e19aa385e93696
                          1.9G     34.2M      1.8G   2% /data
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;image-populator-https-github-com-kubernetes-csi-csi-driver-image-populator&#34;&gt;&lt;a href=&#34;https://github.com/kubernetes-csi/csi-driver-image-populator&#34; target=&#34;_blank&#34;&gt;Image Populator&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;The image populator automatically unpacks a container image and makes
its content available as an ephemeral volume. It&amp;rsquo;s still in
development, but canary images are already available which can be
installed with:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl create -f https://github.com/kubernetes-csi/csi-driver-image-populator/raw/master/deploy/kubernetes-1.16/csi-image-csidriverinfo.yaml
kubectl create -f https://github.com/kubernetes-csi/csi-driver-image-populator/raw/master/deploy/kubernetes-1.16/csi-image-daemonset.yaml&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This example pod will run nginx and have it serve data that
comes from the &lt;code&gt;kfox1111/misc:test&lt;/code&gt; image:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl create -f - &lt;span style=&#34;color:#b44&#34;&gt;&amp;lt;&amp;lt;EOF
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;apiVersion: v1
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;kind: Pod
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;metadata:
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;  name: nginx
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;spec:
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;  containers:
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;  - name: nginx
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;    image: nginx:1.16-alpine
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;    ports:
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;    - containerPort: 80
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;    volumeMounts:
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;    - name: data
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;      mountPath: /usr/share/nginx/html
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;  volumes:
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;  - name: data
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;    csi:
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;      driver: image.csi.k8s.io
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;      volumeAttributes:
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;          image: kfox1111/misc:test
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;EOF&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl &lt;span style=&#34;color:#a2f&#34;&gt;exec&lt;/span&gt; nginx -- cat /usr/share/nginx/html/test&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;That &lt;code&gt;test&lt;/code&gt; file just contains a single word:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;testing
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Such data containers can be built with Dockerfiles such as:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;FROM scratch
COPY index.html /index.html
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;cert-manager-csi-https-github-com-jetstack-cert-manager-csi&#34;&gt;&lt;a href=&#34;https://github.com/jetstack/cert-manager-csi&#34; target=&#34;_blank&#34;&gt;cert-manager-csi&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;cert-manager-csi works together with
&lt;a href=&#34;https://github.com/jetstack/cert-manager&#34; target=&#34;_blank&#34;&gt;cert-manager&lt;/a&gt;. The goal for
this driver is to facilitate requesting and mounting certificate key
pairs to pods seamlessly. This is useful for facilitating mTLS, or
otherwise securing connections of pods with guaranteed present
certificates whilst having all of the features that cert-manager
provides. This project is experimental.&lt;/p&gt;

&lt;h1 id=&#34;next-steps&#34;&gt;Next steps&lt;/h1&gt;

&lt;p&gt;One of the issues with ephemeral inline volumes is that pods get
scheduled by Kubernetes onto nodes without knowing anything about the
currently available storage on that node. Once the pod has been
scheduled, the CSI driver must make the volume available one that
node. If that is currently not possible, the pod cannot start. This
will be retried until eventually the volume becomes ready. The
&lt;a href=&#34;https://github.com/kubernetes/enhancements/pull/1353&#34; target=&#34;_blank&#34;&gt;storage capacity tracking
KEP&lt;/a&gt; is an
attempt to address this problem.&lt;/p&gt;

&lt;p&gt;A related KEP introduces a &lt;a href=&#34;https://github.com/kubernetes/enhancements/pull/1409&#34; target=&#34;_blank&#34;&gt;standardized size
parameter&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Currently, CSI ephemeral inline volumes stay in beta while issues like
these are getting discussed. Your feedback is needed to decide how to
proceed with this feature. For the KEPs, the two PRs linked to above
is a good place to comment. The SIG Storage also &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-storage#meetings&#34; target=&#34;_blank&#34;&gt;meets
regularly&lt;/a&gt;
and can be reached via &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-storage#contact&#34; target=&#34;_blank&#34;&gt;Slack and a mailing
list&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Reviewing 2019 in Docs</title>
      <link>https://kubernetes.io/blog/2020/01/21/reviewing-2019-in-docs/</link>
      <pubDate>Tue, 21 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2020/01/21/reviewing-2019-in-docs/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author:&lt;/strong&gt; Zach Corleissen (Cloud Native Computing Foundation)&lt;/p&gt;

&lt;p&gt;Hi, folks! I&amp;rsquo;m one of the co-chairs for the Kubernetes documentation special interest group (SIG Docs). This blog post is a review of SIG Docs in 2019. Our contributors did amazing work last year, and I want to highlight their successes.&lt;/p&gt;

&lt;p&gt;Although I review 2019 in this post, my goal is to point forward to 2020. I observe some trends in SIG Docs–some good, others troubling. I want to raise visibility before those challenges increase in severity.&lt;/p&gt;

&lt;h2 id=&#34;the-good&#34;&gt;The good&lt;/h2&gt;

&lt;p&gt;There was much to celebrate in SIG Docs in 2019.&lt;/p&gt;

&lt;p&gt;Kubernetes docs started the year with three localizations in progress. By the end of the year, we ended with ten localizations available, four of which (Chinese, French, Japanese, Korean) are reasonably complete. The Korean and French teams deserve special mentions for their contributions to git best practices across all localizations (Korean team) and help bootstrapping other localizations (French team).&lt;/p&gt;

&lt;p&gt;Despite significant transition over the year, SIG Docs &lt;a href=&#34;https://k8s.devstats.cncf.io/d/44/pr-time-to-approve-and-merge?orgId=1&amp;amp;var-period=w&amp;amp;var-repogroup_name=SIG%20Docs&amp;amp;var-apichange=All&amp;amp;var-size_name=All&amp;amp;var-kind_name=All&#34; target=&#34;_blank&#34;&gt;improved its review velocity&lt;/a&gt;, with a median review time from PR open to merge of just over 24 hours.&lt;/p&gt;

&lt;p&gt;Issue triage improved significantly in both volume and speed, largely due to the efforts of GitHub users @sftim, @tengqm, and @kbhawkey.&lt;/p&gt;

&lt;p&gt;Doc sprints remain valuable at KubeCon contributor days, introducing new contributors to Kubernetes documentation.&lt;/p&gt;

&lt;p&gt;The docs component of Kubernetes quarterly releases improved over 2019, thanks to iterative playbook improvements from release leads and their teams.&lt;/p&gt;

&lt;p&gt;Site traffic increased over the year. The website ended the year with ~6 million page views per month in December, up from ~5M page views in January. The kubernetes.io website had 851k site visitors in October, a new all-time high. Reader satisfaction &lt;a href=&#34;https://kubernetes.io/blog/2019/10/29/kubernetes-documentation-end-user-survey/&#34; target=&#34;_blank&#34;&gt;remains general&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We onboarded a new SIG chair: @jimangel, a Cloud Architect at General Motors. Jim was a docs contributor for a year, during which he led the 1.14 docs release, before stepping up as chair.&lt;/p&gt;

&lt;h2 id=&#34;the-not-so-good&#34;&gt;The not so good&lt;/h2&gt;

&lt;p&gt;While reader satisfaction is decent, &lt;strong&gt;most respondents indicated dissatisfaction with stale content&lt;/strong&gt; in every area: concepts, tasks, tutorials, and reference. Additionally, readers requested more diagrams, advanced conceptual content, and code samples&amp;mdash;things that technical writers excel at providing.&lt;/p&gt;

&lt;p&gt;SIG Docs continues to solve how best to handle &lt;a href=&#34;https://github.com/kubernetes/enhancements/pull/1327&#34; target=&#34;_blank&#34;&gt;third-party content&lt;/a&gt;. &lt;strong&gt;There&amp;rsquo;s too much vendor content on kubernetes.io&lt;/strong&gt;, and guidelines for adding or rejecting third-party content remain unclear. The discussion so far has been powerful, including pushback demanding greater collaborative input&amp;mdash;a powerful reminder that Kubernetes is in all ways a communal effort.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;re in the middle of our third chair transition in 18 months. Each chair transition has been healthy and collegial, but it&amp;rsquo;s still a lot of turnover in a short time. Chairing any open source project is difficult, but especially so with SIG Docs. Chairship of SIG Docs requires a steep learning curve across multiple domains: docs (both written and generated from spec), information architecture, specialized contribution paths (for example, localization), how to run a release cycle, website development, CI/CD, community management, on and on. It&amp;rsquo;s a role that requires multiple people to function successfully without burning people out. Training replacements is time-intensive.&lt;/p&gt;

&lt;p&gt;Perhaps most pressing in the Not So Good category is that SIG Docs currently has only one technical writer dedicated full-time to Kubernetes docs. This has impacts on Kubernetes docs: some obvious, some less so.&lt;/p&gt;

&lt;h2 id=&#34;impacts-of-understaffing-on-kubernetes-docs&#34;&gt;Impacts of understaffing on Kubernetes docs&lt;/h2&gt;

&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Me today: &lt;a href=&#34;https://t.co/cDpHOWEsjf&#34;&gt;pic.twitter.com/cDpHOWEsjf&lt;/a&gt;&lt;/p&gt;&amp;mdash; Benjamin Elder (@BenTheElder) &lt;a href=&#34;https://twitter.com/BenTheElder/status/1215453579651104768?ref_src=twsrc%5Etfw&#34;&gt;January 10, 2020&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;If Kubernetes continues through 2020 without more technical writers dedicated to the docs, here&amp;rsquo;s what I see as the most likely possibilities.&lt;/p&gt;

&lt;h3 id=&#34;but-first-a-disclaimer&#34;&gt;But first, a disclaimer&lt;/h3&gt;

&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;Caution:&lt;/strong&gt; It is very hard to predict, especially the future.
-Niels Bohr&lt;/div&gt;
&lt;/blockquote&gt;


&lt;p&gt;Some of my predictions are almost certainly wrong. Any errors are mine alone.&lt;/p&gt;

&lt;p&gt;That said&amp;hellip;&lt;/p&gt;

&lt;h3 id=&#34;effects-in-2020&#34;&gt;Effects in 2020&lt;/h3&gt;

&lt;p&gt;Current levels of function aren&amp;rsquo;t self-sustaining. Even with a strong playbook, the release cycle still requires expert support from at least one (and usually two) chairs during every cycle. Without fail, each release breaks in new and unexpected ways, and it requires familiarity and expertise to diagnose and resolve. As chairs continue to cycle&amp;mdash;and to be clear, regular transitions are part of a healthy project&amp;mdash;we accrue the risks associated with a pool lacking sufficient professional depth and employer support.&lt;/p&gt;

&lt;p&gt;Oddly enough, one of the challenges to staffing is that the docs appear good enough. Based on site analytics and survey responses, readers are pleased with the quality of the docs. When folks visit the site, they generally find what they need and behave like satisfied visitors.&lt;/p&gt;

&lt;p&gt;The danger is that this will change over time: slowly with occasional losses of function, annoying at first, then increasingly critical. The more time passes without adequate staffing, the more difficult and costly fixes will become.&lt;/p&gt;

&lt;p&gt;I suspect this is true because the challenges we face now at decent levels of reader satisfaction are already difficult to fix. API reference generation is complex and brittle; the site&amp;rsquo;s UI is outdated; and our most consistent requests are for more tutorials, advanced concepts, diagrams, and code samples, all of which require ongoing, dedicated time to create.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Release support remains strong.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The release team continues a solid habit of leaving each successive team with better support than the previous release. This mostly takes the form of iterative improvements to the &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-release#docs-lead&#34; target=&#34;_blank&#34;&gt;docs release playbook&lt;/a&gt;, producing better documentation and reducing siloed knowledge.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Staleness accelerates.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Conceptual content becomes less accurate or relevant as features change or deprecate. Tutorial content degrades for the same reason.&lt;/p&gt;

&lt;p&gt;The content structure will also degrade: the categories of concepts, tasks, and tutorials are legacy categories that may not best fit the needs of current readers, let alone future ones.&lt;/p&gt;

&lt;p&gt;Cruft accumulates for both readers and contributors. Reference docs become increasingly brittle without intervention.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Critical knowledge vanishes.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As I mentioned previously, SIG Docs has a wide range of functions, some with a steep learning curve. As contributors change roles or jobs, their expertise and availability will diminish or reduce to zero. Contributors with specific knowledge may not be available for consultation, exposing critical vulnerabilities in docs function. Specific examples include reference generation and chair leadership.&lt;/p&gt;

&lt;h3 id=&#34;that-s-a-lot-to-take-in&#34;&gt;That&amp;rsquo;s a lot to take in&lt;/h3&gt;

&lt;p&gt;It&amp;rsquo;s difficult to strike a balance between the importance of SIG Docs&amp;rsquo; work to the community and our users, the joy it brings me personally, and the fact that things can&amp;rsquo;t remain as they are without significant negative impacts (eventually). SIG Docs is by no means dying; it&amp;rsquo;s a vibrant community with active contributors doing cool things. It&amp;rsquo;s also a community with some critical knowledge and capacity shortages that can only be remedied with trained, paid staff dedicated to documentation.&lt;/p&gt;

&lt;h2 id=&#34;what-the-community-can-do-for-healthy-docs&#34;&gt;What the community can do for healthy docs&lt;/h2&gt;

&lt;p&gt;Hire technical writers dedicated to Kubernetes docs. Support advanced content creation, not just release docs and incremental feature updates.&lt;/p&gt;

&lt;p&gt;Thanks, and Happy 2020.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes on MIPS</title>
      <link>https://kubernetes.io/blog/2020/01/15/kubernetes-on-mips/</link>
      <pubDate>Wed, 15 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2020/01/15/kubernetes-on-mips/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; TimYin Shi, Dominic Yin, Wang Zhan, Jessica Jiang, Will Cai, Jeffrey Gao, Simon Sun (Inspur)&lt;/p&gt;

&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/MIPS_architecture&#34; target=&#34;_blank&#34;&gt;MIPS&lt;/a&gt; (Microprocessor without Interlocked Pipelined Stages) is a reduced instruction set computer (RISC) instruction set architecture (ISA), appeared in 1981 and developed by MIPS Technologies. Now MIPS architecture is widely used in many electronic products.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kubernetes.io&#34; target=&#34;_blank&#34;&gt;Kubernetes&lt;/a&gt; has officially supported a variety of CPU architectures such as x86, arm/arm64, ppc64le, s390x. However, it&amp;rsquo;s a pity that Kubernetes doesn&amp;rsquo;t support MIPS. With the widespread use of cloud native technology, users under MIPS architecture also have an urgent demand for Kubernetes on MIPS.&lt;/p&gt;

&lt;h2 id=&#34;achievements&#34;&gt;Achievements&lt;/h2&gt;

&lt;p&gt;For many years, to enrich the ecology of the open-source community, we have been working on adjusting MIPS architecture for Kubernetes use cases. With the continuous iterative optimization and the performance improvement of the MIPS CPU, we have made some breakthrough progresses on the mips64el platform.&lt;/p&gt;

&lt;p&gt;Over the years, we have been actively participating in the Kubernetes community and have rich experience in the using and optimization of Kubernetes technology. Recently, we tried to adapt the MIPS architecture platform for Kubernetes and achieved a new a stage on that journey. The team has completed migration and adaptation of Kubernetes and related components, built not only a stable and highly available MIPS cluster but also completed the conformance test for Kubernetes v1.16.2.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2020-01-15-Kubernetes-on-MIPS/kubernetes-on-mips.png&#34; alt=&#34;Kubernetes on MIPS&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 1 Kubernetes on MIPS&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;k8s-mips-component-build&#34;&gt;K8S-MIPS component build&lt;/h2&gt;

&lt;p&gt;Almost all native cloud components related to Kubernetes do not provide a MIPS version installation package or image. The prerequisite of deploying Kubernetes on the MIPS platform is to compile and build all required components on the mips64el platform. These components include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;golang&lt;/li&gt;
&lt;li&gt;docker-ce&lt;/li&gt;
&lt;li&gt;hyperkube&lt;/li&gt;
&lt;li&gt;pause&lt;/li&gt;
&lt;li&gt;etcd&lt;/li&gt;
&lt;li&gt;calico&lt;/li&gt;
&lt;li&gt;coredns&lt;/li&gt;
&lt;li&gt;metrics-server&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thanks to the excellent design of Golang and its good support for the MIPS platform, the compilation processes of the above cloud native components are greatly simplified. First of all, we compiled Golang on the latest stable version for the mips64el platform, and then we compiled most of the above components with source code.&lt;/p&gt;

&lt;p&gt;During the compilation processes, we inevitably encountered many platform compatibility problems, such as a Golang system call compatibility problem (syscall), typecasting of syscall. Stat_t from uint32 to uint64, patching for EpollEvent, and so on.&lt;/p&gt;

&lt;p&gt;To build K8S-MIPS components, we used cross-compilation technology. Our process involved integrating a QEMU tool to translate MIPS CPU instructions and modifying the build script of Kubernetes and E2E image script of Kubernetes, Hyperkube, and E2E test images on MIPS architecture.&lt;/p&gt;

&lt;p&gt;After successfully building the above components, we use tools such as kubespray and kubeadm to complete kubernetes cluster construction.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;th&gt;Version&lt;/th&gt;
&lt;th&gt;MIPS Repository&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;golang on MIPS&lt;/td&gt;
&lt;td&gt;1.12.5&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;docker-ce on MIPS&lt;/td&gt;
&lt;td&gt;18.09.8&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;metrics-server for CKE on MIPS&lt;/td&gt;
&lt;td&gt;0.3.2&lt;/td&gt;
&lt;td&gt;&lt;code&gt;registry.inspurcloud.cn/library/cke/kubernetes/metrics-server-mips64el:v0.3.2&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;etcd for CKE on MIPS&lt;/td&gt;
&lt;td&gt;3.2.26&lt;/td&gt;
&lt;td&gt;&lt;code&gt;registry.inspurcloud.cn/library/cke/etcd/etcd-mips64el:v3.2.26&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;pause for CKE on MIPS&lt;/td&gt;
&lt;td&gt;3.1&lt;/td&gt;
&lt;td&gt;&lt;code&gt;registry.inspurcloud.cn/library/cke/kubernetes/pause-mips64el:3.1&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;hyperkube for CKE on MIPS&lt;/td&gt;
&lt;td&gt;1.14.3&lt;/td&gt;
&lt;td&gt;&lt;code&gt;registry.inspurcloud.cn/library/cke/kubernetes/hyperkube-mips64el:v1.14.3&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;coredns for CKE on MIPS&lt;/td&gt;
&lt;td&gt;1.6.5&lt;/td&gt;
&lt;td&gt;&lt;code&gt;registry.inspurcloud.cn/library/cke/kubernetes/coredns-mips64el:v1.6.5&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;calico for CKE on MIPS&lt;/td&gt;
&lt;td&gt;3.8.0&lt;/td&gt;
&lt;td&gt;&lt;code&gt;registry.inspurcloud.cn/library/cke/calico/cni-mips64el:v3.8.0&lt;/code&gt; &lt;code&gt;registry.inspurcloud.cn/library/cke/calico/ctl-mips64el:v3.8.0&lt;/code&gt; &lt;code&gt;registry.inspurcloud.cn/library/cke/calico/node-mips64el:v3.8.0&lt;/code&gt; &lt;code&gt;registry.inspurcloud.cn/library/cke/calico/kube-controllers-mips64el:v3.8.0&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: CKE is a Kubernetes-based cloud container engine launched by Inspur&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2020-01-15-Kubernetes-on-MIPS/k8s-mips-cluster-components.png&#34; alt=&#34;K8S-MIPS Cluster Components&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 2 K8S-MIPS Cluster Components&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2020-01-15-Kubernetes-on-MIPS/cpu-architecture.png&#34; alt=&#34;CPU Architecture&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 3 CPU Architecture&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2020-01-15-Kubernetes-on-MIPS/cluster-node-information.png&#34; alt=&#34;Cluster Node Information&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 4 Cluster Node Information&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;run-k8s-conformance-test&#34;&gt;Run K8S Conformance Test&lt;/h2&gt;

&lt;p&gt;The most straightforward way to verify the stability and availability of the K8S-MIPS cluster is to run a Kubernetes &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/v1.16.2/cluster/images/conformance/README.md&#34; target=&#34;_blank&#34;&gt;conformance test&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Conformance is a standalone container to launch Kubernetes end-to-end tests for conformance testing.&lt;/p&gt;

&lt;p&gt;Once the test has started, it launches several pods for various end-to-end tests. The source code of those images used by these pods is mostly from &lt;code&gt;kubernetes/test/images&lt;/code&gt;, and the built images are at &lt;code&gt;gcr.io/kubernetes-e2e-test-images&lt;/code&gt;. Since there are no MIPS images in the repository, we must first build all needed images to run the test.&lt;/p&gt;

&lt;h3 id=&#34;build-needed-images-for-test&#34;&gt;Build needed images for test&lt;/h3&gt;

&lt;p&gt;The first step is to find all needed images for the test. We can run &lt;code&gt;sonobuoy images-p e2e&lt;/code&gt; command to list all images, or we can find those images in &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/test/utils/image/manifest.go&#34; target=&#34;_blank&#34;&gt;/test/utils/image/manifest.go&lt;/a&gt;. Although Kubernetes officially has a complete Makefile and shell-script that provides commands for building test images, there are still a number of architecture-related issues that have not been resolved, such as the incompatibilities of base images and dependencies. So we cannot directly build mips64el architecture images by executing these commands.&lt;/p&gt;

&lt;p&gt;Most test images are in golang, then compiled into binaries and built as Docker image based on the corresponding Dockerfile. These images are easy to build. But note that most images are using alpine as their base image, which does not officially support mips64el architecture for now. For this moment, we are unable to make mips64el version of &lt;a href=&#34;https://www.alpinelinux.org/&#34; target=&#34;_blank&#34;&gt;alpine&lt;/a&gt;, so we have to replace the alpine to existing MIPS images, such as Debian-stretch, fedora, ubuntu. Replacing the base image also requires replacing the command to install the dependencies, even the version of these dependencies.&lt;/p&gt;

&lt;p&gt;Some images are not in &lt;code&gt;kubernetes/test/images&lt;/code&gt;, such as &lt;code&gt;gcr.io/google-samples/gb-frontend:v6&lt;/code&gt;. There is no clear documentation explaining where these images are locaated, though we found the source code in repository &lt;a href=&#34;https://github.com/GoogleCloudPlatform/kubernetes-engine-samples&#34; target=&#34;_blank&#34;&gt;github.com/GoogleCloudPlatform/kubernetes-engine-samples&lt;/a&gt;. We soon ran into new problems: to build these google sample images, we have to build the base image it uses, even the base image of the base images, such as &lt;code&gt;php:5-apache&lt;/code&gt;, &lt;code&gt;redis&lt;/code&gt;, and &lt;code&gt;perl&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;After a long process of building an image, we finished with about four dozen images, including the images used by the test pod, and the base images. The last step before we run the tests is to place all those images into every node in the cluster and make sure the Pod image pull policy is &lt;code&gt;imagePullPolicy: ifNotPresent&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Here are some of the images we built：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;docker.io/library/busybox:1.29&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;docker.io/library/nginx:1.14-alpine&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;docker.io/library/nginx:1.15-alpine&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;docker.io/library/perl:5.26&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;docker.io/library/httpd:2.4.38-alpine&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;docker.io/library/redis:5.0.5-alpine&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gcr.io/google-containers/conformance:v1.16.2&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gcr.io/google-containers/hyperkube:v1.16.2&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gcr.io/google-samples/gb-frontend:v6&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gcr.io/kubernetes-e2e-test-images/agnhost:2.6&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gcr.io/kubernetes-e2e-test-images/apparmor-loader:1.0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gcr.io/kubernetes-e2e-test-images/dnsutils:1.1&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gcr.io/kubernetes-e2e-test-images/echoserver:2.2&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gcr.io/kubernetes-e2e-test-images/ipc-utils:1.0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gcr.io/kubernetes-e2e-test-images/kitten:1.0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gcr.io/kubernetes-e2e-test-images/metadata-concealment:1.2&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gcr.io/kubernetes-e2e-test-images/mounttest:1.0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gcr.io/kubernetes-e2e-test-images/nautilus:1.0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gcr.io/kubernetes-e2e-test-images/nonewprivs:1.0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gcr.io/kubernetes-e2e-test-images/nonroot:1.0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gcr.io/kubernetes-e2e-test-images/resource-consumer-controller:1.0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gcr.io/kubernetes-e2e-test-images/resource-consumer:1.5&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gcr.io/kubernetes-e2e-test-images/sample-apiserver:1.10&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gcr.io/kubernetes-e2e-test-images/test-webserver:1.0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gcr.io/kubernetes-e2e-test-images/volume/gluster:1.0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gcr.io/kubernetes-e2e-test-images/volume/iscsi:2.0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gcr.io/kubernetes-e2e-test-images/volume/nfs:1.0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gcr.io/kubernetes-e2e-test-images/volume/rbd:1.0.1&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;k8s.gcr.io/etcd:3.3.15&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;k8s.gcr.io/pause:3.1&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Finally, we ran the tests and got the test result, include &lt;code&gt;e2e.log&lt;/code&gt;, which showed that all test cases passed. Additionally, we submitted our test result to &lt;a href=&#34;https://github.com/cncf/k8s-conformance&#34; target=&#34;_blank&#34;&gt;k8s-conformance&lt;/a&gt; as a &lt;a href=&#34;https://github.com/cncf/k8s-conformance/pull/779&#34; target=&#34;_blank&#34;&gt;pull request&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2020-01-15-Kubernetes-on-MIPS/pull-request-for-conformance-test-results.png&#34; alt=&#34;Pull request for conformance test results&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 5 Pull request for conformance test results&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;what-s-next&#34;&gt;What&amp;rsquo;s next&lt;/h2&gt;

&lt;p&gt;We built the kubernetes-MIPS component manually and finished the conformance test, which verified the feasibility of Kubernetes On the MIPS platform and greatly enhanced our confidence in promoting the support of the MIPS architecture by Kubernetes.&lt;/p&gt;

&lt;p&gt;In the future, we plan to actively contribute our experience and achievements to the community, submit PR, and patch for MIPS. We hope that more developers and companies in the community join us and promote Kubernetes on MIPS.&lt;/p&gt;

&lt;p&gt;Contribution plan：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;contribute the source of e2e test images for MIPS&lt;/li&gt;
&lt;li&gt;contribute the source of hyperkube for MIPS&lt;/li&gt;
&lt;li&gt;contribute the source of deploy tools like kubeadm for MIPS&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Announcing the Kubernetes bug bounty program</title>
      <link>https://kubernetes.io/blog/2020/01/14/kubernetes-bug-bounty-announcement/</link>
      <pubDate>Tue, 14 Jan 2020 09:00:00 -0800</pubDate>
      
      <guid>https://kubernetes.io/blog/2020/01/14/kubernetes-bug-bounty-announcement/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Maya Kaczorowski and Tim Allclair, Google, on behalf of the &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/committee-product-security&#34; target=&#34;_blank&#34;&gt;Kubernetes Product Security Committee&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Today, the &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/committee-product-security&#34; target=&#34;_blank&#34;&gt;Kubernetes Product Security Committee&lt;/a&gt; is launching a &lt;a href=&#34;https://hackerone.com/kubernetes&#34; target=&#34;_blank&#34;&gt;new bug bounty program&lt;/a&gt;, funded by the &lt;a href=&#34;https://www.cncf.io/&#34; target=&#34;_blank&#34;&gt;CNCF&lt;/a&gt;, to reward researchers finding security vulnerabilities in Kubernetes.&lt;/p&gt;

&lt;h2 id=&#34;setting-up-a-new-bug-bounty-program&#34;&gt;Setting up a new bug bounty program&lt;/h2&gt;

&lt;p&gt;We aimed to set up this bug bounty program as transparently as possible, with &lt;a href=&#34;https://docs.google.com/document/d/1dvlQsOGODhY3blKpjTg6UXzRdPzv5y8V55RD_Pbo7ag/edit#heading=h.7t1efwpev42p&#34; target=&#34;_blank&#34;&gt;an initial proposal&lt;/a&gt;, &lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/73079&#34; target=&#34;_blank&#34;&gt;evaluation of vendors&lt;/a&gt;, and &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/guide/bug-bounty.md&#34; target=&#34;_blank&#34;&gt;working draft of the components in scope&lt;/a&gt;. Once we onboarded the selected bug bounty program vendor, &lt;a href=&#34;https://www.hackerone.com/&#34; target=&#34;_blank&#34;&gt;HackerOne&lt;/a&gt;, these documents were further refined based on the feedback from HackerOne, as well as what was learned in the recent &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/wg-security-audit/findings/Kubernetes%20Final%20Report.pdf&#34; target=&#34;_blank&#34;&gt;Kubernetes security audit&lt;/a&gt;. The bug bounty program has been in a private release for several months now, with invited researchers able to submit bugs and help us test the triage process. After almost two years since the initial proposal, the program is now ready for all security researchers to contribute!&lt;/p&gt;

&lt;p&gt;What’s exciting is that this is rare: a bug bounty for an open-source infrastructure tool. Some open-source bug bounty programs exist, such as the &lt;a href=&#34;https://internetbugbounty.org/&#34; target=&#34;_blank&#34;&gt;Internet Bug Bounty&lt;/a&gt;, this mostly covers core components that are consistently deployed across environments; but most bug bounties are still for hosted web apps. In fact, with more than&lt;a href=&#34;https://www.cncf.io/certification/kcsp/&#34; target=&#34;_blank&#34;&gt; 100 certified distributions of Kubernetes&lt;/a&gt;, the bug bounty program needs to apply to the Kubernetes code that powers all of them. By far, the most time-consuming challenge here has been ensuring that the program provider (HackerOne) and their researchers who do the first line triage have the awareness of Kubernetes and the ability to easily test the validity of a reported bug. As part of the bootstrapping process, HackerOne had their team pass the &lt;a href=&#34;https://www.cncf.io/certification/cka/&#34; target=&#34;_blank&#34;&gt;Certified Kubernetes Administrator&lt;/a&gt; (CKA) exam.&lt;/p&gt;

&lt;h2 id=&#34;what-s-in-scope&#34;&gt;What’s in scope&lt;/h2&gt;

&lt;p&gt;The bug bounty scope covers code from the main Kubernetes organizations on GitHub, as well as continuous integration, release, and documentation artifacts. Basically, most content you’d think of as ‘core’ Kubernetes, included at &lt;a href=&#34;https://github.com/kubernetes&#34; target=&#34;_blank&#34;&gt;https://github.com/kubernetes&lt;/a&gt;, is in scope. We’re particularly interested in cluster attacks, such as privilege escalations, authentication bugs, and remote code execution in the kubelet or API server. Any information leak about a workload, or unexpected permission changes is also of interest. Stepping back from the cluster admin’s view of the world, you’re also encouraged to look at the Kubernetes supply chain, including the build and release processes, which would allow any unauthorized access to commits, or the ability to publish unauthorized artifacts.&lt;/p&gt;

&lt;p&gt;Notably out of scope is the community management tooling, e.g., the Kubernetes mailing lists or Slack channel. Container escapes, attacks on the Linux kernel, or other dependencies, such as etcd, are also out of scope and should be reported to the appropriate party. We would still appreciate that any Kubernetes vulnerability, even if not in scope for the bug bounty, be &lt;a href=&#34;https://kubernetes.io/docs/reference/issues-security/security/#report-a-vulnerability&#34; target=&#34;_blank&#34;&gt;disclosed privately&lt;/a&gt; to the Kubernetes Product Security Committee. See the full scope on the &lt;a href=&#34;https://hackerone.com/kubernetes&#34; target=&#34;_blank&#34;&gt;program reporting page&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;how-kubernetes-handles-vulnerabilities-and-disclosures&#34;&gt;How Kubernetes handles vulnerabilities and disclosures&lt;/h2&gt;

&lt;p&gt;Kubernetes’ &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/committee-product-security&#34; target=&#34;_blank&#34;&gt;Product Security Committee&lt;/a&gt; is a group of security-focused maintainers who are responsible for receiving and responding to reports of security issues in Kubernetes. This follows the documented &lt;a href=&#34;https://kubernetes.io/docs/reference/issues-security/security/&#34; target=&#34;_blank&#34;&gt;security vulnerability response process&lt;/a&gt;, which includes initial triage, assessing impact, generating and rolling out a fix.&lt;/p&gt;

&lt;p&gt;With our bug bounty program, initial triage and initial assessment are handled by the bug bounty provider, in this case, HackerOne, enabling us better scale our limited Kubernetes security experts to handle only valid reports. Nothing else in this process is changing - the Product Security Committee will continue to develop fixes, build private patches, and coordinate special security releases. New releases with security patches will be announced at &lt;a href=&#34;https://groups.google.com/forum/#!forum/kubernetes-security-announce&#34; target=&#34;_blank&#34;&gt;kubernetes-security-announce@googlegroups.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you want to report a bug, you don’t need to use the bug bounty - you can still follow the &lt;a href=&#34;https://kubernetes.io/docs/reference/issues-security/security/#report-a-vulnerability&#34; target=&#34;_blank&#34;&gt;existing process&lt;/a&gt; and report what you’ve found at &lt;a href=&#34;mailto:security@kubernetes.io&#34; target=&#34;_blank&#34;&gt;security@kubernetes.io&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;get-started&#34;&gt;Get started&lt;/h2&gt;

&lt;p&gt;Just as many organizations support open source by hiring developers, paying bug bounties directly supports security researchers. This bug bounty is a critical step for Kubernetes to build up its community of security researchers and reward their hard work.&lt;/p&gt;

&lt;p&gt;If you’re a security researcher, and new to Kubernetes, check out these resources to learn more and get started bug hunting:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Hardening guides&lt;/strong&gt;

&lt;ul&gt;
&lt;li&gt;Kubernetes.io: &lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/&#34; target=&#34;_blank&#34;&gt;https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Frameworks&lt;/strong&gt;

&lt;ul&gt;
&lt;li&gt;CIS benchmarks: &lt;a href=&#34;https://www.cisecurity.org/benchmark/kubernetes/&#34; target=&#34;_blank&#34;&gt;https://www.cisecurity.org/benchmark/kubernetes/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;NIST 800-190: &lt;a href=&#34;https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf&#34; target=&#34;_blank&#34;&gt;https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Talks&lt;/strong&gt;

&lt;ul&gt;
&lt;li&gt;The Devil in the Details: Kubernetes’ First Security Assessment (KubeCon NA 2019): &lt;a href=&#34;https://www.youtube.com/watch?v=vknE5XEa_Do&#34; target=&#34;_blank&#34;&gt;https://www.youtube.com/watch?v=vknE5XEa_Do&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Crafty Requests: Deep Dive into Kubernetes CVE-2018-1002105 (KubeCon EU 2019): &lt;a href=&#34;https://www.youtube.com/watch?v=VjSJqc13PNk&#34; target=&#34;_blank&#34;&gt;https://www.youtube.com/watch?v=VjSJqc13PNk&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A Hacker’s Guide to Kubernetes and the Cloud (KubeCon EU 2018): &lt;a href=&#34;https://www.youtube.com/watch?v=dxKpCO2dAy8&#34; target=&#34;_blank&#34;&gt;https://www.youtube.com/watch?v=dxKpCO2dAy8&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Shipping in pirate-infested waters (KubeCon NA 2017): &lt;a href=&#34;https://www.youtube.com/watch?v=ohTq0no0ZVU&#34; target=&#34;_blank&#34;&gt;https://www.youtube.com/watch?v=ohTq0no0ZVU&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Hacking and Hardening Kubernetes clusters by example (KubeCon NA 2017): &lt;a href=&#34;https://www.youtube.com/watch?v=vTgQLzeBfRU&#34; target=&#34;_blank&#34;&gt;https://www.youtube.com/watch?v=vTgQLzeBfRU&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you find something, please report a security bug to the Kubernetes bug bounty at &lt;a href=&#34;https://hackerone.com/kubernetes&#34; target=&#34;_blank&#34;&gt;https://hackerone.com/kubernetes&lt;/a&gt;.&lt;/p&gt;

&lt;!-- Docs to Markdown version 1.0β17 --&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Remembering Brad Childs</title>
      <link>https://kubernetes.io/blog/2020/01/10/remembering-brad-childs/</link>
      <pubDate>Fri, 10 Jan 2020 10:00:00 -0800</pubDate>
      
      <guid>https://kubernetes.io/blog/2020/01/10/remembering-brad-childs/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Paul Morie, Red Hat&lt;/p&gt;

&lt;p&gt;Last year, the Kubernetes family lost one of its own. Brad Childs was a
SIG Storage chair and long time contributor to the project. Brad worked on a
number of features in storage and was known as much for his friendliness and
sense of humor as for his technical contributions and leadership.&lt;/p&gt;

&lt;p&gt;We recently spent time remembering Brad at Kubecon NA:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://youtu.be/4eI2PTAJ-sE&#34; target=&#34;_blank&#34;&gt;A Tribute to Bradley Childs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/cncf/memorials/blob/master/bradley-childs.md&#34; target=&#34;_blank&#34;&gt;CNCF Memorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Our hearts go out to Brad’s friends and family and others whose lives he touched
inside and outside the Kubernetes community.&lt;/p&gt;

&lt;p&gt;Thank you for everything, Brad. We’ll miss you.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Testing of CSI drivers</title>
      <link>https://kubernetes.io/blog/2020/01/08/testing-of-csi-drivers/</link>
      <pubDate>Wed, 08 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2020/01/08/testing-of-csi-drivers/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author:&lt;/strong&gt; Patrick Ohly (Intel)&lt;/p&gt;

&lt;p&gt;When developing a &lt;a href=&#34;https://kubernetes-csi.github.io/docs/&#34; target=&#34;_blank&#34;&gt;Container Storage Interface (CSI)
driver&lt;/a&gt;, it is useful to leverage
as much prior work as possible. This includes source code (like the
&lt;a href=&#34;https://github.com/kubernetes-csi/csi-driver-host-path/&#34; target=&#34;_blank&#34;&gt;sample CSI hostpath
driver&lt;/a&gt;) but
also existing tests. Besides saving time, using tests written by
someone else has the advantage that it can point out aspects of the
specification that might have been overlooked otherwise.&lt;/p&gt;

&lt;p&gt;An earlier blog post about &lt;a href=&#34;https://kubernetes.io/blog/2019/03/22/kubernetes-end-to-end-testing-for-everyone/&#34; target=&#34;_blank&#34;&gt;end-to-end
testing&lt;/a&gt;
already showed how to use the &lt;a href=&#34;https://github.com/kubernetes/kubernetes/tree/master/test/e2e/storage/testsuites&#34; target=&#34;_blank&#34;&gt;Kubernetes storage
tests&lt;/a&gt;
for testing of a third-party CSI driver. That
approach makes sense when the goal to also add custom E2E tests, but
depends on quite a bit of effort for setting up and maintaining a test
suite.&lt;/p&gt;

&lt;p&gt;When the goal is to merely run the existing tests, then there are
simpler approaches. This blog post introduces those.&lt;/p&gt;

&lt;h2 id=&#34;sanity-testing&#34;&gt;Sanity testing&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes-csi/csi-test/tree/master/pkg/sanity&#34; target=&#34;_blank&#34;&gt;csi-test
sanity&lt;/a&gt;
ensures that a CSI driver conforms to the CSI specification by calling
the gRPC methods in various ways and checking that the outcome is as
required. Despite
its current hosting under the Kubernetes-CSI organization, it is
completely independent of Kubernetes. Tests connect to a running CSI
driver through its Unix domain socket, so although the tests are
written in Go, the driver itself can be implemented in any language.&lt;/p&gt;

&lt;p&gt;The main
&lt;a href=&#34;https://github.com/kubernetes-csi/csi-test/blob/master/pkg/sanity/README.md&#34; target=&#34;_blank&#34;&gt;README&lt;/a&gt;
explains how to include those tests into an existing Go test
suite. The simpler alternative is to just invoke the &lt;a href=&#34;https://github.com/kubernetes-csi/csi-test/tree/master/cmd/csi-sanity&#34; target=&#34;_blank&#34;&gt;csi-sanity&lt;/a&gt;
command.&lt;/p&gt;

&lt;h3 id=&#34;installation&#34;&gt;Installation&lt;/h3&gt;

&lt;p&gt;Starting with csi-test v3.0.0, you can build the &lt;code&gt;csi-sanity&lt;/code&gt; command
with &lt;code&gt;go get github.com/kubernetes-csi/csi-test/cmd/csi-sanity&lt;/code&gt; and
you&amp;rsquo;ll find the compiled binary in &lt;code&gt;$GOPATH/bin/csi-sanity&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;go get&lt;/code&gt; always builds the latest revision from the master branch. To
build a certain release, &lt;a href=&#34;https://github.com/kubernetes-csi/csi-test/releases&#34; target=&#34;_blank&#34;&gt;get the source
code&lt;/a&gt; and run
&lt;code&gt;make -C cmd/csi-sanity&lt;/code&gt;. This produces &lt;code&gt;cmd/csi-sanity/csi-sanity&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&#34;usage&#34;&gt;Usage&lt;/h3&gt;

&lt;p&gt;The &lt;code&gt;csi-sanity&lt;/code&gt; binary is a full &lt;a href=&#34;http://onsi.github.io/ginkgo/&#34; target=&#34;_blank&#34;&gt;Ginkgo test
suite&lt;/a&gt; and thus has the usual &lt;code&gt;-gingko&lt;/code&gt;
command line flags. In particular, &lt;code&gt;-ginkgo.focus&lt;/code&gt; and
&lt;code&gt;-ginkgo.skip&lt;/code&gt; can be used to select which tests are run resp. not
run.&lt;/p&gt;

&lt;p&gt;During a test run, &lt;code&gt;csi-sanity&lt;/code&gt; simulates the behavior of a container
orchestrator (CO) by creating staging and target directories as required by the CSI spec
and calling a CSI driver via gRPC. The driver must be started before
invoking &lt;code&gt;csi-sanity&lt;/code&gt;. Although the tests currently only check the gRPC
return codes, that might change and so the driver really should make
the changes requested by a call, like mounting a filesystem. That may
mean that it has to run as root.&lt;/p&gt;

&lt;p&gt;At least one &lt;a href=&#34;https://github.com/grpc/grpc/blob/master/doc/naming.md&#34; target=&#34;_blank&#34;&gt;gRPC
endpoint&lt;/a&gt; must
be specified via the &lt;code&gt;-csi.endpoint&lt;/code&gt; parameter when invoking
&lt;code&gt;csi-sanity&lt;/code&gt;, either as absolute path (&lt;code&gt;unix:/tmp/csi.sock&lt;/code&gt;) for a Unix
domain socket or as host name plus port (&lt;code&gt;dns:///my-machine:9000&lt;/code&gt;) for
TCP. &lt;code&gt;csi-sanity&lt;/code&gt; then uses that endpoint for both node and controller
operations. A separate endpoint for controller operations can be
specified with &lt;code&gt;-csi.controllerendpoint&lt;/code&gt;. Directories are created in
&lt;code&gt;/tmp&lt;/code&gt; by default. This can be changed via &lt;code&gt;-csi.mountdir&lt;/code&gt; and
&lt;code&gt;-csi.stagingdir&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Some drivers cannot be deployed such that everything is guaranteed to
run on the same host. In such a case, custom scripts have to be used
to handle directories: they log into the host where the CSI node
controller runs and create or remove the directories there.&lt;/p&gt;

&lt;p&gt;For example, during CI testing the &lt;a href=&#34;https://github.com/kubernetes-csi/csi-driver-host-path&#34; target=&#34;_blank&#34;&gt;CSI hostpath example
driver&lt;/a&gt; gets
deployed on a real Kubernetes cluster before invoking &lt;code&gt;csi-sanity&lt;/code&gt; and then
&lt;code&gt;csi-sanity&lt;/code&gt; connects to it through port forwarding provided by
&lt;a href=&#34;https://github.com/kubernetes-csi/csi-driver-host-path/blob/v1.2.0/deploy/kubernetes-1.16/hostpath/csi-hostpath-testing.yaml&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;socat&lt;/code&gt;&lt;/a&gt;.
&lt;a href=&#34;https://github.com/kubernetes-csi/csi-driver-host-path/blob/v1.2.0/release-tools/prow.sh#L808-L859&#34; target=&#34;_blank&#34;&gt;Scripts&lt;/a&gt;
are used to create and remove the directories.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s how one can replicate that, using the v1.2.0 release of the CSI hostpath driver:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cd csi-driver-host-path
$ git describe --tags HEAD
v1.2.0
$ kubectl get nodes
NAME        STATUS   ROLES    AGE   VERSION
127.0.0.1   Ready    &amp;lt;none&amp;gt;   42m   v1.16.0

$ deploy/kubernetes-1.16/deploy-hostpath.sh 
applying RBAC rules
kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-provisioner/v1.4.0/deploy/kubernetes/rbac.yaml
...
deploying hostpath components
   deploy/kubernetes-1.16/hostpath/csi-hostpath-attacher.yaml
        using           image: quay.io/k8scsi/csi-attacher:v2.0.0
service/csi-hostpath-attacher created
statefulset.apps/csi-hostpath-attacher created
   deploy/kubernetes-1.16/hostpath/csi-hostpath-driverinfo.yaml
csidriver.storage.k8s.io/hostpath.csi.k8s.io created
   deploy/kubernetes-1.16/hostpath/csi-hostpath-plugin.yaml
        using           image: quay.io/k8scsi/csi-node-driver-registrar:v1.2.0
        using           image: quay.io/k8scsi/hostpathplugin:v1.2.0
        using           image: quay.io/k8scsi/livenessprobe:v1.1.0
...
service/hostpath-service created
statefulset.apps/csi-hostpath-socat created
07:38:46 waiting for hostpath deployment to complete, attempt #0
deploying snapshotclass
volumesnapshotclass.snapshot.storage.k8s.io/csi-hostpath-snapclass created

$ cat &amp;gt;mkdir_in_pod.sh &amp;lt;&amp;lt;EOF
#!/bin/sh
kubectl exec csi-hostpathplugin-0 -c hostpath -- mktemp -d /tmp/csi-sanity.XXXXXX
EOF

$ cat &amp;gt;rmdir_in_pod.sh &amp;lt;&amp;lt;EOF
#!/bin/sh
kubectl exec csi-hostpathplugin-0 -c hostpath -- rmdir &amp;quot;\$@&amp;quot;
EOF

$ chmod u+x *_in_pod.sh
$ csi-sanity -ginkgo.v \
             -csi.endpoint dns:///127.0.0.1:$(kubectl get &amp;quot;services/hostpath-service&amp;quot; -o &amp;quot;jsonpath={..nodePort}&amp;quot;) \
             -csi.createstagingpathcmd ./mkdir_in_pod.sh \
             -csi.createmountpathcmd ./mkdir_in_pod.sh \
             -csi.removestagingpathcmd ./rmdir_in_pod.sh \
             -csi.removemountpathcmd ./rmdir_in_pod.sh

Running Suite: CSI Driver Test Suite
====================================
Random Seed: 1570540138
Will run 72 of 72 specs
...
Controller Service [Controller Server] ControllerGetCapabilities 
  should return appropriate capabilities
  /nvme/gopath/src/github.com/kubernetes-csi/csi-test/pkg/sanity/controller.go:111
STEP: connecting to CSI driver
STEP: creating mount and staging directories
STEP: checking successful response
•
------------------------------
Controller Service [Controller Server] GetCapacity 
  should return capacity (no optional values added)
  /nvme/gopath/src/github.com/kubernetes-csi/csi-test/pkg/sanity/controller.go:149
STEP: reusing connection to CSI driver at dns:///127.0.0.1:30056
STEP: creating mount and staging directories
...
Ran 53 of 72 Specs in 148.206 seconds
SUCCESS! -- 53 Passed | 0 Failed | 0 Pending | 19 Skipped
PASS
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Some comments:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The source code of these tests is in the &lt;a href=&#34;https://github.com/kubernetes-csi/csi-test/tree/master/pkg/sanity&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;pkg/sanity&lt;/code&gt;
package&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;How to determine the external IP address of the node depends on the
cluster. In this example, the cluster was brought up with
&lt;code&gt;hack/local-up-cluster.sh&lt;/code&gt; and thus runs on the local host (&lt;code&gt;127.0.0.1&lt;/code&gt;).
It uses a port allocated by Kubernetes, obtained above with &lt;code&gt;kubectl get &amp;quot;services/hostpath-service&amp;quot;&lt;/code&gt;.
The Kubernetes-CSI CI uses
&lt;a href=&#34;https://kind.sigs.k8s.io/docs/user/quick-start/&#34; target=&#34;_blank&#34;&gt;kind&lt;/a&gt; and there &lt;a href=&#34;https://github.com/kubernetes-csi/csi-driver-host-path/blob/3488dc7f994e33485629b86b69a6f34ebb7ef2d9/release-tools/prow.sh#L850&#34; target=&#34;_blank&#34;&gt;a
Docker
command&lt;/a&gt;
can be used.&lt;/li&gt;
&lt;li&gt;The create script must print the final directory. Using a
unique directory for each test case has the advantage that if
something goes wrong in one test case, others still start with a
clean slate.&lt;/li&gt;
&lt;li&gt;The &amp;ldquo;staging directory&amp;rdquo;, aka &lt;code&gt;NodePublishVolumeRequest.target_path&lt;/code&gt;
in the CSI spec, must be created and deleted by the CSI driver while
the CO is responsible for the parent directory. &lt;code&gt;csi-sanity&lt;/code&gt; handles
that by creating a directory and then giving the CSI driver that
directory path with &lt;code&gt;/target&lt;/code&gt; appended at the end. Kubernetes &lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/75535&#34; target=&#34;_blank&#34;&gt;got
this wrong&lt;/a&gt;
and creates the actual &lt;code&gt;target_path&lt;/code&gt; directory, so CSI drivers which
want to work with Kubernetes currently have to be lenient and must
not fail when that directory already exists.&lt;/li&gt;
&lt;li&gt;The &amp;ldquo;mount directory&amp;rdquo; corresponds to
&lt;code&gt;NodeStageVolumeRequest.staging_target_path&lt;/code&gt; and really gets created
by the CO, i.e. &lt;code&gt;csi-sanity&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;end-to-end-testing&#34;&gt;End-to-end testing&lt;/h2&gt;

&lt;p&gt;In contrast to &lt;code&gt;csi-sanity&lt;/code&gt;, end-to-end testing interacts with the CSI
driver through the Kubernetes API, i.e. it simulates operations from a
normal user, like creating a PersistentVolumeClaim. Support for testing external CSI
drivers was
&lt;a href=&#34;https://github.com/kubernetes/kubernetes/commit/6644db9914379a4a7b3d3487b41b2010f226e4dc#diff-5b2d9461c960bc9b146c4ab3d77bcaa5&#34; target=&#34;_blank&#34;&gt;added&lt;/a&gt;
in Kubernetes 1.14.0.&lt;/p&gt;

&lt;h3 id=&#34;installation-1&#34;&gt;Installation&lt;/h3&gt;

&lt;p&gt;For each Kubernetes release, a test tar archive is published. It&amp;rsquo;s not
listed in the release notes (for example, the ones for
&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.16.md#downloads-for-v1161&#34; target=&#34;_blank&#34;&gt;1.16&lt;/a&gt;),
so one has to know that the full URL is
&lt;code&gt;https://dl.k8s.io/&amp;lt;version&amp;gt;/kubernetes-test-linux-amd64.tar.gz&lt;/code&gt; (like
for
&lt;a href=&#34;https://dl.k8s.io/v1.16.0/kubernetes-test-linux-amd64.tar.gz&#34; target=&#34;_blank&#34;&gt;v1.16.0&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;These include a &lt;code&gt;e2e.test&lt;/code&gt; binary for Linux on x86-64. Archives for
other platforms are also available, see &lt;a href=&#34;https://github.com/kubernetes/enhancements/blob/master/keps/sig-testing/20190118-breaking-apart-the-kubernetes-test-tarball.md#proposal&#34; target=&#34;_blank&#34;&gt;this
KEP&lt;/a&gt;. The
&lt;code&gt;e2e.test&lt;/code&gt; binary is completely self-contained, so one can &amp;ldquo;install&amp;rdquo;
it and the &lt;a href=&#34;https://onsi.github.io/ginkgo/&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;ginkgo&lt;/code&gt; test runner&lt;/a&gt; with:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl --location https://dl.k8s.io/v1.16.0/kubernetes-test-linux-amd64.tar.gz | \
  tar --strip-components=3 -zxf - kubernetes/test/bin/e2e.test kubernetes/test/bin/ginkgo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Each &lt;code&gt;e2e.test&lt;/code&gt; binary contains tests that match the features
available in the corresponding release. In particular, the &lt;code&gt;[Feature:
xyz]&lt;/code&gt; tags change between releases: they separate tests of alpha
features from tests of non-alpha features. Also, the tests from an
older release might rely on APIs that were removed in more recent
Kubernetes releases. To avoid problems, it&amp;rsquo;s best to simply use the
&lt;code&gt;e2e.test&lt;/code&gt; binary that matches the Kubernetes release that is used for
testing.&lt;/p&gt;

&lt;h3 id=&#34;usage-1&#34;&gt;Usage&lt;/h3&gt;

&lt;p&gt;Not all features of a CSI driver can be discovered through the
Kubernetes API. Therefore a configuration file in YAML or JSON format
is needed which describes the driver that is to be tested. That file
is used to populate &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/v1.16.0/test/e2e/storage/external/external.go#L142-L211&#34; target=&#34;_blank&#34;&gt;the driverDefinition
struct&lt;/a&gt;
and &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/v1.16.0/test/e2e/storage/testsuites/testdriver.go#L139-L185&#34; target=&#34;_blank&#34;&gt;the DriverInfo
struct&lt;/a&gt;
that is embedded inside it. For detailed usage instructions of
individual fields refer to these structs.&lt;/p&gt;

&lt;p&gt;A word of warning: tests are often only run when setting some fields and the
file parser does not warn about unknown fields, so always check that
the file really matches those structs.&lt;/p&gt;

&lt;p&gt;Here is an example that tests the
&lt;a href=&#34;https://github.com/kubernetes-csi/csi-driver-host-path&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;csi-driver-host-path&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat &amp;gt;test-driver.yaml &amp;lt;&amp;lt;EOF
StorageClass:
  FromName: true
SnapshotClass:
  FromName: true
DriverInfo:
  Name: hostpath.csi.k8s.io
  Capabilities:
    block: true
    controllerExpansion: true
    exec: true
    multipods: true
    persistence: true
    pvcDataSource: true
    snapshotDataSource: true
InlineVolumes:
- Attributes: {}
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;At a minimum, you need to define the storage class you want to use in
the test, the name of your driver, and what capabilities you want to
test.
As with &lt;code&gt;csi-sanity&lt;/code&gt;, the driver has to be running in the cluster
before testing it.
The actual &lt;code&gt;e2e.test&lt;/code&gt; invocation then enables tests for this driver
with &lt;code&gt;-storage.testdriver&lt;/code&gt; and selects the storage tests for it with
&lt;code&gt;-ginkgo.focus&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./e2e.test -ginkgo.v \
             -ginkgo.focus=&#39;External.Storage&#39; \
             -storage.testdriver=test-driver.yaml
Oct  8 17:17:42.230: INFO: The --provider flag is not set. Continuing as if --provider=skeleton had been used.
I1008 17:17:42.230210  648569 e2e.go:92] Starting e2e run &amp;quot;90b9adb0-a3a2-435f-80e0-640742d56104&amp;quot; on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1570547861 - Will randomize all specs
Will run 163 of 5060 specs

Oct  8 17:17:42.237: INFO: &amp;gt;&amp;gt;&amp;gt; kubeConfig: /var/run/kubernetes/admin.kubeconfig
Oct  8 17:17:42.241: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
...
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
External Storage [Driver: hostpath.csi.k8s.io] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow] 
  should access to two volumes with different volume mode and retain data across pod recreation on the same node
  /workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/testsuites/multivolume.go:191
[BeforeEach] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow]
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can use &lt;code&gt;ginkgo&lt;/code&gt; to run some kinds of test in parallel.
Alpha feature tests or those that by design have to be run
sequentially then need to be run separately:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./ginkgo -p -v \
         -focus=&#39;External.Storage&#39; \
         -skip=&#39;\[Feature:|\[Disruptive\]|\[Serial\]&#39; \
         ./e2e.test \
         -- \
         -storage.testdriver=test-driver.yaml
$ ./ginkgo -v \
         -focus=&#39;External.Storage.*(\[Feature:|\[Disruptive\]|\[Serial\])&#39; \
         ./e2e.test \
         -- \
         -storage.testdriver=test-driver.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;getting-involved&#34;&gt;Getting involved&lt;/h2&gt;

&lt;p&gt;Both the Kubernetes storage tests and the sanity tests are meant to be
applicable to arbitrary CSI drivers. But perhaps tests are based on
additional assumptions and your driver does not pass the testing
although it complies with the CSI specification. If that happens then
please file issues (links below).&lt;/p&gt;

&lt;p&gt;These are open source projects which depend on the help of those
using them, so once a problem has been acknowledged, a pull request
addressing it will be highly welcome.&lt;/p&gt;

&lt;p&gt;The same applies to writing new tests. The following searches in the
issue trackers select issues that have been marked specifically as
something that needs someone&amp;rsquo;s help:
- &lt;a href=&#34;https://github.com/kubernetes-csi/csi-test/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22&#34; target=&#34;_blank&#34;&gt;csi-test&lt;/a&gt;
- &lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues?utf8=%E2%9C%93&amp;amp;q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22+label%3Asig%2Fstorage+&#34; target=&#34;_blank&#34;&gt;Kubernetes&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Happy testing! May the issues it finds be few and easy to fix.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes 1.17: Stability</title>
      <link>https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-release-announcement/</link>
      <pubDate>Mon, 09 Dec 2019 13:00:00 -0800</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-release-announcement/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; &lt;a href=&#34;https://github.com/kubernetes/sig-release/blob/master/releases/release-1.17/release_team.md&#34; target=&#34;_blank&#34;&gt;Kubernetes 1.17 Release Team&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We’re pleased to announce the delivery of Kubernetes 1.17, our fourth and final release of 2019! Kubernetes v1.17 consists of 22 enhancements: 14 enhancements have graduated to stable, 4 enhancements are moving to beta, and 4 enhancements are entering alpha.&lt;/p&gt;

&lt;h2 id=&#34;major-themes&#34;&gt;Major Themes&lt;/h2&gt;

&lt;h3 id=&#34;cloud-provider-labels-reach-general-availability&#34;&gt;Cloud Provider Labels reach General Availability&lt;/h3&gt;

&lt;p&gt;Added as a beta feature way back in v1.2, v1.17 sees the general availability of cloud provider labels.&lt;/p&gt;

&lt;h3 id=&#34;volume-snapshot-moves-to-beta&#34;&gt;Volume Snapshot Moves to Beta&lt;/h3&gt;

&lt;p&gt;The Kubernetes Volume Snapshot feature is now beta in Kubernetes v1.17. It was introduced as alpha in Kubernetes v1.12, with a second alpha with breaking changes in Kubernetes v1.13.&lt;/p&gt;

&lt;h3 id=&#34;csi-migration-beta&#34;&gt;CSI Migration Beta&lt;/h3&gt;

&lt;p&gt;The Kubernetes in-tree storage plugin to Container Storage Interface (CSI) migration infrastructure is now beta in Kubernetes v1.17. CSI migration was introduced as alpha in Kubernetes v1.14.&lt;/p&gt;

&lt;h2 id=&#34;cloud-provider-labels-reach-general-availability-1&#34;&gt;Cloud Provider Labels reach General Availability&lt;/h2&gt;

&lt;p&gt;When nodes and volumes are created, a set of standard labels are applied based on the underlying cloud provider of the Kubernetes cluster. Nodes get a label for the instance type. Both nodes and volumes get two labels describing the location of the resource in the cloud provider topology, usually organized in zones and regions.&lt;/p&gt;

&lt;p&gt;Standard labels are used by Kubernetes components to support some features. For example, the scheduler would ensure that pods are placed on the same zone as the volumes they claim; and when scheduling pods belonging to a deployment, the scheduler would prioritize spreading them across zones. You can also use the labels in your pod specs to configure things as such node affinity. Standard labels allow you to write pod specs that are portable among different cloud providers.&lt;/p&gt;

&lt;p&gt;The labels are reaching general availability in this release. Kubernetes components have been updated to populate the GA and beta labels and to react to both. However, if you are using the beta labels in your pod specs for features such as node affinity, or in your custom controllers, we recommend that you start migrating them to the new GA labels. You can find the documentation for the new labels here:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#nodekubernetesioinstance-type&#34; target=&#34;_blank&#34;&gt;node.kubernetes.io/instance-type&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#topologykubernetesioregion&#34; target=&#34;_blank&#34;&gt;topology.kubernetes.io/region&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#topologykubernetesiozone&#34; target=&#34;_blank&#34;&gt;topology.kubernetes.io/zone&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;volume-snapshot-moves-to-beta-1&#34;&gt;Volume Snapshot Moves to Beta&lt;/h2&gt;

&lt;p&gt;The Kubernetes Volume Snapshot feature is now beta in Kubernetes v1.17. It was introduced as alpha in Kubernetes v1.12, with a second alpha with breaking changes in Kubernetes v1.13.  This post summarizes the changes in the beta release.&lt;/p&gt;

&lt;h3 id=&#34;what-is-a-volume-snapshot&#34;&gt;What is a Volume Snapshot?&lt;/h3&gt;

&lt;p&gt;Many storage systems (like Google Cloud Persistent Disks, Amazon Elastic Block Storage, and many on-premise storage systems) provide the ability to create a “snapshot” of a persistent volume. A snapshot represents a point-in-time copy of a volume. A snapshot can be used either to provision a new volume (pre-populated with the snapshot data) or to restore an existing volume to a previous state (represented by the snapshot).&lt;/p&gt;

&lt;h3 id=&#34;why-add-volume-snapshots-to-kubernetes&#34;&gt;Why add Volume Snapshots to Kubernetes?&lt;/h3&gt;

&lt;p&gt;The Kubernetes volume plugin system already provides a powerful abstraction that automates the provisioning, attaching, and mounting of block and file storage.&lt;/p&gt;

&lt;p&gt;Underpinning all these features is the Kubernetes goal of workload portability: Kubernetes aims to create an abstraction layer between distributed systems applications and underlying clusters so that applications can be agnostic to the specifics of the cluster they run on and application deployment requires no “cluster specific” knowledge.&lt;/p&gt;

&lt;p&gt;The Kubernetes Storage SIG identified snapshot operations as critical functionality for many stateful workloads. For example, a database administrator may want to snapshot a database volume before starting a database operation.&lt;/p&gt;

&lt;p&gt;By providing a standard way to trigger snapshot operations in the Kubernetes API, Kubernetes users can now handle use cases like this without having to go around the Kubernetes API (and manually executing storage system specific operations).&lt;/p&gt;

&lt;p&gt;Instead, Kubernetes users are now empowered to incorporate snapshot operations in a cluster agnostic way into their tooling and policy with the comfort of knowing that it will work against arbitrary Kubernetes clusters regardless of the underlying storage.&lt;/p&gt;

&lt;p&gt;Additionally these Kubernetes snapshot primitives act as basic building blocks that unlock the ability to develop advanced, enterprise grade, storage administration features for Kubernetes: including application or cluster level backup solutions.&lt;/p&gt;

&lt;p&gt;You can read more in the blog entry about &lt;a href=&#34;https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-cis-volume-snapshot-beta/&#34; target=&#34;_blank&#34;&gt;releasing CSI volume snapshots to beta&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;csi-migration-beta-1&#34;&gt;CSI Migration Beta&lt;/h2&gt;

&lt;h3 id=&#34;why-are-we-migrating-in-tree-plugins-to-csi&#34;&gt;Why are we migrating in-tree plugins to CSI?&lt;/h3&gt;

&lt;p&gt;Prior to CSI, Kubernetes provided a powerful volume plugin system. These volume plugins were “in-tree” meaning their code was part of the core Kubernetes code and shipped with the core Kubernetes binaries. However, adding support for new volume plugins to Kubernetes was challenging. Vendors that wanted to add support for their storage system to Kubernetes (or even fix a bug in an existing volume plugin) were forced to align with the Kubernetes release process. In addition, third-party storage code caused reliability and security issues in core Kubernetes binaries and the code was often difficult (and in some cases impossible) for Kubernetes maintainers to test and maintain. Using the Container Storage Interface in Kubernetes resolves these major issues.&lt;/p&gt;

&lt;p&gt;As more CSI Drivers were created and became production ready, we wanted all Kubernetes users to reap the benefits of the CSI model. However, we did not want to force users into making workload/configuration changes by breaking the existing generally available storage APIs. The way forward was clear - we would have to replace the backend of the “in-tree plugin” APIs with CSI.
What is CSI migration?&lt;/p&gt;

&lt;p&gt;The CSI migration effort enables the replacement of existing in-tree storage plugins such as &lt;code&gt;kubernetes.io/gce-pd&lt;/code&gt; or &lt;code&gt;kubernetes.io/aws-ebs&lt;/code&gt; with a corresponding CSI driver. If CSI Migration is working properly, Kubernetes end users shouldn’t notice a difference. After migration, Kubernetes users may continue to rely on all the functionality of in-tree storage plugins using the existing interface.&lt;/p&gt;

&lt;p&gt;When a Kubernetes cluster administrator updates a cluster to enable CSI migration, existing stateful deployments and workloads continue to function as they always have; however, behind the scenes Kubernetes hands control of all storage management operations (previously targeting in-tree drivers) to CSI drivers.&lt;/p&gt;

&lt;p&gt;The Kubernetes team has worked hard to ensure the stability of storage APIs and for the promise of a smooth upgrade experience. This involves meticulous accounting of all existing features and behaviors to ensure backwards compatibility and API stability. You can think of it like changing the wheels on a racecar while it’s speeding down the straightaway.&lt;/p&gt;

&lt;p&gt;You can read more in the blog entry about &lt;a href=&#34;https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-csi-migration-beta/&#34; target=&#34;_blank&#34;&gt;CSI migration going to beta&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;other-updates&#34;&gt;Other Updates&lt;/h2&gt;

&lt;h3 id=&#34;graduated-to-stable&#34;&gt;Graduated to Stable 💯&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/382&#34; target=&#34;_blank&#34;&gt;Taint Node by Condition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/495&#34; target=&#34;_blank&#34;&gt;Configurable Pod Process Namespace Sharing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/548&#34; target=&#34;_blank&#34;&gt;Schedule DaemonSet Pods by kube-scheduler&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/554&#34; target=&#34;_blank&#34;&gt;Dynamic Maximum Volume Count&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/557&#34; target=&#34;_blank&#34;&gt;Kubernetes CSI Topology Support&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/559&#34; target=&#34;_blank&#34;&gt;Provide Environment Variables Expansion in SubPath Mount&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/575&#34; target=&#34;_blank&#34;&gt;Defaulting of Custom Resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/589&#34; target=&#34;_blank&#34;&gt;Move Frequent Kubelet Heartbeats To Lease Api&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/714&#34; target=&#34;_blank&#34;&gt;Break Apart The Kubernetes Test Tarball&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/956&#34; target=&#34;_blank&#34;&gt;Add Watch Bookmarks Support&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/960&#34; target=&#34;_blank&#34;&gt;Behavior-Driven Conformance Testing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/980&#34; target=&#34;_blank&#34;&gt;Finalizer Protection For Service Loadbalancers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/1152&#34; target=&#34;_blank&#34;&gt;Avoid Serializing The Same Object Independently For Every Watcher&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;major-changes&#34;&gt;Major Changes&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/563&#34; target=&#34;_blank&#34;&gt;Add IPv4/IPv6 Dual Stack Support&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;other-notable-features&#34;&gt;Other Notable Features&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/536&#34; target=&#34;_blank&#34;&gt;Topology Aware Routing of Services (Alpha)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/1043&#34; target=&#34;_blank&#34;&gt;RunAsUserName for Windows&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;availability&#34;&gt;Availability&lt;/h3&gt;

&lt;p&gt;Kubernetes 1.17 is available for &lt;a href=&#34;https://github.com/kubernetes/kubernetes/releases/tag/v1.17.0&#34; target=&#34;_blank&#34;&gt;download on GitHub&lt;/a&gt;. To get started with Kubernetes, check out these &lt;a href=&#34;https://kubernetes.io/docs/tutorials/&#34; target=&#34;_blank&#34;&gt;interactive tutorials&lt;/a&gt;. You can also easily install 1.17 using &lt;a href=&#34;https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/&#34; target=&#34;_blank&#34;&gt;kubeadm&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;release-team&#34;&gt;Release Team&lt;/h3&gt;

&lt;p&gt;This release is made possible through the efforts of hundreds of individuals who contributed both technical and non-technical content. Special thanks to the &lt;a href=&#34;https://github.com/kubernetes/sig-release/blob/master/releases/release-1.17/release_team.md&#34; target=&#34;_blank&#34;&gt;release team&lt;/a&gt; led by Guinevere Saenger. The 35 individuals on the release team coordinated many aspects of the release, from documentation to testing, validation, and feature completeness.&lt;/p&gt;

&lt;p&gt;As the Kubernetes community has grown, our release process represents an amazing demonstration of collaboration in open source software development. Kubernetes continues to gain new users at a rapid pace. This growth creates a positive feedback cycle where more contributors commit code creating a more vibrant ecosystem. Kubernetes has had over &lt;a href=&#34;https://k8s.devstats.cncf.io/d/24/overall-project-statistics?orgId=1&#34; target=&#34;_blank&#34;&gt;39,000 individual contributors&lt;/a&gt; to date and an active community of more than 66,000 people.&lt;/p&gt;

&lt;h3 id=&#34;webinar&#34;&gt;Webinar&lt;/h3&gt;

&lt;p&gt;Join members of the Kubernetes 1.17 release team on Jan 7th, 2020 to learn about the major features in this release. Register &lt;a href=&#34;https://zoom.us/webinar/register/9315759188139/WN_kPOZA_6RTjeGdXTG7YFO3A&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;get-involved&#34;&gt;Get Involved&lt;/h3&gt;

&lt;p&gt;The simplest way to get involved with Kubernetes is by joining one of the many &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/sig-list.md&#34; target=&#34;_blank&#34;&gt;Special Interest Groups&lt;/a&gt; (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/communication&#34; target=&#34;_blank&#34;&gt;community meeting&lt;/a&gt;, and through the channels below. Thank you for your continued feedback and support.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Follow us on Twitter &lt;a href=&#34;https://twitter.com/kubernetesio&#34; target=&#34;_blank&#34;&gt;@Kubernetesio&lt;/a&gt; for latest updates&lt;/li&gt;
&lt;li&gt;Join the community discussion on &lt;a href=&#34;https://discuss.kubernetes.io/&#34; target=&#34;_blank&#34;&gt;Discuss&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Join the community on &lt;a href=&#34;http://slack.k8s.io/&#34; target=&#34;_blank&#34;&gt;Slack&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Post questions (or answer questions) on &lt;a href=&#34;http://stackoverflow.com/questions/tagged/kubernetes&#34; target=&#34;_blank&#34;&gt;Stack Overflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Share your Kubernetes &lt;a href=&#34;https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform&#34; target=&#34;_blank&#34;&gt;story&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes 1.17 Feature: Kubernetes Volume Snapshot Moves to Beta</title>
      <link>https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-cis-volume-snapshot-beta/</link>
      <pubDate>Mon, 09 Dec 2019 10:00:00 -0800</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-cis-volume-snapshot-beta/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xing Yang, VMware &amp;amp; Xiangqian Yu, Google&lt;/p&gt;

&lt;p&gt;The Kubernetes Volume Snapshot feature is now beta in Kubernetes v1.17. It was introduced &lt;a href=&#34;https://kubernetes.io/blog/2018/10/09/introducing-volume-snapshot-alpha-for-kubernetes/&#34; target=&#34;_blank&#34;&gt;as alpha&lt;/a&gt; in Kubernetes v1.12, with a &lt;a href=&#34;https://kubernetes.io/blog/2019/01/17/update-on-volume-snapshot-alpha-for-kubernetes/&#34; target=&#34;_blank&#34;&gt;second alpha&lt;/a&gt; with breaking changes in Kubernetes v1.13.  This post summarizes the changes in the beta release.&lt;/p&gt;

&lt;h2 id=&#34;what-is-a-volume-snapshot&#34;&gt;What is a Volume Snapshot?&lt;/h2&gt;

&lt;p&gt;Many storage systems (like Google Cloud Persistent Disks, Amazon Elastic Block Storage, and many on-premise storage systems) provide the ability to create a “snapshot” of a persistent volume. A snapshot represents a point-in-time copy of a volume. A snapshot can be used either to provision a new volume (pre-populated with the snapshot data) or to restore an existing volume to a previous state (represented by the snapshot).&lt;/p&gt;

&lt;h2 id=&#34;why-add-volume-snapshots-to-kubernetes&#34;&gt;Why add Volume Snapshots to Kubernetes?&lt;/h2&gt;

&lt;p&gt;The Kubernetes volume plugin system already provides a powerful abstraction that automates the provisioning, attaching, and mounting of block and file storage.&lt;/p&gt;

&lt;p&gt;Underpinning all these features is the Kubernetes goal of workload portability: Kubernetes aims to create an abstraction layer between distributed applications and underlying clusters so that applications can be agnostic to the specifics of the cluster they run on and application deployment requires no “cluster specific” knowledge.&lt;/p&gt;

&lt;p&gt;The Kubernetes Storage SIG identified snapshot operations as critical functionality for many stateful workloads. For example, a database administrator may want to snapshot a database volume before starting a database operation.&lt;/p&gt;

&lt;p&gt;By providing a standard way to trigger snapshot operations in the Kubernetes API, Kubernetes users can now handle use cases like this without having to go around the Kubernetes API (and manually executing storage system specific operations).&lt;/p&gt;

&lt;p&gt;Instead, Kubernetes users are now empowered to incorporate snapshot operations in a cluster agnostic way into their tooling and policy with the comfort of knowing that it will work against arbitrary Kubernetes clusters regardless of the underlying storage.&lt;/p&gt;

&lt;p&gt;Additionally these Kubernetes snapshot primitives act as basic building blocks that unlock the ability to develop advanced, enterprise grade, storage administration features for Kubernetes: including application or cluster level backup solutions.&lt;/p&gt;

&lt;h2 id=&#34;what-s-new-in-beta&#34;&gt;What’s new in Beta?&lt;/h2&gt;

&lt;p&gt;With the promotion of Volume Snapshot to beta, the feature is now enabled by default on standard Kubernetes deployments instead of being opt-in.&lt;/p&gt;

&lt;p&gt;The move of the Kubernetes Volume Snapshot feature to beta also means:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A revamp of volume snapshot APIs.&lt;/li&gt;
&lt;li&gt;The CSI external-snapshotter sidecar is split into two controllers, a common snapshot controller and a CSI external-snapshotter sidecar.&lt;/li&gt;
&lt;li&gt;Deletion secret is added as an annotation to the volume snapshot content.&lt;/li&gt;
&lt;li&gt;A new finalizer is added to the volume snapshot API object to prevent it from being deleted when it is bound to a volume snapshot content API object.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;kubernetes-volume-snapshots-requirements&#34;&gt;Kubernetes Volume Snapshots Requirements&lt;/h2&gt;

&lt;p&gt;As mentioned above, with the promotion of Volume Snapshot to beta, the feature is now enabled by default on standard Kubernetes deployments instead of being opt-in.&lt;/p&gt;

&lt;p&gt;In order to use the Kubernetes Volume Snapshot feature, you must ensure the following components have been deployed on your Kubernetes cluster:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-csi/external-snapshotter/tree/master/config/crd&#34; target=&#34;_blank&#34;&gt;Kubernetes Volume Snapshot CRDs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-csi/external-snapshotter/tree/master/pkg/common-controller&#34; target=&#34;_blank&#34;&gt;Volume snapshot controller&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CSI Driver supporting Kubernetes volume snapshot beta&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;See the deployment section below for details.&lt;/p&gt;

&lt;h2 id=&#34;which-drivers-support-kubernetes-volume-snapshots&#34;&gt;Which drivers support Kubernetes Volume Snapshots?&lt;/h2&gt;

&lt;p&gt;Kubernetes supports three types of volume plugins: in-tree, Flex, and CSI. See &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md&#34; target=&#34;_blank&#34;&gt;Kubernetes Volume Plugin FAQ&lt;/a&gt; for details.&lt;/p&gt;

&lt;p&gt;Snapshots are only supported for CSI drivers (not for in-tree or Flex). To use the Kubernetes snapshots feature, ensure that a CSI Driver that implements snapshots is deployed on your cluster.&lt;/p&gt;

&lt;p&gt;Read the “&lt;a href=&#34;https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/&#34; target=&#34;_blank&#34;&gt;Container Storage Interface (CSI) for Kubernetes GA&lt;/a&gt;” blog post to learn more about CSI and how to deploy CSI drivers.&lt;/p&gt;

&lt;p&gt;As of the publishing of this blog, the following CSI drivers have been updated to support volume snapshots beta:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver&#34; target=&#34;_blank&#34;&gt;GCE Persistent Disk CSI Driver&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/libopenstorage/openstorage/tree/master/csi&#34; target=&#34;_blank&#34;&gt;Portworx CSI Driver&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/NetApp/trident&#34; target=&#34;_blank&#34;&gt;NetApp Trident CSI Driver&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Beta level Volume Snapshot support for other &lt;a href=&#34;https://kubernetes-csi.github.io/docs/drivers.html&#34; target=&#34;_blank&#34;&gt;CSI drivers&lt;/a&gt; is pending, and should be available soon.&lt;/p&gt;

&lt;h2 id=&#34;kubernetes-volume-snapshot-beta-api&#34;&gt;Kubernetes Volume Snapshot Beta API&lt;/h2&gt;

&lt;p&gt;A number of changes were made to the Kubernetes volume snapshot API between alpha to beta. These changes are not backward compatible. The purpose of these changes was to make API definitions clear and easier to use.&lt;/p&gt;

&lt;p&gt;The following changes are made:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;DeletionPolicy&lt;/code&gt; is now a required field rather than optional in both &lt;code&gt;VolumeSnapshotClass&lt;/code&gt; and &lt;code&gt;VolumeSnapshotContent&lt;/code&gt;. This way the user has to explicitly specify it, leaving no room for confusion.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;VolumeSnapshotSpec&lt;/code&gt; has a new required &lt;code&gt;Source&lt;/code&gt; field. &lt;code&gt;Source&lt;/code&gt; may be either a &lt;code&gt;PersistentVolumeClaimName&lt;/code&gt; (if dynamically provisioning a snapshot) or &lt;code&gt;VolumeSnapshotContentName&lt;/code&gt; (if pre-provisioning a snapshot).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;VolumeSnapshotContentSpec&lt;/code&gt; also has a new required &lt;code&gt;Source&lt;/code&gt; field. This &lt;code&gt;Source&lt;/code&gt; may be either a &lt;code&gt;VolumeHandle&lt;/code&gt; (if dynamically provisioning a snapshot) or a &lt;code&gt;SnapshotHandle&lt;/code&gt; (if pre-provisioning volume snapshots).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;VolumeSnapshotStatus&lt;/code&gt; now contains a &lt;code&gt;BoundVolumeSnapshotContentName&lt;/code&gt; to indicate the &lt;code&gt;VolumeSnapshot&lt;/code&gt; object is bound to a &lt;code&gt;VolumeSnapshotContent&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;VolumeSnapshotContent&lt;/code&gt;now contains a &lt;code&gt;Status&lt;/code&gt; to indicate the current state of the content. It has a field &lt;code&gt;SnapshotHandle&lt;/code&gt; to indicate that the &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; represents a snapshot on the storage system.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The beta Kubernetes VolumeSnapshot API object:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;type&lt;/span&gt; VolumeSnapshot &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;struct&lt;/span&gt; {
        metav1.TypeMeta
        metav1.ObjectMeta

        Spec VolumeSnapshotSpec
        Status &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt;VolumeSnapshotStatus
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;type&lt;/span&gt; VolumeSnapshotSpec &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;struct&lt;/span&gt; {
	Source VolumeSnapshotSource
	VolumeSnapshotClassName &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#0b0;font-weight:bold&#34;&gt;string&lt;/span&gt;
}
&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// Exactly one of its members MUST be specified
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;type&lt;/span&gt; VolumeSnapshotSource &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;struct&lt;/span&gt; {
	&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// +optional
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;&lt;/span&gt;	PersistentVolumeClaimName &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#0b0;font-weight:bold&#34;&gt;string&lt;/span&gt;
	&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// +optional
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;&lt;/span&gt;	VolumeSnapshotContentName &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#0b0;font-weight:bold&#34;&gt;string&lt;/span&gt;
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;type&lt;/span&gt; VolumeSnapshotStatus &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;struct&lt;/span&gt; {
	BoundVolumeSnapshotContentName &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#0b0;font-weight:bold&#34;&gt;string&lt;/span&gt;
	CreationTime &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt;metav1.Time
	ReadyToUse &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#0b0;font-weight:bold&#34;&gt;bool&lt;/span&gt;
	RestoreSize &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt;resource.Quantity
	Error &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt;VolumeSnapshotError
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The beta Kubernetes VolumeSnapshotContent API object:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;type&lt;/span&gt; VolumeSnapshotContent &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;struct&lt;/span&gt; {
        metav1.TypeMeta
        metav1.ObjectMeta

        Spec VolumeSnapshotContentSpec
        Status &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt;VolumeSnapshotContentStatus
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;type&lt;/span&gt; VolumeSnapshotContentSpec &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;struct&lt;/span&gt; {
         VolumeSnapshotRef core_v1.ObjectReference
         Source VolumeSnapshotContentSource
         DeletionPolicy DeletionPolicy
         Driver &lt;span style=&#34;color:#0b0;font-weight:bold&#34;&gt;string&lt;/span&gt;
         VolumeSnapshotClassName &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#0b0;font-weight:bold&#34;&gt;string&lt;/span&gt;
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;type&lt;/span&gt; VolumeSnapshotContentSource &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;struct&lt;/span&gt; {
	&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// +optional
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;&lt;/span&gt;	VolumeHandle &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#0b0;font-weight:bold&#34;&gt;string&lt;/span&gt;
	&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// +optional
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;&lt;/span&gt;	SnapshotHandle &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#0b0;font-weight:bold&#34;&gt;string&lt;/span&gt;
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;type&lt;/span&gt; VolumeSnapshotContentStatus &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;struct&lt;/span&gt; {
  CreationTime &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#0b0;font-weight:bold&#34;&gt;int64&lt;/span&gt;
  ReadyToUse &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#0b0;font-weight:bold&#34;&gt;bool&lt;/span&gt;
  RestoreSize &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#0b0;font-weight:bold&#34;&gt;int64&lt;/span&gt;
  Error &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt;VolumeSnapshotError
  SnapshotHandle &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#0b0;font-weight:bold&#34;&gt;string&lt;/span&gt;
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The beta Kubernetes VolumeSnapshotClass API object:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;type&lt;/span&gt; VolumeSnapshotClass &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;struct&lt;/span&gt; {
        metav1.TypeMeta
        metav1.ObjectMeta

        Driver &lt;span style=&#34;color:#0b0;font-weight:bold&#34;&gt;string&lt;/span&gt;
        Parameters &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;map&lt;/span&gt;[&lt;span style=&#34;color:#0b0;font-weight:bold&#34;&gt;string&lt;/span&gt;]&lt;span style=&#34;color:#0b0;font-weight:bold&#34;&gt;string&lt;/span&gt;
        DeletionPolicy DeletionPolicy
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id=&#34;how-do-i-deploy-support-for-volume-snapshots-on-my-kubernetes-cluster&#34;&gt;How do I deploy support for Volume Snapshots on my Kubernetes Cluster?&lt;/h3&gt;

&lt;p&gt;Please note that the Volume Snapshot feature now depends on a new, common &lt;a href=&#34;https://github.com/kubernetes-csi/external-snapshotter/tree/master/pkg/common-controller&#34; target=&#34;_blank&#34;&gt;volume snapshot controller&lt;/a&gt; in addition to the volume snapshot CRDs. Both the volume snapshot controller and the CRDs are independent of any CSI driver. Regardless of the number of CSI drivers deployed on the cluster, there must be only one instance of the volume snapshot controller running and one set of volume snapshot CRDs installed per cluster.&lt;/p&gt;

&lt;p&gt;Therefore, it is strongly recommended that Kubernetes distributors bundle and deploy the controller and CRDs as part of their Kubernetes cluster management process (independent of any CSI Driver).&lt;/p&gt;

&lt;p&gt;If your cluster does not come pre-installed with the correct components, you may manually install these components by executing the following steps.&lt;/p&gt;

&lt;h4 id=&#34;install-snapshot-beta-crds&#34;&gt;Install Snapshot Beta CRDs&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;kubectl create -f config/crd&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-csi/external-snapshotter/tree/master/config/crd&#34; target=&#34;_blank&#34;&gt;https://github.com/kubernetes-csi/external-snapshotter/tree/master/config/crd&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Do this once per cluster&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;install-common-snapshot-controller&#34;&gt;Install Common Snapshot Controller&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;kubectl create -f deploy/kubernetes/snapshot-controller&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-csi/external-snapshotter/tree/master/deploy/kubernetes/snapshot-controller&#34; target=&#34;_blank&#34;&gt;https://github.com/kubernetes-csi/external-snapshotter/tree/master/deploy/kubernetes/snapshot-controller&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Do this once per cluster&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;install-csi-driver&#34;&gt;Install CSI Driver&lt;/h4&gt;

&lt;p&gt;Follow instructions provided by your CSI Driver vendor.&lt;/p&gt;

&lt;h3 id=&#34;how-do-i-use-kubernetes-volume-snapshots&#34;&gt;How do I use Kubernetes Volume Snapshots?&lt;/h3&gt;

&lt;p&gt;Assuming all the required components (including CSI driver) are already deployed and running on your cluster, you can create volume snapshots using the VolumeSnapshot API object, and restore them by specifying a VolumeSnapshot data source on a PVC.&lt;/p&gt;

&lt;h4 id=&#34;creating-a-new-volume-snapshot-with-kubernetes&#34;&gt;Creating a New Volume Snapshot with Kubernetes&lt;/h4&gt;

&lt;p&gt;You can enable creation/deletion of volume snapshots in a Kubernetes cluster, by creating a VolumeSnapshotClass API object pointing to a CSI Driver that support volume snapshots.&lt;/p&gt;

&lt;p&gt;The following VolumeSnapshotClass, for example, tells the Kubernetes cluster that a CSI driver, &lt;code&gt;testdriver.csi.k8s.io&lt;/code&gt;, can handle volume snapshots, and that when these snapshots are created, their deletion policy should be to delete.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;snapshot.storage.k8s.io/v1beta1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;VolumeSnapshotClass&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;test-snapclass&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;driver:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;testdriver.csi.k8s.io&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;deletionPolicy:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Delete&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;parameters:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;csi.storage.k8s.io/snapshotter-secret-name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;mysecret&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;csi.storage.k8s.io/snapshotter-secret-namespace:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;mysecretnamespace&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The common snapshot controller reserves the parameter keys &lt;code&gt;csi.storage.k8s.io/snapshotter-secret-name&lt;/code&gt; and &lt;code&gt;csi.storage.k8s.io/snapshotter-secret-namespace&lt;/code&gt;. If specified, it fetches the referenced Kubernetes secret and sets it as an annotation on the volume snapshot content object.  The CSI external-snapshotter sidecar retrieves it from the content annotation and passes it to the CSI driver during snapshot creation.&lt;/p&gt;

&lt;p&gt;Creation of a volume snapshot is triggered by the creation of a VolumeSnapshot API object.&lt;/p&gt;

&lt;p&gt;The VolumeSnapshot object must specify the following source type:
&lt;code&gt;persistentVolumeClaimName&lt;/code&gt; - The name of the PVC to snapshot. Please note that the source PVC, PV, and VolumeSnapshotClass for a VolumeSnapshot object must point to the same CSI driver.&lt;/p&gt;

&lt;p&gt;The following VolumeSnapshot, for example, triggers the creation of a snapshot for a PVC called &lt;code&gt;test-pvc&lt;/code&gt; using the VolumeSnapshotClass above.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;snapshot.storage.k8s.io/v1beta1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;VolumeSnapshot&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;test-snapshot&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;volumeSnapshotClassName:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;test-snapclass&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;source:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;persistentVolumeClaimName:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;test-pvc&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;When volume snapshot creation is invoked, the common snapshot controller first creates a VolumeSnapshotContent object with the &lt;code&gt;volumeSnapshotRef&lt;/code&gt;, source &lt;code&gt;volumeHandle&lt;/code&gt;, &lt;code&gt;volumeSnapshotClassName&lt;/code&gt; if specified, &lt;code&gt;driver&lt;/code&gt;, and &lt;code&gt;deletionPolicy&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The CSI external-snapshotter sidecar then passes the VolumeSnapshotClass parameters, the source volume ID, and any referenced secret(s) to the CSI driver (in this case &lt;code&gt;testdriver.csi.k8s.io&lt;/code&gt;) via a CSI &lt;code&gt;CreateSnapshot&lt;/code&gt; call. In response, the CSI driver creates a new snapshot for the specified volume, and returns the ID for that snapshot. The CSI external-snapshotter sidecar then updates the &lt;code&gt;snapshotHandle&lt;/code&gt;, &lt;code&gt;creationTime&lt;/code&gt;, &lt;code&gt;restoreSize&lt;/code&gt;, and &lt;code&gt;readyToUse&lt;/code&gt; in the status field of the VolumeSnapshotContent object that represents the new snapshot. For a storage system that needs to upload the snapshot after it has been cut, the CSI external-snapshotter sidecar will keep calling the CSI &lt;code&gt;CreateSnapshot&lt;/code&gt; to check the status until upload is complete and set &lt;code&gt;readyToUse&lt;/code&gt; to true.&lt;/p&gt;

&lt;p&gt;The common snapshot controller binds the VolumeSnapshotContent object to the VolumeSnapshot (sets &lt;code&gt;BoundVolumeSnapshotContentName&lt;/code&gt;), and updates the &lt;code&gt;creationTime&lt;/code&gt;, &lt;code&gt;restoreSize&lt;/code&gt;, and &lt;code&gt;readyToUse&lt;/code&gt; in the status field of the VolumeSnapshot object based on the status field of the VolumeSnapshotContent object.&lt;/p&gt;

&lt;p&gt;If no &lt;code&gt;volumeSnapshotClassName&lt;/code&gt; is specified, one is automatically selected as follows:&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;StorageClass&lt;/code&gt; from PVC or PV of the source volume is fetched. The default VolumeSnapshotClass is fetched, if available. A default VolumeSnapshotClass is a snapshot class created by the admin with the &lt;code&gt;snapshot.storage.kubernetes.io/is-default-class&lt;/code&gt; annotation. If the &lt;code&gt;Driver&lt;/code&gt; field of the default VolumeSnapshotClass is the same as the &lt;code&gt;Provisioner&lt;/code&gt; field in the StorageClass, the default VolumeSnapshotClass is used. If there is no default VolumeSnapshotClass or more than one default VolumeSnapshotClass for a snapshot, an error will be returned.&lt;/p&gt;

&lt;p&gt;Please note that the Kubernetes Snapshot API does not provide any consistency guarantees. You have to prepare your application (pause application, freeze filesystem etc.) before taking the snapshot for data consistency either manually or using some other higher level APIs/controllers.&lt;/p&gt;

&lt;p&gt;You can verify that the VolumeSnapshot object is created and bound with VolumeSnapshotContent by running &lt;code&gt;kubectl describe volumesnapshot&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Bound Volume Snapshot Content Name&lt;/code&gt; - field in the &lt;code&gt;Status&lt;/code&gt; indicates the volume is bound to the specified VolumeSnapshotContent.
&lt;code&gt;Ready To Use&lt;/code&gt; - field in the &lt;code&gt;Status&lt;/code&gt; indicates this volume snapshot is ready for use.
&lt;code&gt;Creation Time&lt;/code&gt; - field in the &lt;code&gt;Status&lt;/code&gt; indicates when the snapshot was actually created (cut).
&lt;code&gt;Restore Size&lt;/code&gt; - field in the &lt;code&gt;Status&lt;/code&gt; indicates the minimum volume size required when restoring a volume from this snapshot.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Name:         test-snapshot
Namespace:    default
Labels:       &amp;lt;none&amp;gt;
Annotations:  &amp;lt;none&amp;gt;
API Version:  snapshot.storage.k8s.io/v1beta1
Kind:         VolumeSnapshot
Metadata:
  Creation Timestamp:  2019-11-16T00:36:04Z
  Finalizers:
    snapshot.storage.kubernetes.io/volumesnapshot-as-source-protection
    snapshot.storage.kubernetes.io/volumesnapshot-bound-protection
  Generation:        1
  Resource Version:  1294
  Self Link:         /apis/snapshot.storage.k8s.io/v1beta1/namespaces/default/volumesnapshots/new-snapshot-demo
  UID:               32ceaa2a-3802-4edd-a808-58c4f1bd7869
Spec:
  Source:
    Persistent Volume Claim Name:  test-pvc
  Volume Snapshot Class Name:      test-snapclass
Status:
  Bound Volume Snapshot Content Name:  snapcontent-32ceaa2a-3802-4edd-a808-58c4f1bd7869
  Creation Time:                       2019-11-16T00:36:04Z
  Ready To Use:                        true
  Restore Size:                        1Gi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As a reminder to any developers building controllers using volume snapshot APIs: before using a VolumeSnapshot API object, validate the bi-directional binding between the VolumeSnpashot and the VolumeSnapshotContent it is bound to, to ensure the binding is complete and correct (not doing so may result in security issues).&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl describe volumesnapshotcontent&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;Name:         snapcontent-32ceaa2a-3802-4edd-a808-58c4f1bd7869
Namespace:
Labels:       &amp;lt;none&amp;gt;
Annotations:  &amp;lt;none&amp;gt;
API Version:  snapshot.storage.k8s.io/v1beta1
Kind:         VolumeSnapshotContent
Metadata:
  Creation Timestamp:  2019-11-16T00:36:04Z
  Finalizers:
    snapshot.storage.kubernetes.io/volumesnapshotcontent-bound-protection
  Generation:        1
  Resource Version:  1292
  Self Link:         /apis/snapshot.storage.k8s.io/v1beta1/volumesnapshotcontents/snapcontent-32ceaa2a-3802-4edd-a808-58c4f1bd7869
  UID:               7dfdf22e-0b0c-4b71-9ddf-2f1612ca2aed
Spec:
  Deletion Policy:  Delete
  Driver:           testdriver.csi.k8s.io
  Source:
    Volume Handle:             d1b34a5f-0808-11ea-808a-0242ac110003
  Volume Snapshot Class Name:  test-snapclass
  Volume Snapshot Ref:
    API Version:       snapshot.storage.k8s.io/v1beta1
    Kind:              VolumeSnapshot
    Name:              test-snapshot
    Namespace:         default
    Resource Version:  1286
    UID:               32ceaa2a-3802-4edd-a808-58c4f1bd7869
Status:
  Creation Time:    1573864564608810101
  Ready To Use:     true
  Restore Size:     1073741824
  Snapshot Handle:  127c5798-0809-11ea-808a-0242ac110003
Events:             &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;importing-an-existing-volume-snapshot-with-kubernetes&#34;&gt;Importing an existing volume snapshot with Kubernetes&lt;/h4&gt;

&lt;p&gt;You can always expose a pre-existing volume snapshot in Kubernetes by manually creating a VolumeSnapshotContent object to represent the existing volume snapshot. Because VolumeSnapshotContent is a non-namespace API object, only a cluster admin may have the permission to create it. By specifying the &lt;code&gt;volumeSnapshotRef&lt;/code&gt; the cluster admin specifies exactly which user can use the snapshot.&lt;/p&gt;

&lt;p&gt;The following VolumeSnapshotContent, for example exposes a volume snapshot with the name &lt;code&gt;7bdd0de3-aaeb-11e8-9aae-0242ac110002&lt;/code&gt; belonging to a CSI driver called &lt;code&gt;testdriver.csi.k8s.io&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;A VolumeSnapshotContent object should be created by a cluster admin with the following fields to represent an existing snapshot:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;driver&lt;/code&gt; - CSI driver used to handle this volume. This field is required.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;source&lt;/code&gt; - Snapshot identifying information&lt;/li&gt;
&lt;li&gt;&lt;code&gt;snapshotHandle&lt;/code&gt; - name/identifier of the snapshot. This field is required.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;volumeSnapshotRef&lt;/code&gt; - Pointer to the VolumeSnapshot object this content should bind to.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;name&lt;/code&gt; and &lt;code&gt;namespace&lt;/code&gt; - Specifies the name and namespace of the VolumeSnapshot object which the content is bound to.&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;deletionPolicy&lt;/code&gt; - Valid values are &lt;code&gt;Delete&lt;/code&gt; and &lt;code&gt;Retain&lt;/code&gt;. If the &lt;code&gt;deletionPolicy&lt;/code&gt; is &lt;code&gt;Delete&lt;/code&gt;, then the underlying storage snapshot will be deleted along with the VolumeSnapshotContent object. If the - &lt;code&gt;deletionPolicy&lt;/code&gt; is &lt;code&gt;Retain&lt;/code&gt;, then both the underlying snapshot and VolumeSnapshotContent remain.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;snapshot.storage.k8s.io/v1beta1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;VolumeSnapshotContent&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;manually-created-snapshot-content&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;deletionPolicy:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Delete&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;driver:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;testdriver.csi.k8s.io&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;source:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;snapshotHandle:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;7bdd0de3-aaeb&lt;span style=&#34;color:#666&#34;&gt;-11e8&lt;/span&gt;-9aae-0242ac110002&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;volumeSnapshotRef:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;test-snapshot&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;namespace:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;default&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Once a VolumeSnapshotContent object is created, a user can create a VolumeSnapshot object pointing to the VolumeSnapshotContent object. The name and namespace of the VolumeSnapshot object must match the name/namespace specified in the volumeSnapshotRef of the VolumeSnapshotContent. It specifies the following fields:
&lt;code&gt;volumeSnapshotContentName&lt;/code&gt; - name of the volume snapshot content specified above. This field is required.
&lt;code&gt;volumeSnapshotClassName&lt;/code&gt; - name of the volume snapshot class. This field is optional.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;snapshot.storage.k8s.io/v1beta1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;VolumeSnapshot&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;manually-created-snapshot&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;source:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;volumeSnapshotContentName:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;test-content&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Once both objects are created, the common snapshot controller verifies the binding between VolumeSnapshot and VolumeSnapshotContent objects is correct and marks the VolumeSnapshot as ready (if the CSI driver supports the &lt;code&gt;ListSnapshots&lt;/code&gt; call, the controller also validates that the referenced snapshot exists). The CSI external-snapshotter sidecar checks if the snapshot exists if ListSnapshots CSI method is implemented, otherwise it assumes the snapshot exists. The external-snapshotter sidecar sets &lt;code&gt;readyToUse&lt;/code&gt; to true in the status field of VolumeSnapshotContent. The common snapshot controller marks the snapshot as ready accordingly.&lt;/p&gt;

&lt;h2 id=&#34;create-volume-from-snapshot&#34;&gt;Create Volume From Snapshot&lt;/h2&gt;

&lt;p&gt;Once you have a bound and ready VolumeSnapshot object, you can use that object to provision a new volume that is pre-populated with data from the snapshot.&lt;/p&gt;

&lt;p&gt;To provision a new volume pre-populated with data from a snapshot, use the &lt;code&gt;dataSource&lt;/code&gt; field in the &lt;code&gt;PersistentVolumeClaim&lt;/code&gt;. It has three parameters:
&lt;code&gt;name&lt;/code&gt; - name of the VolumeSnapshot object representing the snapshot to use as source
&lt;code&gt;kind&lt;/code&gt; - must be VolumeSnapshot
&lt;code&gt;apiGroup&lt;/code&gt; - must be snapshot.storage.k8s.io&lt;/p&gt;

&lt;p&gt;The namespace of the source VolumeSnapshot object is assumed to be the same as the namespace of the &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; object.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;v1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;PersistentVolumeClaim&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;pvc-restore&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;namespace:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;demo-namespace&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;storageClassName:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;testdriver.csi.k8s.io&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;dataSource:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;manually-created-snapshot&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;VolumeSnapshot&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;apiGroup:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;snapshot.storage.k8s.io&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;accessModes:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;ReadWriteOnce&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;resources:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;requests:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;storage:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;1Gi&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;When the &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; object is created, it will trigger provisioning of a new volume that is pre-populated with data from the specified snapshot.
As a storage vendor, how do I add support for snapshots to my CSI driver?
To implement the snapshot feature, a CSI driver MUST add support for additional controller capabilities &lt;code&gt;CREATE_DELETE_SNAPSHOT&lt;/code&gt; and &lt;code&gt;LIST_SNAPSHOTS&lt;/code&gt;, and implement additional controller RPCs: &lt;code&gt;CreateSnapshot&lt;/code&gt;, &lt;code&gt;DeleteSnapshot&lt;/code&gt;, and &lt;code&gt;ListSnapshots&lt;/code&gt;. For details, see the &lt;a href=&#34;https://github.com/container-storage-interface/spec/blob/master/spec.md&#34; target=&#34;_blank&#34;&gt;CSI spec&lt;/a&gt; and the &lt;a href=&#34;https://kubernetes-csi.github.io/docs/snapshot-restore-feature.html&#34; target=&#34;_blank&#34;&gt;Kubernetes-CSI Driver Developer Guide&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Although Kubernetes is as minimally prescriptive on the packaging and deployment of a CSI Volume Driver as possible, it provides a suggested mechanism for deploying an arbitrary containerized CSI driver on Kubernetes to simplify deployment of containerized CSI compatible volume drivers.&lt;/p&gt;

&lt;p&gt;As part of this recommended deployment process, the Kubernetes team provides a number of sidecar (helper) containers, including the &lt;a href=&#34;https://kubernetes-csi.github.io/docs/external-snapshotter.html&#34; target=&#34;_blank&#34;&gt;external-snapshotter sidecar&lt;/a&gt; container.&lt;/p&gt;

&lt;p&gt;The external-snapshotter watches the Kubernetes API server for VolumeSnapshotContent object and triggers &lt;code&gt;CreateSnapshot&lt;/code&gt; and &lt;code&gt;DeleteSnapshot&lt;/code&gt; operations against a CSI endpoint. The CSI &lt;a href=&#34;https://kubernetes-csi.github.io/docs/external-provisioner.html&#34; target=&#34;_blank&#34;&gt;external-provisioner sidecar container&lt;/a&gt; has also been updated to support restoring volume from snapshot using the dataSource PVC field.&lt;/p&gt;

&lt;p&gt;In order to support snapshot feature, it is recommended that storage vendors deploy the external-snapshotter sidecar containers in addition to the external provisioner, along with their CSI driver.&lt;/p&gt;

&lt;h2 id=&#34;what-are-the-limitations-of-beta&#34;&gt;What are the limitations of beta?&lt;/h2&gt;

&lt;p&gt;The beta implementation of volume snapshots for Kubernetes has the following limitations:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Does not support reverting an existing volume to an earlier state represented by a snapshot (beta only supports provisioning a new volume from a snapshot).&lt;/li&gt;
&lt;li&gt;No snapshot consistency guarantees beyond any guarantees provided by storage system (e.g. crash consistency). These are the responsibility of higher level APIs/controllers&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;what-s-next&#34;&gt;What’s next?&lt;/h2&gt;

&lt;p&gt;Depending on feedback and adoption, the Kubernetes team plans to push the CSI Snapshot implementation to GA in either 1.18 or 1.19. Some of the features we are interested in supporting include consistency groups, application consistent snapshots, workload quiescing, in-place restores, volume backups, and more.&lt;/p&gt;

&lt;h2 id=&#34;how-can-i-learn-more&#34;&gt;How can I learn more?&lt;/h2&gt;

&lt;p&gt;You can also have a look at the &lt;a href=&#34;https://github.com/kubernetes-csi/external-snapshotter&#34; target=&#34;_blank&#34;&gt;external-snapshotter source code repository&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Check out additional documentation on the snapshot feature &lt;a href=&#34;http://k8s.io/docs/concepts/storage/volume-snapshots&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://kubernetes-csi.github.io/docs/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;how-do-i-get-involved&#34;&gt;How do I get involved?&lt;/h2&gt;

&lt;p&gt;This project, like all of Kubernetes, is the result of hard work by many contributors from diverse backgrounds working together.&lt;/p&gt;

&lt;p&gt;We offer a huge thank you to the contributors who stepped up these last few quarters to help the project reach Beta:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Xing Yang (xing-yang)&lt;/li&gt;
&lt;li&gt;Xiangqian Yu (yuxiangqian)&lt;/li&gt;
&lt;li&gt;Jing Xu (jingxu97)&lt;/li&gt;
&lt;li&gt;Grant Griffiths (ggriffiths)&lt;/li&gt;
&lt;li&gt;Can Zhu (zhucan)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With special thanks to the following people for their insightful reviews and thorough consideration with the design:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Michelle Au (msau42)&lt;/li&gt;
&lt;li&gt;Saad Ali (saadali)&lt;/li&gt;
&lt;li&gt;Patrick Ohly (pohly)&lt;/li&gt;
&lt;li&gt;Tim Hockin (thockin)&lt;/li&gt;
&lt;li&gt;Jordan Liggitt (liggitt).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Those interested in getting involved with the design and development of CSI or any part of the Kubernetes Storage system, join the &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-storage&#34; target=&#34;_blank&#34;&gt;Kubernetes Storage Special Interest Group (SIG)&lt;/a&gt;. We’re rapidly growing and always welcome new contributors.&lt;/p&gt;

&lt;p&gt;We also hold regular &lt;a href=&#34;https://docs.google.com/document/d/1qdfvAj5O-tTAZzqJyz3B-yczLLxOiQd-XKpJmTEMazs/edit?usp=sharing&#34; target=&#34;_blank&#34;&gt;SIG-Storage Snapshot Working Group meetings&lt;/a&gt;. New attendees are welcome to join for design and development discussions.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes 1.17 Feature: Kubernetes In-Tree to CSI Volume Migration Moves to Beta</title>
      <link>https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-csi-migration-beta/</link>
      <pubDate>Mon, 09 Dec 2019 09:00:00 +0800</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-csi-migration-beta/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; David Zhu, Software Engineer, Google&lt;/p&gt;

&lt;p&gt;The Kubernetes in-tree storage plugin to &lt;a href=&#34;https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/&#34; target=&#34;_blank&#34;&gt;Container Storage Interface (CSI)&lt;/a&gt; migration infrastructure is now beta in Kubernetes v1.17. CSI migration was introduced as alpha in Kubernetes v1.14.&lt;/p&gt;

&lt;p&gt;Kubernetes features are generally introduced as alpha and moved to beta (and eventually to stable/GA) over subsequent Kubernetes releases. This process allows Kubernetes developers to get feedback, discover and fix issues, iterate on the designs, and deliver high quality, production grade features.&lt;/p&gt;

&lt;h2 id=&#34;why-are-we-migrating-in-tree-plugins-to-csi&#34;&gt;Why are we migrating in-tree plugins to CSI?&lt;/h2&gt;

&lt;p&gt;Prior to CSI, Kubernetes provided a powerful volume plugin system. These volume plugins were “in-tree” meaning their code was part of the core Kubernetes code and shipped with the core Kubernetes binaries. However, adding support for new volume plugins to Kubernetes was challenging. Vendors that wanted to add support for their storage system to Kubernetes (or even fix a bug in an existing volume plugin) were forced to align with the Kubernetes release process. In addition, third-party storage code caused reliability and security issues in core Kubernetes binaries and the code was often difficult (and in some cases impossible) for Kubernetes maintainers to test and maintain. Using the Container Storage Interface in Kubernetes resolves these major issues.&lt;/p&gt;

&lt;p&gt;As more CSI Drivers were created and became production ready, we wanted all Kubernetes users to reap the benefits of the CSI model. However, we did not want to force users into making workload/configuration changes by breaking the existing generally available storage APIs. The way forward was clear - we would have to replace the backend of the “in-tree plugin” APIs with CSI.&lt;/p&gt;

&lt;h2 id=&#34;what-is-csi-migration&#34;&gt;What is CSI migration?&lt;/h2&gt;

&lt;p&gt;The CSI migration effort enables the replacement of existing in-tree storage plugins such as &lt;code&gt;kubernetes.io/gce-pd&lt;/code&gt; or &lt;code&gt;kubernetes.io/aws-ebs&lt;/code&gt; with a corresponding &lt;a href=&#34;https://kubernetes-csi.github.io/docs/introduction.html&#34; target=&#34;_blank&#34;&gt;CSI driver&lt;/a&gt;. If CSI Migration is working properly, Kubernetes end users shouldn’t notice a difference. After migration, Kubernetes users may continue to rely on all the functionality of in-tree storage plugins using the existing interface.&lt;/p&gt;

&lt;p&gt;When a Kubernetes cluster administrator updates a cluster to enable CSI migration, existing stateful deployments and workloads continue to function as they always have; however, behind the scenes Kubernetes hands control of all storage management operations (previously targeting in-tree drivers) to CSI drivers.&lt;/p&gt;

&lt;p&gt;The Kubernetes team has worked hard to ensure the stability of storage APIs and for the promise of a smooth upgrade experience. This involves meticulous accounting of all existing features and behaviors to ensure backwards compatibility and API stability. You can think of it like changing the wheels on a racecar while it’s speeding down the straightaway.&lt;/p&gt;

&lt;h2 id=&#34;how-to-try-out-csi-migration-for-existing-plugins&#34;&gt;How to try out CSI migration for existing plugins?&lt;/h2&gt;

&lt;p&gt;If you are Kubernetes distributor that deploys in one of the environments listed below, now would be a good time to start testing the CSI migration and figuring out how to deploy/manage the appropriate CSI driver.&lt;/p&gt;

&lt;p&gt;To try out CSI migration in beta for an existing plugin you must be using Kubernetes v1.17 or higher. First, you must update/create a Kubernetes cluster with the feature flags &lt;code&gt;CSIMigration&lt;/code&gt; (on by default in 1.17) and &lt;code&gt;CSIMigration{provider}&lt;/code&gt; (off by default) enabled on all Kubernetes components (master and node). Where {provider} is the in-tree cloud provider storage type that is used in your cluster. Please note that during a cluster upgrade you must drain each node (remove running workloads) before updating or changing configuration of your Kubelet. You may also see an optional &lt;code&gt;CSIMigration{provider}Complete&lt;/code&gt; flag that you &lt;em&gt;may&lt;/em&gt; enable if all of your nodes have CSI migration enabled.&lt;/p&gt;

&lt;p&gt;You must also install the requisite CSI driver on your cluster - instructions for this can generally be found from you provider of choice. CSI migration is available for GCE Persistent Disk and AWS Elastic Block Store in beta as well as for Azure File/Disk and Openstack Cinder in alpha. Kubernetes distributors should look at automating the deployment and management (upgrade, downgrade, etc.) of the CSI Drivers they will depend on.&lt;/p&gt;

&lt;p&gt;To verify the feature flag is enabled and driver installed on a particular node you can get the CSINode object. You should see the in-tree plugin name of the migrated plugin as well as your [installed] driver in the drivers list.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get csinodes -o yaml&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;storage.k8s.io/v1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;CSINode&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;annotations:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;storage.alpha.kubernetes.io/migrated-plugins:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;kubernetes.io/gce-pd&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;test-node&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;...&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;drivers:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;pd.csi.storage.gke.io&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;...&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;After the above set up is complete you can confirm that your cluster has functioning CSI migration by deploying a stateful workload using the legacy APIs.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;v1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;PersistentVolumeClaim&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;test-disk&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;storageClassName:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;standard&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;accessModes:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;ReadWriteOnce&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;resources:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;requests:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;storage:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;10Gi&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;---&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;v1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Pod&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;web-server&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;containers:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;   &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;web-server&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;     &lt;/span&gt;image:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;nginx&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;     &lt;/span&gt;volumeMounts:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;       &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;mountPath:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/var/lib/www/html&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;         &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;mypvc&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;volumes:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;   &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;mypvc&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;     &lt;/span&gt;persistentVolumeClaim:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;       &lt;/span&gt;claimName:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;test-disk&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Verify that the pod is RUNNING after some time&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get pods web-server&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;NAME         READY   STATUS    RESTARTS   AGE
web-server   1/1     Running   0          39s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To confirm that the CSI driver is actually serving your requests it may be prudent to check the container logs of the CSI Driver after exercising the storage management operations. Note that your container logs may look different depending on the provider used.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl logs &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;CSIdriverPodName&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt; --container&lt;span style=&#34;color:#666&#34;&gt;={&lt;/span&gt;CSIdriverContainerName&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;/csi.v1.Controller/ControllerPublishVolume called with request: ...
Attaching disk ... to ...
ControllerPublishVolume succeeded for disk ... to instance ...
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;current-limitations&#34;&gt;Current limitations&lt;/h2&gt;

&lt;p&gt;Although CSI migration is now beta there is one major limitation that prevents us from turning it on by default. Turning on migration still requires a cluster administrator to install a CSI driver before storage functionality is seamlessly handed over. We are currently working with SIG-CloudProvider to provide a frictionless experience of bundling the required CSI Drivers with cloud distributions.&lt;/p&gt;

&lt;h2 id=&#34;what-is-the-timeline-status&#34;&gt;What is the timeline/status?&lt;/h2&gt;

&lt;p&gt;The timeline for CSI migration is actually set by the cloud provider extraction project. It is part of the effort to remove all cloud provider code from Kubernetes. By migrating cloud storage plugins to external CSI drivers we are able to extract out all the cloud provider dependencies.&lt;/p&gt;

&lt;p&gt;Although the overall feature is beta and not on by default, there is still work to be done on a per-plugin basis. Currently only GCE PD and AWS EBS have gone beta with Migration and yet both are still off by default since they depend on a manual installation of their respective CSI Drivers. Azure File/Disk, OpenStack, and VMWare plugins are currently in less mature states and non-cloud plugins such as NFS, Portworx, RBD etc are still in the planning stages.&lt;/p&gt;

&lt;p&gt;The current and targeted releases for each individual cloud driver is shown in the table below:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Driver&lt;/th&gt;
&lt;th&gt;Alpha&lt;/th&gt;
&lt;th&gt;Beta (in-tree deprecated)&lt;/th&gt;
&lt;th&gt;GA&lt;/th&gt;
&lt;th&gt;Target &amp;ldquo;in-tree plugin&amp;rdquo; removal&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;AWS EBS&lt;/td&gt;
&lt;td&gt;1.14&lt;/td&gt;
&lt;td&gt;1.17&lt;/td&gt;
&lt;td&gt;1.19 (Target)&lt;/td&gt;
&lt;td&gt;1.21&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;GCE PD&lt;/td&gt;
&lt;td&gt;1.14&lt;/td&gt;
&lt;td&gt;1.17&lt;/td&gt;
&lt;td&gt;1.19 (Target)&lt;/td&gt;
&lt;td&gt;1.21&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;OpenStack Cinder&lt;/td&gt;
&lt;td&gt;1.14&lt;/td&gt;
&lt;td&gt;1.18 (Target)&lt;/td&gt;
&lt;td&gt;1.19 (Target)&lt;/td&gt;
&lt;td&gt;1.21&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Azure Disk + File&lt;/td&gt;
&lt;td&gt;1.15&lt;/td&gt;
&lt;td&gt;1.18 (Target)&lt;/td&gt;
&lt;td&gt;1.19 (Target)&lt;/td&gt;
&lt;td&gt;1.21&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;VSphere&lt;/td&gt;
&lt;td&gt;1.18 (Target)&lt;/td&gt;
&lt;td&gt;1.19 (Target)&lt;/td&gt;
&lt;td&gt;1.20 (Target)&lt;/td&gt;
&lt;td&gt;1.22&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;what-s-next&#34;&gt;What&amp;rsquo;s next?&lt;/h2&gt;

&lt;p&gt;Major upcoming work includes implementing and hardening CSI migration for the remaining in-tree plugins, installing CSI Drivers by default in distributions, turning on CSI migration by default, and finally removing all in-tree plugin code as a part of cloud provider extraction. We expect to complete this project including the full switch to “on-by-default” migration by Kubernetes v1.21.&lt;/p&gt;

&lt;h2 id=&#34;what-should-i-do-as-a-user&#34;&gt;What should I do as a user?&lt;/h2&gt;

&lt;p&gt;Note that all new features for the Kubernetes storage system (like volume snapshotting) will only be added to the CSI interface. Therefore, if you are starting up a new cluster, creating stateful applications for the first time, or require these new features we recommend using CSI drivers natively (instead of the in-tree volume plugin API). Follow the &lt;a href=&#34;https://kubernetes-csi.github.io/docs/drivers.html&#34; target=&#34;_blank&#34;&gt;updated user guides for CSI drivers&lt;/a&gt; and use the new CSI APIs.&lt;/p&gt;

&lt;p&gt;However, if you choose to roll a cluster forward or continue using specifications with the legacy volume APIs, CSI Migration will ensure we continue to support those deployments with the new CSI drivers.&lt;/p&gt;

&lt;h2 id=&#34;how-do-i-get-involved&#34;&gt;How do I get involved?&lt;/h2&gt;

&lt;p&gt;The Kubernetes Slack channel csi-migration along with any of the standard &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/sig-storage/README.md#contact&#34; target=&#34;_blank&#34;&gt;SIG Storage communication channels&lt;/a&gt; are great mediums to reach out to the SIG Storage and migration working group teams.&lt;/p&gt;

&lt;p&gt;This project, like all of Kubernetes, is the result of hard work by many contributors from diverse backgrounds working together. We offer a huge thank you to the contributors who stepped up these last quarters to help the project reach Beta:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;David Zhu&lt;/li&gt;
&lt;li&gt;Deep Debroy&lt;/li&gt;
&lt;li&gt;Cheng Pan&lt;/li&gt;
&lt;li&gt;Jan Šafránek&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With special thanks to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Michelle Au&lt;/li&gt;
&lt;li&gt;Saad Ali&lt;/li&gt;
&lt;li&gt;Jonathan Basseri&lt;/li&gt;
&lt;li&gt;Fabio Bertinatto&lt;/li&gt;
&lt;li&gt;Ben Elder&lt;/li&gt;
&lt;li&gt;Andrew Sy Kim&lt;/li&gt;
&lt;li&gt;Hemant Kumar&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For fruitful dialogues, insightful reviews, and thorough consideration of CSI migration in other features.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: When you&#39;re in the release team, you&#39;re family: the Kubernetes 1.16 release interview</title>
      <link>https://kubernetes.io/blog/2019/12/06/when-youre-in-the-release-team-youre-family-the-kubernetes-1.16-release-interview/</link>
      <pubDate>Fri, 06 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/12/06/when-youre-in-the-release-team-youre-family-the-kubernetes-1.16-release-interview/</guid>
      <description>
        
        
        &lt;p&gt;&lt;b&gt;Author&lt;/b&gt;: Craig Box (Google)&lt;/p&gt;

&lt;p&gt;It is a pleasure to co-host the weekly &lt;a href=&#34;https://kubernetespodcast.com/&#34; target=&#34;_blank&#34;&gt;Kubernetes Podcast from Google&lt;/a&gt; with Adam Glick.  We get to talk to friends old and new from the community, as well as give people a download on the Cloud Native news every week.&lt;/p&gt;

&lt;p&gt;It was also a pleasure to see Lachlan Evenson, the release team lead for Kubernetes 1.16, &lt;a href=&#34;https://www.cncf.io/announcement/2019/11/19/cloud-native-computing-foundation-announces-2019-community-awards-winners/&#34; target=&#34;_blank&#34;&gt;win the CNCF &amp;ldquo;Top Ambassador&amp;rdquo; award&lt;/a&gt; at KubeCon.  We &lt;a href=&#34;https://kubernetespodcast.com/episode/072-kubernetes-1.16/&#34; target=&#34;_blank&#34;&gt;talked with Lachie&lt;/a&gt; when 1.16 was released, and as is &lt;a href=&#34;https://kubernetes.io/blog/2018/07/16/how-the-sausage-is-made-the-kubernetes-1.11-release-interview-from-the-kubernetes-podcast/&#34; target=&#34;_blank&#34;&gt;becoming&lt;/a&gt; a &lt;a href=&#34;https://kubernetes.io/blog/2019/05/13/cat-shirts-and-groundhog-day-the-kubernetes-1.14-release-interview/&#34; target=&#34;_blank&#34;&gt;tradition&lt;/a&gt;, we are delighted to share an abridged version of that interview with the readers of the Kubernetes Blog.&lt;/p&gt;

&lt;p&gt;If you&amp;rsquo;re paying attention to the release calendar, you&amp;rsquo;ll see 1.17 is due out soon. &lt;a href=&#34;https://kubernetespodcast.com/subscribe/&#34; target=&#34;_blank&#34;&gt;Subscribe to our show&lt;/a&gt; in your favourite podcast player for another release interview!&lt;/p&gt;

&lt;hr/&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: Lachie, I&amp;rsquo;ve been looking forward to chatting to you for some time. We first met at KubeCon Berlin in 2017 when you were with Deis. Let&amp;rsquo;s start with a question on everyone&amp;rsquo;s ears&amp;ndash; which part of England are you from?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: The prison part! See, we didn&amp;rsquo;t have a choice about going to Australia, but I&amp;rsquo;d like to say we got the upper hand in the long run. We got that beautiful country, so yes, from Australia, the southern part of England&amp;ndash; the southern tip.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: We did set that question up a little bit. I&amp;rsquo;m actually in Australia this week, and I&amp;rsquo;ll let you know it&amp;rsquo;s quite a nice place. I can&amp;rsquo;t imagine why you would have left.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: Yeah, it seems fitting that you&amp;rsquo;re interviewing an Australian from Australia, and that Australian is in San Francisco.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: Oh, well, thank you very much for joining us and making it work. This is the third in our occasional series of release lead interviews. We talked to Josh and Tim from Red Hat and VMware, respectively, in &lt;a href=&#34;https://kubernetespodcast.com/episode/010-kubernetes-1.11/&#34; target=&#34;_blank&#34;&gt;episode 10&lt;/a&gt;, and we talked to Aaron from Google in &lt;a href=&#34;https://kubernetespodcast.com/episode/046-kubernetes-1.14/&#34; target=&#34;_blank&#34;&gt;episode 46&lt;/a&gt;. And we asked all three how their journey in cloud-native started. What was your start in cloud-native?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: I remember back in early 2014, I was working for a company called Lithium Technologies. We&amp;rsquo;d been using containers for quite some time, and my boss at the time had put a challenge out to me&amp;ndash; go and find a way to orchestrate these containers, because they seem to be providing quite a bit of value to our developer velocity.&lt;/p&gt;

&lt;p&gt;He gave me a week, and he said, go and check out both Mesos and Kubernetes. And at the end of that week, I had Kubernetes up and running, and I had workloads scheduled. I was a little bit more challenged on the Mesos side, but Kubernetes was there, and I had it up and running. And from there, I actually went and was offered to speak at the Kubernetes 1.0 launch in OSCOM in Portland in 2014, I believe.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: So, a real early adopter?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: Really, really early. I remember, I think, I started in 0.8, before CrashLoopBackOff was a thing. I remember writing that thing myself.&lt;/p&gt;

&lt;p&gt;[LAUGHING]&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: You were contributing to the code at that point as well?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: I was just a user. I was part of the community at that point, but from a user perspective. I showed up to things like the community meeting. I remember meeting Sarah Novotny in the very early years of the community meeting, and I spent some time in SIG Apps, so really looking at how people were putting workloads onto Kubernetes&amp;ndash; so going through that whole process.&lt;/p&gt;

&lt;p&gt;It turned out we built some tools like Helm, before Helm existed, to facilitate rollout and putting applications onto Kubernetes. And then, once Helm existed, that&amp;rsquo;s when I met the folks from Deis, and I said, hey, I think you want to get rid of this code that we&amp;rsquo;ve built internally and then go and use the open-source code that Helm provided.&lt;/p&gt;

&lt;p&gt;So we got into the Helm ecosystem there, and I subsequently went and worked for Deis, specifically on professional services&amp;ndash; helping people out in the community with their Kubernetes journey. And that was when we actually met, Craig, back in Berlin. It seems, you know, I say container years are like dog years; it&amp;rsquo;s 7:1.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: Right.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: Seven years ago, we were about 50 years&amp;ndash; much younger.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: That sounds like the same ratio as kangaroos to people in Australia.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: It&amp;rsquo;s much the same arithmetic, yes.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: What was the most interesting implementation that you ran into at that time?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: There wasn&amp;rsquo;t a lot of the workload APIs. Back in 1.0, there wasn&amp;rsquo;t even Deployments. There wasn&amp;rsquo;t Ingress. Back in the day, there were a lot of people in those points trying to build those workload APIs on top of Kubernetes, but they didn&amp;rsquo;t actually have any way to extend Kubernetes itself. There were no third-party resources. There were no operators, no custom resources.&lt;/p&gt;

&lt;p&gt;A lot of people are actually trying to figure out how to interact with the Kubernetes API and deliver things like deployments, because you just had&amp;ndash; in those days, you didn&amp;rsquo;t have replica sets. You had a ReplicationController that we called the RC, back in the day. You didn&amp;rsquo;t have a lot of these things that we take for granted today. There wasn&amp;rsquo;t RBAC. There wasn&amp;rsquo;t a lot of the things that we have today.&lt;/p&gt;

&lt;p&gt;So it&amp;rsquo;s great to have seen and been a part of the Kubernetes community from 0.8 to 1.16, and actually leading that release. So I&amp;rsquo;ve seen a lot, and it&amp;rsquo;s been a wonderful part of my adventures in open-source.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: You were also part of the Deis team that transitioned and became a part of the Microsoft team. What was that transition like, from small startup to joining a large player in the cloud and technology community?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: It was fantastic. When we came on board with Microsoft, they didn&amp;rsquo;t have a managed Kubernetes offering, and we were brought on to try and seed that. There was also a bigger part that we were actually building open-source tools to help people in the community integrate. We had the autonomy with&amp;ndash; Brendan Burns was on the team. We had Gabe Monroy. And we really had that top-down autonomy that was believing and placing a bet on open-source and helping us build tools and give us that autonomy to go and solve problems in open-source, along with contributing to things like Kubernetes.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m part of the upstream team from a PM perspective, and we have a bunch of engineers, a bunch of PMs that are actually working on these things in the Cloud Native Compute Foundation to help folks integrate their workloads into things like Kubernetes and build and aid their cloud-native journeys.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: There are a number of new tools, and specifications, and so on that are still coming out from Microsoft under the Deis brand. That must be exciting to you as one of the people who joined from Deis initially.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: Yeah, absolutely. We really took that Deis brand&amp;ndash; it&amp;rsquo;s now Deis Labs&amp;ndash; but we really wanted this a home to signal to the community that we were building things in the hope to put them out into foundation. You may see things like CNAB, Cloud Native Application Bundles. I know &lt;a href=&#34;https://kubernetespodcast.com/episode/061-cnab/&#34; target=&#34;_blank&#34;&gt;you&amp;rsquo;ve had both Ralph and Jeremy on the show before&lt;/a&gt; talking about CNAB, SMI - Service Mesh Interface, other tooling in the ecosystem where we want to signal to the community that we want to go give that to a foundation. We really want a neutral place to begin that nascent work, but then things, for example, Virtual Kubelet started there as well, and it went out into the Cloud Native Compute Foundation.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: Is there any consternation about the fact that Phippy has become the character people look to rather than the actual &amp;ldquo;Captain Kube&amp;rdquo; owl, in the &lt;a href=&#34;https://www.cncf.io/phippy/&#34; target=&#34;_blank&#34;&gt;family of donated characters&lt;/a&gt;?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: Yes, so it&amp;rsquo;s interesting because I didn&amp;rsquo;t actually work on that project back at Deis, but the Deis folks, Karen Chu and Matt Butcher actually created &amp;ldquo;The Children&amp;rsquo;s Guide to Kubernetes,&amp;rdquo; which I thought was fantastic.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: Totally.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: Because I could sit down and read it to my parents, as well, and tell them&amp;ndash; it wasn&amp;rsquo;t for children. It was more for the adults in my life, I like to say. And so when I give out a copy of that book, I&amp;rsquo;m like, take it home and read it to mum. She might actually understand what you do by the end of that book.&lt;/p&gt;

&lt;p&gt;But it was really a creative way, because this was back in that nascent Kubernetes where people were trying to get their head around those concepts&amp;ndash; what is a pod? What is a secret? What is a namespace? Having that vehicle of a fun set of characters&amp;ndash;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: Yep.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: And Phippy is a PHP app. Remember them? So yeah, it&amp;rsquo;s totally in line with the things that we&amp;rsquo;re seeing people want to containerize and put onto Kubernetes at that. But Phippy is still cute. I was questioned last week about Captain Kube, as well, on the release logo, so we could talk about that a little bit more. But there&amp;rsquo;s a swag of characters in there that are quite cute and illustrate the fun concept behind the Kubernetes community.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: &lt;a href=&#34;https://kubernetes.io/blog/2019/09/18/kubernetes-1-16-release-announcement/&#34; target=&#34;_blank&#34;&gt;1.16 has just been released&lt;/a&gt;. You were the release team lead for that&amp;ndash; congratulations.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: Thank you very much. It was a pleasure to serve the community.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: What are the headline announcements in Kubernetes 1.16?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: Well, I think there are a few. Custom Resources hit GA. Now, that is a big milestone for extensibility and Kubernetes. I know we&amp;rsquo;ve spoken about them for some time&amp;ndash; custom resources were introduced in 1.7, and we&amp;rsquo;ve been trying to work through that ecosystem to bring the API up to a GA standard. So it hit GA, and I think a lot of the features that went in as part of the GA release will help people in the community that are writing operators.&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s a lot of lifecycle management, a lot of tooling that you can put into the APIs themselves. Doing strict dependency checks&amp;ndash; you can do typing, you can do validation, you can do pruning superfluous fields, and allowing for that ecosystem of operators and extensibility in the community to exist on top of Kubernetes.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s been a long road to get to GA for Custom Resources, but it&amp;rsquo;s great now that they&amp;rsquo;re here and people can really bank on that being an API they can use to extend Kubernetes. So I&amp;rsquo;d say that&amp;rsquo;s a large headline feature. The metrics overhaul, as well&amp;ndash; I know this was on the release blog.&lt;/p&gt;

&lt;p&gt;The metrics team have actually tried to standardize the metrics in Kubernetes and put them through the same paces as all other enhancements that go into Kubernetes. So they&amp;rsquo;re really trying to put through, what are the criteria? How do we make them standard? How do we test them? How to make sure that they&amp;rsquo;re extensible? So it was great to see that team actually step up and create stable metrics that everybody can build and stack on.&lt;/p&gt;

&lt;p&gt;Finally, there were some other additions to CSI, as well. Volume resizing was added. This is a maturity story around the Container Storage Interface, which was introduced several releases ago in GA. But really, you&amp;rsquo;ve seen volume providers actually build on that interface and that interface get a little bit more broader to adopt things like &amp;ldquo;I want to resize dynamically at runtime on my storage volume&amp;rdquo;.  That&amp;rsquo;s a great story as well, for those providers out there.&lt;/p&gt;

&lt;p&gt;I think they&amp;rsquo;re the big headline features for 1.16, but there are a slew. There were 31 enhancements that went into Kubernetes 1.16. And I know there have been questions out there in the community saying, well, how do we decide what&amp;rsquo;s stable? Eight of those were stable, eight of those were beta, and the rest of those features, the 15 remaining, were actually in alpha. There were quite a few things that went from alpha into beta and beta into stable, so I think that&amp;rsquo;s a good progression for the release, as well.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: As you&amp;rsquo;ve looked at all these, which of them is your personal favorite?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: I probably have two. One is a little bit biased, but I personally worked on, with the &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/dual-stack/&#34; target=&#34;_blank&#34;&gt;dual-stack&lt;/a&gt; team in the community. Dual-stack is the ability to give IPv4 and IPv6 addresses to both pods and services. And I think where this is interesting in the community is Kubernetes is becoming a runtime that is going to new spaces. Think IoT, think edge, think cloud edge.&lt;/p&gt;

&lt;p&gt;When you&amp;rsquo;re pushing Kubernetes into these new operational environments, things like addressing may become a problem, where you might want to run thousands and thousands of pods which all need IP addresses. So, having that same crossover point where I can have v4 and v6 at the same time, get comfortable with v6, I think Kubernetes may be an accelerator to v6 adoption through things like IoT workloads on top of Kubernetes.&lt;/p&gt;

&lt;p&gt;The other one is &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/&#34; target=&#34;_blank&#34;&gt;Endpoint Slices&lt;/a&gt;. Endpoint slices is about scaling. As you may know, services have endpoints attached to them, and endpoints are all the pod IPs that actually match that label selector on a service. Now, when you have large clusters, you can imagine the number of pod IPs being attached to that service growing to tens of thousands. And when you update that, everything that actually watches those service endpoints needs to get an update, which is the delta change over time, which gets rather large as things are being attached, added, and removed, as is the dynamic nature of Kubernetes.&lt;/p&gt;

&lt;p&gt;But what endpoint slices makes available is you can actually slice those endpoints up into groups of 100 and then only update the ones that you really need to worry about, which means as a scaling factor, we don&amp;rsquo;t need to update everybody listening into tens of thousands of updates. We only need to update a subsection. So I&amp;rsquo;d say they&amp;rsquo;re my two highlights, yeah.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: Are there any early stage or alpha features that you&amp;rsquo;re excited to see where they go personally?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: Personally, &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/&#34; target=&#34;_blank&#34;&gt;ephemeral containers&lt;/a&gt;. The tooling that you have available at runtime in a pod is dependent on the constituents or the containers that are part of that pod. And what we&amp;rsquo;ve seen in containers being built by scratch and tools like &lt;a href=&#34;https://github.com/GoogleContainerTools/distroless&#34; target=&#34;_blank&#34;&gt;distroless&lt;/a&gt; from the folks out of Google, where you can build scratch containers that don&amp;rsquo;t actually have any tooling inside them but just the raw compiled binaries, if you want to go in and debug that at runtime, it&amp;rsquo;s incredibly difficult to insert something in.&lt;/p&gt;

&lt;p&gt;And this is where ephemeral containers come in. I can actually insert a container into a running pod&amp;ndash; and let&amp;rsquo;s just call that a debug container&amp;ndash; that has all my slew of tools that I need to debug that running workload, and I can insert that into a pod at runtime. So I think ephemeral containers is a really interesting feature that&amp;rsquo;s been included in 1.16 in alpha, which allows a greater debugging story for the Kubernetes community.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: What feature that slipped do you wish would have made it into the release?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: The feature that slipped that I was a little disappointed about was &lt;a href=&#34;https://github.com/kubernetes/enhancements/blob/master/keps/sig-apps/sidecarcontainers.md&#34; target=&#34;_blank&#34;&gt;sidecar containers&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: Right.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: In the world of service meshes, you may want to order the start of some containers, and it&amp;rsquo;s very specific to things like service meshes in the case of the data plane. I need the Envoy sidecar to start before everything else so that it can wire up the networking.&lt;/p&gt;

&lt;p&gt;The inverse is true as well. I need it to stop last. Sidecar containers gave you that ordered start. And what we see a lot of people doing in the ecosystem is just laying down one sidecar per node as a DaemonSet, and they want that to start before all the other pods on the machine. Or if it&amp;rsquo;s inside the pod, or the context of one pod, they want to say that sidecar needs to stop before all the other containers in a pod. So giving you that ordered guarantee, I think, is really interesting and is really hot, especially given the service mesh ecosystem heating up.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: This release &lt;a href=&#34;https://kubernetes.io/blog/2019/07/18/api-deprecations-in-1-16/&#34; target=&#34;_blank&#34;&gt;deprecates a few beta API groups&lt;/a&gt;, for things like ReplicaSets and Deployments. That will break deployment for the group of people who have just taken example code off the web and don&amp;rsquo;t really understand it. The GA version of these APIs were released in 1.9, so it&amp;rsquo;s obviously a long time ago. There&amp;rsquo;s been a lot of preparation going into this. But what considerations and concerns have we had about the fact that these are now being deprecated in this particular release?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: Let me start by saying that this is the first release that we&amp;rsquo;ve had a big API deprecation, so the proof is going to be in the pudding. And we do have an API deprecation policy. So as you mentioned, Craig, the apps/v1 group has been around since 1.9. If you go and read the &lt;a href=&#34;https://kubernetes.io/docs/reference/using-api/deprecation-policy/&#34; target=&#34;_blank&#34;&gt;API deprecation policy&lt;/a&gt;, you can see that we have a three-release announcement. Around the 1.12, 1.13 time frame, we actually went and announced this deprecation, and over the last few releases, we&amp;rsquo;ve been reiterating that.&lt;/p&gt;

&lt;p&gt;But really, what we want to do is get the whole community on those stable APIs because it really starts to become a problem when we&amp;rsquo;re supporting all these many now-deprecated APIs, and people are building tooling around them and trying to build reliable tooling. So this is the first test for us to move people, and I&amp;rsquo;m sure it will break a lot of tools that depend on things. But I think in the long run, once we get onto those stable APIs, people can actually guarantee that their tools work, and it&amp;rsquo;s going to become easier in the long run.&lt;/p&gt;

&lt;p&gt;So we&amp;rsquo;ve put quite a bit of work in announcing this. There was a blog sent out about six months ago by Valerie Lancey in the Kubernetes community which said, hey, go use &amp;lsquo;kubectl convert&amp;rsquo;, where you can actually say, I want to convert this resource from this API version to that API version, and it actually makes that really easy. But I think there&amp;rsquo;ll be some problems in the ecosystem, but we need to do this going forward, pruning out the old APIs and making sure that people are on the stable ones.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: Congratulations on the release of 1.16. Obviously, that&amp;rsquo;s a big thing. It must have been a lot of work for you. Can you talk a little bit about what went into leading this release?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: The job of the release lead is to oversee throughout the process of the release and make sure that the release gets out the door on a specific schedule. So really, what that is is wrangling a lot of different resources and a lot of different people in the community, and making sure that they show up and do the things that they are committed to as part of their duties as either SIG chairs or other roles in the community, and making sure that enhancements are in the right state, and code shows up at the right time, and that things are looking green.&lt;/p&gt;

&lt;p&gt;A lot of it is just making sure you know who to contact and how to contact them, and ask them to actually show up. But when I was asked at the end of the 1.15 release cycle if I would lead, you have to consider how much time it&amp;rsquo;s going to take and the scheduling, where hours a week are dedicated to making sure that this release actually hits the shelves on time and is of a certain quality. So there is lots of pieces to that.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: Had you been on the path through the shadow program for release management?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: Yeah, I had. I actually joined the shadow program&amp;ndash; so the shadow program for the release team. The Kubernetes release team is tasked with staffing a specific release, and I came in the 1.14 release under the lead of Aaron Crickenberger. And I was an enhancement shadow at that point. I was really interested in how KEPs worked, so the Kubernetes Enhancement Proposal work. I wanted to make sure that I understood that part of the release team, and I came in and helped in that release.&lt;/p&gt;

&lt;p&gt;And then, in 1.15, I was asked if I could be a lead shadow. And the lead shadow is to stand alongside the lead and help the lead fill their duties. So if they&amp;rsquo;re out, if they need people to wrangle different parts of the community, I would go out and do that. I&amp;rsquo;ve served on three releases at this point&amp;ndash; 1.14, 1.15, and 1.16.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: Thank you for your service.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: Absolutely, it&amp;rsquo;s my pleasure.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: Release lead emeritus is the next role for you, I assume?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: [LAUGHS] Yes. We also have a new role on the release lead team called Emeritus Advisors, which are actually to go back and help answer the questions of, why was this decision made? How can we do better? What was this like in the previous release? So we do have that continuity, and in 1.17, we have the old release lead from 1.15. Claire Lawrence is coming back to fill in as emeritus advisor. So that is something we do take.&lt;/p&gt;

&lt;p&gt;And I think for the shadow program in general, the release team is a really good example of how you can actually build continuity across releases in an open-source fashion. We &lt;a href=&#34;https://www.youtube.com/watch?v=ritHCLd2xeE&#34; target=&#34;_blank&#34;&gt;actually have a session at KubeCon San Diego&lt;/a&gt; on how that shadowing program works. But it&amp;rsquo;s really to get people excited about how we can do mentoring in open-source communities and make sure that the project goes on after all of us have rolled on and off the team.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: Speaking of the team, &lt;a href=&#34;https://github.com/kubernetes/sig-release/tree/master/releases/release-1.16&#34; target=&#34;_blank&#34;&gt;there were 32 people involved&lt;/a&gt;, including yourself, in this release. What is it like to coordinate that group? That sounds like a full time job.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: It is a full time job. And let me say that this release team in 1.16 represented five different continents. We can count Antarctica as not having anybody, but we didn&amp;rsquo;t have anybody from South America for that release, which was unfortunate. But we had people from Australia, China, India, Tanzania. We have a good spread&amp;ndash; Europe, North America. It&amp;rsquo;s great to have that spread and that continuity, which allowed for us to get things done throughout the day.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: Until you want to schedule a meeting.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: Scheduling a meeting was extremely difficult. Typically, on the release team, we run one Europe, Western Europe, and North American-friendly meeting, and then we ask the team if they would like to hold another meeting. Now, in the case of 1.16, they didn&amp;rsquo;t want to hold another meeting. We actually put it out to survey. But in previous releases, we held an EU in the morning so that people in India, as well, or maybe even late-night in China, could be involved.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: Any interesting facts about the team, besides the incredible geographic diversity that you had, to work around that?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: I really appreciate about the release team that we&amp;rsquo;re from all different backgrounds, from all different parts of the world and all different companies. There are people who are doing this on their own time, There are people who are doing this on company time, but we all come together with that shared common goal of shipping that release.&lt;/p&gt;

&lt;p&gt;This release was we had the five continents. It was really exciting in 1.17 that we have in the lead roles, it was represented mainly by women. So 1.17, watch out&amp;ndash; most of the leads for 1.17 are women, which is a great result, and that&amp;rsquo;s through that shadow program that we can foster different types of talent. I&amp;rsquo;m excited to see future releases benefiting from different diverse groups of people from the Kubernetes community.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: What are you going to put in the proverbial envelope for the 1.17 team?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: We&amp;rsquo;ve had this theme of a lot of roles in the release team being cut and dry, right? We have these release handbooks, so for each of the members of the team, they&amp;rsquo;re cut into different roles. There&amp;rsquo;s seven different roles on the team. There&amp;rsquo;s the lead. There&amp;rsquo;s the CI signal role. There&amp;rsquo;s bug triage. There&amp;rsquo;s comms. There&amp;rsquo;s docs. And there&amp;rsquo;s release notes. And there&amp;rsquo;s also the release branch managers who actually cut the code and make sure that they have shipped and it ends up in all the repositories.&lt;/p&gt;

&lt;p&gt;What we did in the previous 1.15, we actually had a role call the test-infra role. And thanks to the wonderful work of the folks of the test-infra team out of Google&amp;ndash; &lt;a href=&#34;https://kubernetespodcast.com/episode/077-eng-prod-and-testing/&#34; target=&#34;_blank&#34;&gt;Katharine Berry&lt;/a&gt;, and &lt;a href=&#34;https://kubernetespodcast.com/episode/069-kind/&#34; target=&#34;_blank&#34;&gt;Ben Elder&lt;/a&gt;, and other folks&amp;ndash; they actually automated this role completely that we could get rid of it in the 1.16 release and still have our same&amp;ndash; and be able to get a release out the door.&lt;/p&gt;

&lt;p&gt;I think a lot of these things are ripe for automation, and therefore, we can have a lot less of a footprint going forward. Let&amp;rsquo;s automate the bits of the process that we can and actually refine the process to make sure that the people that are involved are not doing the repetitive tasks over and over again. In the era of enhancements, we could streamline that process. CI signal and bug triage, there are places we could actually go in and automate that as well. I think one place that&amp;rsquo;s been done really well in 1.16 was in the release notes.&lt;/p&gt;

&lt;p&gt;I don&amp;rsquo;t know if you&amp;rsquo;ve seen &lt;a href=&#34;https://relnotes.k8s.io&#34; target=&#34;_blank&#34;&gt;relnotes.k8s.io&lt;/a&gt;, but you can go and check out the release notes and now, basically, annotated PRs show up as release notes that are searchable and sortable, all through an automated means, whereas that was previously some YAML jockeying to make sure that that would actually happen and be digestible to the users.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: Come on, Lachie, all Kubernetes is just YAML jockeying.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;[LAUGHING]&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: Yeah, but it&amp;rsquo;s great to have an outcome where we can actually make that searchable and get people out of the mundaneness of things like, let&amp;rsquo;s make sure we&amp;rsquo;re copying and pasting YAML from left to right.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: After the release, you had a &lt;a href=&#34;https://docs.google.com/document/d/1VQDIAB0OqiSjIHI8AWMvSdceWhnz56jNpZrLs6o7NJY/edit#heading=h.ipohe1hgr315&#34; target=&#34;_blank&#34;&gt;retrospective meeting&lt;/a&gt;. What was the takeaway from that meeting?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: At the end of each release, we do have a retrospective. It&amp;rsquo;s during the community meeting. That retrospective, it was good. I was just really excited to see that there were so many positives. It&amp;rsquo;s a typical retrospective where we go, what did we say we were going to do last release? Did we do that? What was great? What can we do better? And some actions out of that.&lt;/p&gt;

&lt;p&gt;It was great to see people giving other people on the team so many compliments. It was really, really deep and rich, saying, thank you for doing this, thank you for doing that. People showed up and pulled their weight in the release team, and other people were acknowledging that. That was great.&lt;/p&gt;

&lt;p&gt;I think one thing we want to do is&amp;ndash; we have a code freeze as part of the release process, which is where we make sure that code basically stops going into master in Kubernetes. Only things destined for the release can actually be put in there. But we don&amp;rsquo;t actually stop the test infrastructure from changing, so the test infrastructure has a lifecycle of its own.&lt;/p&gt;

&lt;p&gt;One of the things that was proposed was that we actually code freeze the test infrastructure as well, to make sure that we&amp;rsquo;re not actually looking at changes in the test-infra causing jobs to fail while we&amp;rsquo;re trying to stabilize the code. I think that&amp;rsquo;s something we have some high level agreement about, but getting down into the low-level nitty-gritty would be great in 1.17 and beyond.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: We talked about sidecar containers slipping out of this release. Most of the features are on a release train, and are put in when they&amp;rsquo;re ready. What does it mean for the process of managing a release when those things happen?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: Basically, we have an enhancements freeze, and that says that enhancements&amp;ndash; so the KEPs that are backing these enhancements&amp;ndash; so the sidecar containers would have had an enhancement proposal. And the SIG that owns that code would then need to sign off and say that this is in a state called &amp;ldquo;implementable.&amp;rdquo; When we&amp;rsquo;ve agreed on the high-level details, you can go and proceed and implement that.&lt;/p&gt;

&lt;p&gt;Now, that had actually happened in the case of sidecar containers. The challenge was you still need to write the code and get the code actually implemented, and there&amp;rsquo;s a month gap between enhancement freeze and code freeze. If the code doesn&amp;rsquo;t show up, or the code shows up and needs to be reviewed a little bit more, you may miss that deadline.&lt;/p&gt;

&lt;p&gt;I think that&amp;rsquo;s what happened in the case of this specific feature. It went all the way through to code freeze, the code wasn&amp;rsquo;t complete at that time, and we basically had to make a call&amp;ndash; do we want to grant it an exception? In this case, they didn&amp;rsquo;t ask for an exception. They said, let&amp;rsquo;s just move it to 1.17.&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s still a lot of people and SIGs show up at the start of a new release and put forward the whole release of all the things they want to ship, and obviously, throughout the release, a lot of those things get plucked off. I think we started with something like 60 enhancements, and then what we got out the door was 31. They either fall off as part of the enhancement freeze or as part of the code freeze, and that is absolutely typical of any release.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: Do you think that a three-month wait is acceptable for something that might have had a one- or two-week slip, or would you like to see enhancements be able to be released in point releases between the three-month releases?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: Yeah, there&amp;rsquo;s back and forth about this in the community, about how can we actually roll things at different cadences, I think, is the high-level question. Tim Hockin actually put out, how about we do stability cycles as well? Because there are a lot of new features going in, and there are a lot of stability features going in. But if you look at it, half of the features were beta or stable, and the other half were alpha, which means we&amp;rsquo;re still introducing a lot more complexity and largely untested code into alpha state&amp;ndash; which, as much as we wouldn&amp;rsquo;t like to admit, it does affect the stability of the system.&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s talk of LTS. There&amp;rsquo;s talk of stability releases as well. I think they&amp;rsquo;re all things that are interesting now that Kubernetes has that momentum, and you are seeing a lot of things go to GA. People are like, &amp;ldquo;I don&amp;rsquo;t need to be drinking from the firehose as fast. I have CRDs in GA. I have all these other things in GA. Do I actually need to consume this at the rate?&amp;rdquo; So I think&amp;ndash; stay tuned. If you&amp;rsquo;re interested in those discussions, the upstream community is having those. Show up there and voice your opinion.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: Is this the first release with its own &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/sig-release/master/releases/release-1.16/116_unlimited_breadsticks_for_all.png&#34; target=&#34;_blank&#34;&gt;release mascot&lt;/a&gt;?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: I think that release mascot goes back to&amp;ndash; I would like to say 1.11? If you &lt;a href=&#34;https://github.com/kubernetes/sig-release/blob/master/releases/release-1.11/README.md&#34; target=&#34;_blank&#34;&gt;go back to 1.11&lt;/a&gt;, you can actually see the different mascots. I remember 1.11 being &amp;ldquo;The Hobbit.&amp;rdquo; So it&amp;rsquo;s the Hobbiton front door of Bilbo Baggins with the Kubernetes Helm on the front of it, and that was called 11ty-one&amp;ndash;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: Uh-huh.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: A long-expected release. So they go through from each release, and you can actually go check them out on the SIG release repository upstream.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: I do think this is the first time that&amp;rsquo;s managed to make it into a blog post, though.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: I do think it is the case. I wanted to have a little bit of fun with the release team, so typically you will see the release teams have a t-shirt. I have, &lt;a href=&#34;https://github.com/kubernetes/sig-release/blob/master/releases/release-1.14/README.md&#34; target=&#34;_blank&#34;&gt;from 1.14, the Caternetes&lt;/a&gt;, which Aaron designed, which has a bunch of cats kind of trying to look at a Kubernetes logo.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: We had a fun conversation with Aaron about his love of cats.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: [LAUGHS] And it becomes a token of, hey, remember this hard work that you put together? It becomes a badge of honor for everybody that participated in the release.  I wanted to highlight it as a release mascot. I don&amp;rsquo;t think a lot of people knew that we did have those across the last few releases. But it&amp;rsquo;s just a bit of fun, and I wanted to put my own spin on things just so that the team could come together. A lot of it was around the laughs that we had as a team throughout this release&amp;ndash; and my love of Olive Garden.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: Your love of Olive Garden feels like it may have become a meme to a community which might need a little explanation for our audience. For those who are not familiar with American fine dining, can we start with&amp;ndash; what exactly is Olive Garden?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: Olive Garden is the finest Italian dining experience you will have in the continental United States. I see everybody&amp;rsquo;s faces saying, is he sure about that? I&amp;rsquo;m sure.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: That might require a slight justification on behalf of some of our Italian-American listeners.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: Is it the unlimited breadsticks and salad that really does it for you, or is the plastic boat that it comes in?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: I think it&amp;rsquo;s a combination of all three things. You know, the tour of Italy, you can&amp;rsquo;t go past. The free breadsticks are fantastic. But Olive Garden just represents the large chain restaurant and that kind of childhood I had growing up and thinking about these large-scale chain restaurants. You don&amp;rsquo;t get to choose your meme. And the legacy&amp;ndash; I would have liked to have had a different mascot.&lt;/p&gt;

&lt;p&gt;But I just had a run with the meme of Olive Garden. And this came about, I would like to say, about three or four months ago. Paris Pittman from Google, who is another member of the Kubernetes community, kind of put out there, what&amp;rsquo;s your favorite sit-down large-scale restaurant? And of course, I pitched in very early and said, it&amp;rsquo;s got to be the Olive Garden.&lt;/p&gt;

&lt;p&gt;And then everybody kind of jumped onto that. And my inbox is full of free Olive Garden gift certificates now, and it&amp;rsquo;s taken on a life of its own. And at this point, I&amp;rsquo;m just embracing it&amp;ndash; so much so that we might even have the 1.16 release party at an Olive Garden in San Diego, if it can accommodate 10,000 people.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: &lt;a href=&#34;https://www.youtube.com/watch?v=9ZJF5-EyjXs&#34; target=&#34;_blank&#34;&gt;When you&amp;rsquo;re there, are you family?&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: Yes. Absolutely, absolutely. And I would have loved to put that. I think the release name was &amp;ldquo;unlimited breadsticks for all.&amp;rdquo; I would have liked to have done, &amp;ldquo;When you&amp;rsquo;re here, you&amp;rsquo;re family,&amp;rdquo; but that is, sadly, trademarked.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: Aww. What&amp;rsquo;s next for you in the community?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;LACHLAN EVENSON: I&amp;rsquo;ve really been looking at Cluster API a lot&amp;ndash; so building Kubernetes clusters on top of a declarative approach. I&amp;rsquo;ve been taking a look at what we can do in the Cluster API ecosystem. I&amp;rsquo;m also a chair of SIG PM, so helping foster the KEP process as well&amp;ndash; making sure that that continues to happen and continues to be fruitful for the community.&lt;/p&gt;

&lt;hr/&gt;

&lt;p&gt;&lt;i&gt;&lt;a href=&#34;https://twitter.com/lachlanevenson&#34; target=&#34;_blank&#34;&gt;Lachlan Evenson&lt;/a&gt; is a Principal Program Manager at Microsoft and an Australian living in the US, and most recently served as the Kubernetes 1.16 release team lead.&lt;/p&gt;

&lt;p&gt;You can find the &lt;a href=&#34;http://www.kubernetespodcast.com/&#34; target=&#34;_blank&#34;&gt;Kubernetes Podcast from Google&lt;/a&gt; at &lt;a href=&#34;https://twitter.com/KubernetesPod&#34; target=&#34;_blank&#34;&gt;@kubernetespod&lt;/a&gt; on Twitter, and you can &lt;a href=&#34;https://kubernetespodcast.com/subscribe/&#34; target=&#34;_blank&#34;&gt;subscribe&lt;/a&gt; so you never miss an episode.&lt;/i&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Gardener Project Update</title>
      <link>https://kubernetes.io/blog/2019/12/02/gardener-project-update/</link>
      <pubDate>Mon, 02 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/12/02/gardener-project-update/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; &lt;a href=&#34;mailto:rafael.franzke@sap.com&#34; target=&#34;_blank&#34;&gt;Rafael Franzke&lt;/a&gt; (SAP), &lt;a href=&#34;mailto:vasu.chandrasekhara@sap.com&#34; target=&#34;_blank&#34;&gt;Vasu
Chandrasekhara&lt;/a&gt; (SAP)&lt;/p&gt;

&lt;p&gt;Last year, we introduced &lt;a href=&#34;https://gardener.cloud&#34; target=&#34;_blank&#34;&gt;Gardener&lt;/a&gt; in the &lt;a href=&#34;https://www.youtube.com/watch?v=DpFTcTnBxbM&amp;amp;feature=youtu.be&amp;amp;t=1642&#34; target=&#34;_blank&#34;&gt;Kubernetes
Community
Meeting&lt;/a&gt;
and in a post on the &lt;a href=&#34;https://kubernetes.io/blog/2018/05/17/gardener/&#34; target=&#34;_blank&#34;&gt;Kubernetes
Blog&lt;/a&gt;. At SAP, we have been
running Gardener for more than two years, and are successfully managing
thousands of &lt;a href=&#34;https://k8s-testgrid.appspot.com/conformance-gardener&#34; target=&#34;_blank&#34;&gt;conformant&lt;/a&gt;
clusters in various versions on all major hyperscalers as well as in numerous
infrastructures and private clouds that typically join an enterprise via
acquisitions.&lt;/p&gt;

&lt;p&gt;We are often asked why a handful of dynamically scalable clusters would not
suffice. We also started our journey into Kubernetes with a similar mindset. But
we realized that applying the architecture and principles of Kubernetes to
productive scenarios, our internal and external customers very quickly required
the rational separation of concerns and ownership, which in most circumstances
led to the use of multiple clusters. Therefore, a scalable and managed
Kubernetes as a service solution is often also the basis for adoption.
Particularly, when a larger organization runs multiple products on different
providers and in different regions, the number of clusters will quickly rise to
the hundreds or even thousands.&lt;/p&gt;

&lt;p&gt;Today, we want to give an update on what we have implemented in the past year
regarding extensibility and customizability, and what we plan to work on for our
next milestone.&lt;/p&gt;

&lt;h2 id=&#34;short-recap-what-is-gardener&#34;&gt;Short Recap: What Is Gardener?&lt;/h2&gt;

&lt;p&gt;Gardener&amp;rsquo;s main principle is to leverage Kubernetes primitives for all of its
operations, commonly described as inception or kubeception. The feedback from
the community was that initially our &lt;a href=&#34;https://github.com/gardener/documentation/wiki/Architecture&#34; target=&#34;_blank&#34;&gt;architecture
diagram&lt;/a&gt; looks
&amp;ldquo;overwhelming&amp;rdquo;, but after some little digging into the material, everything we
do is the &amp;ldquo;Kubernetes way&amp;rdquo;. One can re-use all learnings with respect to APIs,
control loops, etc. &lt;br /&gt;
The essential idea is that so-called &lt;strong&gt;seed&lt;/strong&gt; clusters are used to host the
control planes of end-user clusters (botanically named &lt;strong&gt;shoots&lt;/strong&gt;). &lt;br /&gt;
Gardener provides vanilla Kubernetes clusters as a service independent of the
underlying infrastructure provider in a homogenous way, utilizing the upstream
provided &lt;code&gt;k8s.gcr.io/*&lt;/code&gt; images as open distribution. The project is built
entirely on top of Kubernetes extension concepts, and as such adds a custom API
server, a controller-manager, and a scheduler to create and manage the lifecycle
of Kubernetes clusters. It extends the Kubernetes API with custom resources,
most prominently the Gardener cluster specification (&lt;code&gt;Shoot&lt;/code&gt; resource), that can
be used to &amp;ldquo;order&amp;rdquo; a Kubernetes cluster in a declarative way (for day-1, but
also reconcile all management activities for day-2).&lt;/p&gt;

&lt;p&gt;By leveraging Kubernetes as base infrastructure, we were able to devise a
combined &lt;a href=&#34;https://github.com/gardener/hvpa-controller&#34; target=&#34;_blank&#34;&gt;Horizontal and Vertical Pod Autoscaler
(HVPA)&lt;/a&gt; that, when configured with
custom heuristics, scales all control plane components up/down or out/in
automatically. This enables a fast scale-out, even beyond the capacity of
typically some fixed number of master nodes. This architectural feature is one
of the main differences compared to many other Kubernetes cluster provisioning
tools. But in our production, Gardener does not only effectively reduce the
total costs of ownership by bin-packing control planes. It also simplifies
implementation of &amp;ldquo;day-2 operations&amp;rdquo; (like cluster updates or robustness
qualities). Again, essentially by relying on all the mature Kubernetes features
and capabilities.&lt;/p&gt;

&lt;p&gt;The newly introduced extension concepts for Gardener now enable providers to
only maintain their specific extension without the necessity to develop inside
the core source tree.&lt;/p&gt;

&lt;h2 id=&#34;extensibility&#34;&gt;Extensibility&lt;/h2&gt;

&lt;p&gt;As result of its growth over the past years, the Kubernetes code base contained
a numerous amount of provider-specific code that is now being externalized from
its core source tree. The same has happened with Project Gardener: over time,
lots of specifics for cloud providers, operating systems, network plugins, etc.
have been accumulated. Generally, this leads to a significant increase of
efforts when it comes to maintainability, testability, or to new releases. Our
community member &lt;a href=&#34;https://www.packet.com&#34; target=&#34;_blank&#34;&gt;Packet&lt;/a&gt; contributed &lt;a href=&#34;https://www.packet.com/kubernetes/&#34; target=&#34;_blank&#34;&gt;Gardener
support&lt;/a&gt; for their infrastructure in-tree,
and suffered from the mentioned downsides.&lt;/p&gt;

&lt;p&gt;Consequently, similar to how the Kubernetes community decided to move their
cloud-controller-managers out-of-tree, or volumes plugins to CSI, etc., the
Gardener community
&lt;a href=&#34;https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md&#34; target=&#34;_blank&#34;&gt;proposed&lt;/a&gt;
and implemented likewise extension concepts. The Gardener core source-tree is
now devoid of any provider specifics, allowing vendors to solely focus on their
infrastructure specifics, and enabling core contributors becoming more agile
again.&lt;/p&gt;

&lt;p&gt;Typically, setting up a cluster requires a flow of interdependent steps,
beginning with the generation of certificates and preparation of the
infrastructure, continuing with the provisioning of the control plane and the
worker nodes, and ending with the deployment of system components. We would like
to emphasize here that all these steps are necessary (cf. &lt;a href=&#34;https://github.com/kelseyhightower/kubernetes-the-hard-way&#34; target=&#34;_blank&#34;&gt;Kubernetes the Hard
Way&lt;/a&gt;) and all
Kubernetes cluster creation tools implement the same steps (automated to some
degree) in one way or another.&lt;/p&gt;

&lt;p&gt;The general idea of Gardener&amp;rsquo;s extensibility concept was to make &lt;a href=&#34;https://github.com/gardener/gardener/blob/0.31.1/pkg/controllermanager/controller/shoot/shoot_control_reconcile.go#L69-L298&#34; target=&#34;_blank&#34;&gt;this
flow&lt;/a&gt;
more generic and to carve out custom resources for each step which can serve as
ideal extension points.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-11-10-gardener-project-update/flow.png&#34;
         alt=&#34;Cluster reconciliation flow with extension points&#34;/&gt; 
&lt;/figure&gt;


&lt;p&gt;&lt;em&gt;Figure 1 Cluster reconciliation flow with extension points.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;With Gardener&amp;rsquo;s flow framework we implicitly have a reproducible state machine
for all infrastructures and all possible states of a cluster.&lt;/p&gt;

&lt;p&gt;The Gardener extensibility approach defines custom resources that serve as ideal
extension points for the following categories:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;DNS providers (e.g., Route53, CloudDNS, &amp;hellip;),&lt;/li&gt;
&lt;li&gt;Blob storage providers (e.g., S3, GCS, ABS,&amp;hellip;),&lt;/li&gt;
&lt;li&gt;Infrastructure providers (e.g., AWS, GCP, Azure, &amp;hellip;),&lt;/li&gt;
&lt;li&gt;Operating systems (e.g., CoreOS Container Linux, Ubuntu, FlatCar Linux, &amp;hellip;),&lt;/li&gt;
&lt;li&gt;Network plugins (e.g., Calico, Flannel, Cilium, &amp;hellip;),&lt;/li&gt;
&lt;li&gt;Non-essential extensions (e.g., Let&amp;rsquo;s Encrypt certificate service).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;extension-points&#34;&gt;Extension Points&lt;/h3&gt;

&lt;p&gt;Besides leveraging custom resource definitions, we also effectively use mutating
/ validating webhooks in the seed clusters. Extension controllers themselves run
in these clusters and react on CRDs and workload resources (like &lt;code&gt;Deployment&lt;/code&gt;,
&lt;code&gt;StatefulSet&lt;/code&gt;, etc.) they are responsible for. Similar to the &lt;a href=&#34;https://cluster-api.sigs.k8s.io&#34; target=&#34;_blank&#34;&gt;Cluster
API&lt;/a&gt;&amp;rsquo;s approach, these CRDs may also contain
provider specific information.&lt;/p&gt;

&lt;p&gt;The steps 2. - 10. [cf. Figure 1] involve infrastructure specific meta data
referring to infrastructure specific implementations, e.g. for DNS records there
might be &lt;code&gt;aws-route53&lt;/code&gt;, &lt;code&gt;google-clouddns&lt;/code&gt;, or for isolated networks even
&lt;code&gt;openstack-designate&lt;/code&gt;, and many more. We are going to  examine the steps 4 and 6
in the next paragraphs as examples for the general concepts (based on the
implementation for AWS). If you&amp;rsquo;re interested you can read up the fully
documented API contract in our &lt;a href=&#34;https://github.com/gardener/gardener/tree/master/docs/extensions&#34; target=&#34;_blank&#34;&gt;extensibility
documents&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;example-infrastructure-crd&#34;&gt;Example: &lt;code&gt;Infrastructure&lt;/code&gt; CRD&lt;/h3&gt;

&lt;p&gt;Kubernetes clusters on AWS require a certain infrastructure preparation before
they can be used. This includes, for example, the creation of a VPC, subnets,
etc. The purpose of the &lt;code&gt;Infrastructure&lt;/code&gt; CRD is to trigger this preparation:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;extensions.gardener.cloud/v1alpha1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Infrastructure&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;infrastructure&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;namespace:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;shoot--foobar--aws&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;aws&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;region:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;eu-west&lt;span style=&#34;color:#666&#34;&gt;-1&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;secretRef:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;cloudprovider&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;namespace:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;shoot--foobar—aws&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;sshPublicKey:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;c3NoLXJzYSBBQUFBQ...&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;providerConfig:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;aws.provider.extensions.gardener.cloud/v1alpha1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;InfrastructureConfig&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;networks:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;vpc:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;cidr:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;10.250.0.0&lt;/span&gt;/&lt;span style=&#34;color:#666&#34;&gt;16&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;zones:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;eu-west-1a&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;internal:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;10.250.112.0&lt;/span&gt;/&lt;span style=&#34;color:#666&#34;&gt;22&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;public:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;10.250.96.0&lt;/span&gt;/&lt;span style=&#34;color:#666&#34;&gt;22&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;workers:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;10.250.0.0&lt;/span&gt;/&lt;span style=&#34;color:#666&#34;&gt;19&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Based on the &lt;code&gt;Shoot&lt;/code&gt; resource, Gardener creates this &lt;code&gt;Infrastructure&lt;/code&gt; resource
as part of its reconciliation flow. The AWS-specific &lt;code&gt;providerConfig&lt;/code&gt; is part of
the end-user&amp;rsquo;s configuration in the &lt;code&gt;Shoot&lt;/code&gt; resource and not evaluated by
Gardener but just passed to the extension controller in the seed cluster.&lt;/p&gt;

&lt;p&gt;In its current implementation, the AWS extension creates a new VPC and three
subnets in the &lt;code&gt;eu-west-1a&lt;/code&gt; zones. Also, it creates a NAT and an internet
gateway, elastic IPs, routing tables, security groups, IAM roles, instances
profiles, and an EC2 key pair.&lt;/p&gt;

&lt;p&gt;After it has completed its tasks it will report the status and some
provider-specific output:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;extensions.gardener.cloud/v1alpha1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Infrastructure&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;infrastructure&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;namespace:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;shoot--foobar--aws&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;...&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;status:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;lastOperation:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Reconcile&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;state:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Succeeded&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;providerStatus:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;aws.provider.extensions.gardener.cloud/v1alpha1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;InfrastructureStatus&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;ec2:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;keyName:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;shoot--foobar--aws-ssh-publickey&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;iam:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;instanceProfiles:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;shoot--foobar--aws-nodes&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;purpose:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;nodes&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;roles:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;arn:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;arn:aws:iam::&amp;lt;accountID&amp;gt;:role/shoot...&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;purpose:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;nodes&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;vpc:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;id:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;vpc&lt;span style=&#34;color:#666&#34;&gt;-0815&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;securityGroups:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;id:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;sg&lt;span style=&#34;color:#666&#34;&gt;-0246&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;purpose:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;nodes&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;subnets:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;id:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;subnet&lt;span style=&#34;color:#666&#34;&gt;-1234&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;purpose:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;nodes&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;zone:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;eu-west-1b&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;id:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;subnet&lt;span style=&#34;color:#666&#34;&gt;-5678&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;purpose:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;public&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;zone:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;eu-west-1b&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The information inside the &lt;code&gt;providerStatus&lt;/code&gt; can be used in subsequent steps,
e.g. to configure the cloud-controller-manager or to instrument the
machine-controller-manager.&lt;/p&gt;

&lt;h3 id=&#34;example-deployment-of-the-cluster-control-plane&#34;&gt;Example: Deployment of the Cluster Control Plane&lt;/h3&gt;

&lt;p&gt;One of the major features of Gardener is the homogeneity of the clusters it
manages across different infrastructures. Consequently, it is still in charge of
deploying the provider-independent control plane components into the seed
cluster (like etcd, kube-apiserver). The deployment of provider-specific control
plane components like cloud-controller-manager or CSI controllers is triggered
by a dedicated &lt;code&gt;ControlPlane&lt;/code&gt; CRD. In this paragraph, however, we want to focus
on the customization of the standard components.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s focus on both the kube-apiserver and the kube-controller-manager
&lt;code&gt;Deployment&lt;/code&gt;s. Our AWS extension for Gardener is not yet using CSI but relying
on the in-tree EBS volume plugin. Hence, it needs to enable the
&lt;code&gt;PersistentVolumeLabel&lt;/code&gt; admission plugin and to provide the cloud provider
config to the kube-apiserver. Similarly, the kube-controller-manager will be
instructed to use its in-tree volume plugin.&lt;/p&gt;

&lt;p&gt;The kube-apiserver &lt;code&gt;Deployment&lt;/code&gt; incorporates the &lt;code&gt;kube-apiserver&lt;/code&gt; container and
is deployed by Gardener like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;containers:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;command:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/hyperkube&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;apiserver&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;--enable-admission-plugins=Priority,...,NamespaceLifecycle&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;--allow-privileged=&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;true&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;--anonymous-auth=&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;false&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;...&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Using a &lt;code&gt;MutatingWebhookConfiguration&lt;/code&gt; the AWS extension injects the mentioned
flags and modifies the spec as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;containers:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;command:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/hyperkube&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;apiserver&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;--enable-admission-plugins=Priority,...,NamespaceLifecycle,PersistentVolumeLabel&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;--allow-privileged=&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;true&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;--anonymous-auth=&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;false&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;...&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;--cloud-provider=aws&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;--cloud-config=/etc/kubernetes/cloudprovider/cloudprovider.conf&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;--endpoint-reconciler-type=none&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;...&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;volumeMounts:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;mountPath:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/etc/kubernetes/cloudprovider&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;cloud-provider-config&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;volumes:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;configMap:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;defaultMode:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;420&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;cloud-provider-config&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;cloud-provider-config&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The kube-controller-manager &lt;code&gt;Deployment&lt;/code&gt; is handled in a similar way.&lt;/p&gt;

&lt;p&gt;Webhooks in the seed cluster can be used to mutate anything related to the shoot
cluster control plane deployed by Gardener or any other extension. There is a
similar webhook concept for resources in shoot clusters in case extension
controllers need to customize system components deployed by Gardener.&lt;/p&gt;

&lt;h3 id=&#34;registration-of-extension-controllers&#34;&gt;Registration of Extension Controllers&lt;/h3&gt;

&lt;p&gt;The Gardener API uses two special resources to register and install extensions.
The registration itself is declared via the &lt;code&gt;ControllerRegistration&lt;/code&gt; resource.
The easiest option is to define the Helm chart as well as some values to render
the chart, however, any other deployment mechanism is supported via custom code
as well.&lt;/p&gt;

&lt;p&gt;Gardener determines whether an extension controller is required in a specific
seed cluster, and creates a &lt;code&gt;ControllerInstallation&lt;/code&gt; that is used to trigger the
deployment.&lt;/p&gt;

&lt;p&gt;To date, every registered extension controller is deployed to every seed cluster
which is not necessary in general. In the future, Gardener will become more
selective to only deploy those extensions required on the specific seed
clusters.&lt;/p&gt;

&lt;p&gt;Our dynamic registration approach allows to add or remove extensions in the
running system - without the necessity to rebuild or restart any component.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-11-10-gardener-project-update/architecture.png&#34;
         alt=&#34;Gardener architecture with extension controllers&#34;/&gt; 
&lt;/figure&gt;


&lt;p&gt;&lt;em&gt;Figure 2 Gardener architecture with extension controllers.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;status-quo&#34;&gt;Status Quo&lt;/h3&gt;

&lt;p&gt;We have recently introduced the new &lt;code&gt;core.gardener.cloud&lt;/code&gt; API group that
incorporates fully forwards and backwards compatible &lt;code&gt;Shoot&lt;/code&gt; resources, and that
allows providers to use Gardener without modifying anything in its core source
tree.&lt;/p&gt;

&lt;p&gt;We have already adapted all controllers to use this new API group and have
deprecated the old API. Eventually, after a few months we will remove it, so
end-users are advised to start migrating to the new API soon.&lt;/p&gt;

&lt;p&gt;Apart from that, we have enabled all relevant extensions to contribute to the
shoot health status and implemented the respective contract. The basic idea is
that the CRDs may have &lt;code&gt;.status.conditions&lt;/code&gt; that are picked up by Gardener and
merged with its standard health checks into the &lt;code&gt;Shoot&lt;/code&gt; status field.&lt;/p&gt;

&lt;p&gt;Also, we want to implement some easy-to-use library functions facilitating
defaulting and validation webhooks for the CRDs in order to validate the
&lt;code&gt;providerConfig&lt;/code&gt; field controlled by end-users.&lt;/p&gt;

&lt;p&gt;Finally, we will split the
&lt;a href=&#34;https://github.com/gardener/gardener-extensions&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;gardener/gardener-extensions&lt;/code&gt;&lt;/a&gt;
repository into separate repositories and keep it only for the generic library
functions that can be used to write extension controllers.&lt;/p&gt;

&lt;h2 id=&#34;next-steps&#34;&gt;Next Steps&lt;/h2&gt;

&lt;p&gt;Kubernetes has externalized many of the infrastructural management challenges.
The inception design solves most of them by delegating lifecycle operations to a
separate management plane (seed clusters). But what if the garden cluster or a
seed cluster goes down? How do we scale beyond tens of thousands of managed
clusters that need to be reconciled in parallel? We are further investing into
hardening the Gardener scalability and disaster recovery features. Let&amp;rsquo;s briefly
highlight three of the features in more detail:&lt;/p&gt;

&lt;h3 id=&#34;gardenlet&#34;&gt;Gardenlet&lt;/h3&gt;

&lt;p&gt;Right from the beginning of the Gardener Project we started implementing the
&lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/operator/&#34; target=&#34;_blank&#34;&gt;operator
pattern&lt;/a&gt;: We
have a custom controller-manager that acts on our own custom resources. Now,
when you start thinking about the &lt;a href=&#34;https://github.com/gardener/documentation/wiki/Architecture&#34; target=&#34;_blank&#34;&gt;Gardener
architecture&lt;/a&gt;, you
will recognize some interesting similarity with respect to the Kubernetes
architecture: Shoot clusters can be compared with pods, and seed clusters can be
seen as worker nodes. Guided by this observation we introduced the
&lt;strong&gt;gardener-scheduler&lt;/strong&gt;. Its main task is to find an appropriate seed cluster to
host the control-plane for newly ordered clusters, similar to how the
kube-scheduler finds an appropriate node for newly created pods. By providing
multiple seed clusters for a region (or provider) and distributing the workload,
we reduce the blast-radius of potential hick-ups as well.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-11-10-gardener-project-update/gardenlet.png&#34;
         alt=&#34;Similarities between Kubernetes and Gardener architecture&#34;/&gt; 
&lt;/figure&gt;


&lt;p&gt;&lt;em&gt;Figure 3 Similarities between Kubernetes and Gardener architecture.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Yet, there is still a significant difference between the Kubernetes and the
Gardener architectures: Kubernetes runs a primary &amp;ldquo;agent&amp;rdquo; on every node, the
kubelet, which is mainly responsible for managing pods and containers on its
particular node. Gardener uses its controller-manager which is responsible for
all shoot clusters on all seed clusters, and it is performing its reconciliation
loops centrally from the garden cluster.&lt;/p&gt;

&lt;p&gt;While this works well at scale for thousands of clusters today, our goal is to
enable true scalability following the Kubernetes principles (beyond the capacity
of a single controller-manager): We are now working on distributing the logic
(or the Gardener operator) into the seed cluster and will introduce a
corresponding component, adequately named the &lt;strong&gt;gardenlet&lt;/strong&gt;. It will be
Gardener&amp;rsquo;s primary &amp;ldquo;agent&amp;rdquo; on every seed cluster and will be only responsible
for shoot clusters located in its particular seed cluster.&lt;/p&gt;

&lt;p&gt;The gardener-controller-manager will still keep its control loops for other
resources of the Gardener API, however, it will no longer talk to seed/shoot
clusters.&lt;/p&gt;

&lt;p&gt;Reversing the control flow will even allow placing seed/shoot clusters behind
firewalls without the necessity of direct accessibility (via VPN tunnels)
anymore.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-11-10-gardener-project-update/gardenlet-detailed.png&#34;
         alt=&#34;Detailed architecture with Gardenlet&#34;/&gt; 
&lt;/figure&gt;


&lt;p&gt;&lt;em&gt;Figure 4 Detailed architecture with Gardenlet.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;control-plane-migration-between-seed-clusters&#34;&gt;Control Plane Migration between Seed Clusters&lt;/h3&gt;

&lt;p&gt;When a seed cluster fails, the user&amp;rsquo;s static workload will continue to operate.
However, administrating the cluster won&amp;rsquo;t be possible anymore because the shoot
cluster&amp;rsquo;s API server running in the failed seed is no longer reachable.&lt;/p&gt;

&lt;p&gt;We have implemented the relocation of failed control planes hit by some seed
disaster to another seed and are now working on fully automating this unique
capability. In fact, this approach is not only feasible, we have performed the
fail-over procedure multiple times in our production.&lt;/p&gt;

&lt;p&gt;The automated failover capability will enable us to implement even more
comprehensive disaster recovery and scalability qualities, e.g., the automated
provisioning and re-balancing of seed clusters or automated migrations for all
non-foreseeable cases. Again, think about the similarities with Kubernetes with
respect to pod eviction and node drains.&lt;/p&gt;

&lt;h3 id=&#34;gardener-ring&#34;&gt;Gardener Ring&lt;/h3&gt;

&lt;p&gt;The Gardener Ring is our novel approach for provisioning and managing Kubernetes
clusters without relying on an external provision tool for the initial cluster.
By using Kubernetes in a recursive manner, we can drastically reduce the
management complexity by avoiding imperative tool sets, while creating new
qualities with a self-stabilizing circular system.&lt;/p&gt;

&lt;p&gt;The Ring approach is conceptually different from self-hosting and static pod
based deployments. The idea is to create a ring of three (or more) shoot
clusters that each host the control plane of its successor.&lt;/p&gt;

&lt;p&gt;An outage of one cluster will not affect the stability and availability of the
Ring, and as the control plane is externalized the failed cluster can be
automatically recovered by Gardener&amp;rsquo;s self-healing capabilities. As long as
there is a quorum of at least &lt;code&gt;n/2+1&lt;/code&gt; available clusters the Ring will always
stabilize itself. Running these clusters on different cloud providers (or at
least in different regions / data centers) reduces the potential for quorum
losses.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-11-10-gardener-project-update/ring.png&#34;
         alt=&#34;Self-stabilizing ring of Kubernetes clusters&#34;/&gt; 
&lt;/figure&gt;


&lt;p&gt;&lt;em&gt;Figure 5 Self-stabilizing ring of Kubernetes clusters.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The way how the distributed instances of Gardener can share the same data is by
deploying separate kube-apiserver instances talking to the same etcd cluster.
These kube-apiservers are forming a node-less Kubernetes cluster that can be
used as &amp;ldquo;data container&amp;rdquo; for Gardener and its associated applications.&lt;/p&gt;

&lt;p&gt;We are running test landscapes internally protected by the ring and it has saved
us from manual interventions. With the automated control plane migration in
place we can easily bootstrap the Ring and will solve the &amp;ldquo;initial cluster
problem&amp;rdquo; as well as improve the overall robustness.&lt;/p&gt;

&lt;h2 id=&#34;getting-started&#34;&gt;Getting Started!&lt;/h2&gt;

&lt;p&gt;If you are interested in writing an extension, you might want to check out the
following resources:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md&#34; target=&#34;_blank&#34;&gt;GEP-1: Extensibility proposal
document&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/gardener/gardener/blob/master/docs/proposals/04-new-core-gardener-cloud-apis.md&#34; target=&#34;_blank&#34;&gt;GEP-4: New &lt;code&gt;core.gardener.cloud/v1alpha1&lt;/code&gt;
API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/gardener/gardener-extensions/tree/master/controllers/provider-aws&#34; target=&#34;_blank&#34;&gt;Example extension controller implementation for
AWS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://godoc.org/github.com/gardener/gardener-extensions/pkg&#34; target=&#34;_blank&#34;&gt;Gardener Extensions Golang
library&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/gardener/gardener/tree/master/docs/extensions&#34; target=&#34;_blank&#34;&gt;Extension contract
documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://gardener.cloud/api-reference/&#34; target=&#34;_blank&#34;&gt;Gardener API Reference&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Of course, any other contribution to our project is very welcome as well! We are
always looking for new community members.&lt;/p&gt;

&lt;p&gt;If you want to try out Gardener, please check out our &lt;a href=&#34;https://gardener.cloud/installer/&#34; target=&#34;_blank&#34;&gt;quick installation
guide&lt;/a&gt;. This installer will setup a complete
Gardener environment ready to be used for testing and evaluation within just a
few minutes.&lt;/p&gt;

&lt;h2 id=&#34;contributions-welcome&#34;&gt;Contributions Welcome!&lt;/h2&gt;

&lt;p&gt;The Gardener project is developed as Open Source and hosted on GitHub:
&lt;a href=&#34;https://github.com/gardener&#34; target=&#34;_blank&#34;&gt;https://github.com/gardener&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If you see the potential of the Gardener project, please join us via GitHub.&lt;/p&gt;

&lt;p&gt;We are having a weekly &lt;a href=&#34;https://docs.google.com/document/d/1314v8ziVNQPjdBrWp-Y4BYrTDlv7dq2cWDFIa9SMaP4&#34; target=&#34;_blank&#34;&gt;public community
meeting&lt;/a&gt;
scheduled every Friday 10-11 a.m. CET, and a public &lt;a href=&#34;https://kubernetes.slack.com/messages/gardener&#34; target=&#34;_blank&#34;&gt;#gardener
Slack&lt;/a&gt; channel in the Kubernetes
workspace. Also, we are planning a &lt;a href=&#34;https://docs.google.com/document/d/1EQ_kt70gwybiL7FY8F7Dx--GtiNwdv0oRDwqQqAIYMk/edit#heading=h.a43vkkp847f1&#34; target=&#34;_blank&#34;&gt;Gardener Hackathon in Q1
2020&lt;/a&gt;
and are looking forward meeting you there!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Develop a Kubernetes controller in Java</title>
      <link>https://kubernetes.io/blog/2019/11/26/develop-a-kubernetes-controller-in-java/</link>
      <pubDate>Tue, 26 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/11/26/develop-a-kubernetes-controller-in-java/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Min Kim (Ant Financial), Tony Ado (Ant Financial)&lt;/p&gt;

&lt;p&gt;The official &lt;a href=&#34;https://github.com/kubernetes-client/java&#34; target=&#34;_blank&#34;&gt;Kubernetes Java SDK&lt;/a&gt; project
recently released their latest work on providing the Java Kubernetes developers
a handy Kubernetes controller-builder SDK which is helpful for easily developing
advanced workloads or systems.&lt;/p&gt;

&lt;h2 id=&#34;overall&#34;&gt;Overall&lt;/h2&gt;

&lt;p&gt;Java is no doubt one of the most popular programming languages in the world but
it&amp;rsquo;s been difficult for a period time for those non-Golang developers to build up
their customized controller/operator due to the lack of library resources in the
community. In the world of Golang, there&amp;rsquo;re already some excellent controller
frameworks, for example, &lt;a href=&#34;https://github.com/kubernetes-sigs/controller-runtime&#34; target=&#34;_blank&#34;&gt;controller runtime&lt;/a&gt;,
&lt;a href=&#34;https://github.com/operator-framework/operator-sdk&#34; target=&#34;_blank&#34;&gt;operator SDK&lt;/a&gt;. These
existing Golang frameworks are relying on the various utilities from the
&lt;a href=&#34;https://github.com/kubernetes/client-go&#34; target=&#34;_blank&#34;&gt;Kubernetes Golang SDK&lt;/a&gt; proven to
be stable over years. Driven by the emerging need of further integration into
the platform of Kubernetes, we not only ported many essential toolings from the Golang
SDK into the kubernetes Java SDK including informers, work-queues, leader-elections,
etc. but also developed a controller-builder SDK which wires up everything into
a runnable controller without hiccups.&lt;/p&gt;

&lt;h2 id=&#34;backgrounds&#34;&gt;Backgrounds&lt;/h2&gt;

&lt;p&gt;Why use Java to implement Kubernetes tooling? You might pick Java for:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Integrating legacy enterprise Java systems&lt;/strong&gt;: Many companies have their legacy
systems or frameworks written in Java in favor of stability. We are not able to
move everything to Golang easily.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;More open-source community resources&lt;/strong&gt;: Java is mature and has accumulated abundant open-source
libraries over decades, even though Golang is getting more and more fancy and
popular for developers. Additionally, nowadays developers are able to develop
their aggregated-apiservers over SQL-storage and Java has way better support on SQLs.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;how-to-use&#34;&gt;How to use?&lt;/h2&gt;

&lt;p&gt;Take maven project as example, adding the following dependencies into your dependencies:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-xml&#34; data-lang=&#34;xml&#34;&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;lt;dependency&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;lt;groupId&amp;gt;&lt;/span&gt;io.kubernetes&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;lt;/groupId&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;lt;artifactId&amp;gt;&lt;/span&gt;client-java-extended&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;lt;/artifactId&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;lt;version&amp;gt;&lt;/span&gt;6.0.1&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;lt;/version&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;lt;/dependency&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then we can make use of the provided builder libraries to write your own controller.
For example, the following one is a simple controller prints out node information
on watch notification, see complete example &lt;a href=&#34;https://github.com/kubernetes-client/java/blob/master/examples/src/main/java/io/kubernetes/client/examples/ControllerExample.java&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-java&#34; data-lang=&#34;java&#34;&gt;...
    &lt;span style=&#34;&#34;&gt;Reconciler reconciler &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;new&lt;/span&gt; Reconciler() {
      &lt;span style=&#34;color:#a2f&#34;&gt;@Override&lt;/span&gt;
      &lt;span style=&#34;&#34;&gt;public Result &lt;/span&gt;reconcile(&lt;span style=&#34;&#34;&gt;Request request&lt;/span&gt;) {
        &lt;span style=&#34;&#34;&gt;V1Node node &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; nodeLister.&lt;span style=&#34;color:#b44&#34;&gt;get&lt;/span&gt;(request.&lt;span style=&#34;color:#b44&#34;&gt;getName&lt;/span&gt;());
        System.&lt;span style=&#34;color:#b44&#34;&gt;out&lt;/span&gt;.&lt;span style=&#34;color:#b44&#34;&gt;println&lt;/span&gt;(&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;triggered reconciling &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; node.&lt;span style=&#34;color:#b44&#34;&gt;getMetadata&lt;/span&gt;().&lt;span style=&#34;color:#b44&#34;&gt;getName&lt;/span&gt;());
        &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;new&lt;/span&gt; Result(&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;false&lt;/span&gt;);
      }
    };
    &lt;span style=&#34;&#34;&gt;Controller controller &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;
        ControllerBuilder.&lt;span style=&#34;color:#b44&#34;&gt;defaultBuilder&lt;/span&gt;(informerFactory)
            .&lt;span style=&#34;color:#b44&#34;&gt;watch&lt;/span&gt;(
                (workQueue) &lt;span style=&#34;color:#666&#34;&gt;-&amp;gt;&lt;/span&gt; ControllerBuilder.&lt;span style=&#34;color:#b44&#34;&gt;controllerWatchBuilder&lt;/span&gt;(V1Node.&lt;span style=&#34;color:#b44&#34;&gt;class&lt;/span&gt;, workQueue).&lt;span style=&#34;color:#b44&#34;&gt;build&lt;/span&gt;())
            .&lt;span style=&#34;color:#b44&#34;&gt;withReconciler&lt;/span&gt;(nodeReconciler) &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// required, set the actual reconciler
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;&lt;/span&gt;            .&lt;span style=&#34;color:#b44&#34;&gt;withName&lt;/span&gt;(&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;node-printing-controller&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// optional, set name for controller for logging, thread-tracing
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;&lt;/span&gt;            .&lt;span style=&#34;color:#b44&#34;&gt;withWorkerCount&lt;/span&gt;(4) &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// optional, set worker thread count
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;&lt;/span&gt;            .&lt;span style=&#34;color:#b44&#34;&gt;withReadyFunc&lt;/span&gt;( nodeInformer&lt;span style=&#34;color:#666&#34;&gt;::&lt;/span&gt;hasSynced) &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// optional, only starts controller when the cache has synced up
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;&lt;/span&gt;            .&lt;span style=&#34;color:#b44&#34;&gt;build&lt;/span&gt;();&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you notice, the new Java controller framework learnt a lot from the design of
&lt;a href=&#34;https://github.com/kubernetes-sigs/controller-runtime&#34; target=&#34;_blank&#34;&gt;controller-runtime&lt;/a&gt; which
successfully encapsulates the complex components inside controller into several
clean interfaces. With the help of Java Generics, we even move on a bit and simply
the encapsulation in a better way.&lt;/p&gt;

&lt;p&gt;As for more advanced usage, we can wrap multiple controllers into a controller-manager
or a leader-electing controller which helps deploying in HA setup. In a word, we can
basically find most of the equivalence implementations here from Golang SDK and
more advanced features are under active development by us.&lt;/p&gt;

&lt;h2 id=&#34;future-steps&#34;&gt;Future steps&lt;/h2&gt;

&lt;p&gt;The community behind the official Kubernetes Java SDK project will be focusing on
providing more useful utilities for developers who hope to program cloud native
Java applications to extend Kubernetes. If you are interested in more details,
please look at our repo &lt;a href=&#34;https://github.com/kubernetes-client/java&#34; target=&#34;_blank&#34;&gt;kubernetes-client/java&lt;/a&gt;.
Feel free to share also your feedback with us, through Issues or &lt;a href=&#34;http://kubernetes.slack.com/messages/kubernetes-client/&#34; target=&#34;_blank&#34;&gt;Slack&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Running Kubernetes locally on Linux with Microk8s</title>
      <link>https://kubernetes.io/blog/2019/11/26/running-kubernetes-locally-on-linux-with-microk8s/</link>
      <pubDate>Tue, 26 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/11/26/running-kubernetes-locally-on-linux-with-microk8s/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt;: &lt;a href=&#34;https://twitter.com/idvoretskyi&#34; target=&#34;_blank&#34;&gt;Ihor Dvoretskyi&lt;/a&gt;, Developer Advocate, Cloud Native Computing Foundation; &lt;a href=&#34;https://twitter.com/carminerimi&#34; target=&#34;_blank&#34;&gt;Carmine Rimi&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This article, the second in a &lt;a href=&#34;https://kubernetes.io/blog/2019/03/28/running-kubernetes-locally-on-linux-with-minikube-now-with-kubernetes-1.14-support/&#34;&gt;series&lt;/a&gt; about local deployment options on Linux, and covers &lt;a href=&#34;https://microk8s.io/&#34; target=&#34;_blank&#34;&gt;MicroK8s&lt;/a&gt;. Microk8s is the click-and-run solution for deploying a Kubernetes cluster locally, originally developed by Canonical, the publisher of Ubuntu.&lt;/p&gt;

&lt;p&gt;While Minikube usually spins up a local virtual machine (VM) for the Kubernetes cluster, MicroK8s doesn’t require a VM. It uses &lt;a href=&#34;https://snapcraft.io/&#34; target=&#34;_blank&#34;&gt;snap&lt;/a&gt; packages, an application packaging and isolation technology.&lt;/p&gt;

&lt;p&gt;This difference has its pros and cons. Here we’ll discuss a few of the interesting differences, and comparing the benefits of a VM based approach with the benefits of a non-VM approach. One of the first factors is cross-platform portability. While a Minikube VM is portable across operating systems - it supports not only Linux, but Windows, macOS, and even FreeBSD - Microk8s requires Linux, and only on those distributions &lt;a href=&#34;https://snapcraft.io/docs/installing-snapd&#34; target=&#34;_blank&#34;&gt;that support snaps&lt;/a&gt;. Most popular Linux distributions are supported.&lt;/p&gt;

&lt;p&gt;Another factor to consider is resource consumption. While a VM appliance gives you greater portability, it does mean you’ll consume more resources to run the VM, primarily because the VM ships a complete operating system, and runs on top of a hypervisor. You’ll consume more disk space when the VM is dormant. You’ll consume more RAM and CPU while it is running. Since Microk8s doesn’t require spinning up a virtual machine you’ll have more resources to run your workloads and other applications. Given its smaller footprint, MicroK8s is ideal for IoT devices - you can even use it on a Raspberry Pi device!&lt;/p&gt;

&lt;p&gt;Finally, the projects appear to follow a different release cadence and strategy. MicroK8s, and snaps in general provide &lt;a href=&#34;https://snapcraft.io/docs/channels&#34; target=&#34;_blank&#34;&gt;channels&lt;/a&gt; that allow you to consume beta and release candidate versions of new releases of Kubernetes, as well as the previous stable release. Microk8s generally releases the stable release of upstream Kubernetes almost immediately.&lt;/p&gt;

&lt;p&gt;But wait, there’s more! Minikube and MicroK8s both started as single-node clusters. Essentially, they allow you to create a Kubernetes cluster with a single worker node. That is about to change - there’s an early alpha release of MicroK8s that includes clustering. With this capability, you can create Kubernetes clusters with as many worker nodes as you wish. This is effectively an un-opinionated option for creating a cluster - the developer must create the network connectivity between the nodes, as well as integrate with other infrastructure that may be required, like an external load-balancer. In summary, MicroK8s offers a quick and easy way to turn a handful of computers or VMs into a multi-node Kubernetes cluster. We’ll write more about this kind of architecture in a future article.&lt;/p&gt;

&lt;h2 id=&#34;disclaimer&#34;&gt;Disclaimer&lt;/h2&gt;

&lt;p&gt;This is not an official guide to MicroK8s. You may find detailed information on running and using MicroK8s on it&amp;rsquo;s official &lt;a href=&#34;https://microk8s.io/docs/&#34; target=&#34;_blank&#34;&gt;webpage&lt;/a&gt;, where different use cases, operating systems, environments, etc. are covered. Instead, the purpose of this post is to provide clear and easy guidelines for running MicroK8s on Linux.&lt;/p&gt;

&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;

&lt;p&gt;A Linux distribution that &lt;a href=&#34;https://snapcraft.io/docs/installing-snapd&#34; target=&#34;_blank&#34;&gt;supports snaps&lt;/a&gt;, is required. In this guide, we’ll use Ubuntu 18.04 LTS, it supports snaps out-of-the-box.
If you are interested in running Microk8s on Windows or Mac, you should check out &lt;a href=&#34;https://multipass.run&#34; target=&#34;_blank&#34;&gt;Multipass&lt;/a&gt; to stand up a quick Ubuntu VM as the official way to run virtual Ubuntu on your system.&lt;/p&gt;

&lt;h2 id=&#34;microk8s-installation&#34;&gt;MicroK8s installation&lt;/h2&gt;

&lt;p&gt;MicroK8s installation is straightforward:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;sudo snap install microk8s --classic&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;center&gt;&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/001-install.png&#34; width=&#34;600&#34;/&gt; 
&lt;/figure&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;The command above installs a local single-node Kubernetes cluster in seconds. Once the command execution is finished, your Kubernetes cluster is up and running.&lt;/p&gt;

&lt;p&gt;You may verify the MicroK8s status with the following command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;sudo microk8s.status&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;center&gt;&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/002-status.png&#34; width=&#34;600&#34;/&gt; 
&lt;/figure&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;h2 id=&#34;using-microk8s&#34;&gt;Using microk8s&lt;/h2&gt;

&lt;p&gt;Using MicroK8s is as straightforward as installing it. MicroK8s itself includes a &lt;code&gt;kubectl&lt;/code&gt; binary, which can be accessed by running the &lt;code&gt;microk8s.kubectl&lt;/code&gt; command. As an example:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;microk8s.kubectl get nodes&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;center&gt;&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/003-nodes.png&#34; width=&#34;600&#34;/&gt; 
&lt;/figure&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;While using the prefix &lt;code&gt;microk8s.kubectl&lt;/code&gt; allows for a parallel install of another system-wide kubectl without impact, you can easily get rid of it by using the &lt;code&gt;snap alias&lt;/code&gt; command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;sudo snap &lt;span style=&#34;color:#a2f&#34;&gt;alias&lt;/span&gt; microk8s.kubectl kubectl&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This will allow you to simply use &lt;code&gt;kubectl&lt;/code&gt; after. You can revert this change using the &lt;code&gt;snap unalias&lt;/code&gt; command.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/004-alias.png&#34; width=&#34;600&#34;/&gt; 
&lt;/figure&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get nodes&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;center&gt;&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/005-nodes.png&#34; width=&#34;600&#34;/&gt; 
&lt;/figure&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;h2 id=&#34;microk8s-addons&#34;&gt;MicroK8s addons&lt;/h2&gt;

&lt;p&gt;One of the biggest benefits of using Microk8s is the fact that it also supports various add-ons and extensions. What is even more important is they are shipped out of the box, the user just has to enable them.&lt;/p&gt;

&lt;p&gt;The full list of extensions can be checked by running the &lt;code&gt;microk8s.status&lt;/code&gt; command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo microk8s.status
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As of the time of writing this article, the following add-ons are supported:&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/006-status.png&#34; width=&#34;600&#34;/&gt; 
&lt;/figure&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;More add-ons are being created and contributed by the community all the time, it definitely helps to check often!&lt;/p&gt;

&lt;h2 id=&#34;release-channels&#34;&gt;Release channels&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;sudo snap info microk8s&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;center&gt;&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/010-releases.png&#34; width=&#34;600&#34;/&gt; 
&lt;/figure&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;h2 id=&#34;installing-the-sample-application&#34;&gt;Installing the sample application&lt;/h2&gt;

&lt;p&gt;In this tutorial we’ll use NGINX as a sample application (&lt;a href=&#34;https://hub.docker.com/_/nginx&#34; target=&#34;_blank&#34;&gt;the official Docker Hub image&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;It will be installed as a Kubernetes deployment:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl create deployment nginx --image&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;nginx&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To verify the installation, let’s run the following:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get deployments&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get pods&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;center&gt;&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/007-deployments.png&#34; width=&#34;600&#34;/&gt; 
&lt;/figure&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Also, we can retrieve the full output of all available objects within our Kubernetes cluster:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get all --all-namespaces&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;center&gt;&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/008-all.png&#34; width=&#34;600&#34;/&gt; 
&lt;/figure&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;h2 id=&#34;uninstalling-microk8s&#34;&gt;Uninstalling MicroK8s&lt;/h2&gt;

&lt;p&gt;Uninstalling your microk8s cluster is so easy as uninstalling the snap:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;sudo snap remove microk8s&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;center&gt;&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/009-remove.png&#34; width=&#34;600&#34;/&gt; 
&lt;/figure&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;h2 id=&#34;screencast&#34;&gt;Screencast&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://asciinema.org/a/263394&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://asciinema.org/a/263394.svg&#34; alt=&#34;asciicast&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Grokkin&#39; the Docs</title>
      <link>https://kubernetes.io/blog/2019/11/05/grokkin-the-docs/</link>
      <pubDate>Tue, 05 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/11/05/grokkin-the-docs/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author:&lt;/strong&gt; &lt;a href=&#34;https://www.linkedin.com/in/aimee-ukasick/&#34; target=&#34;_blank&#34;&gt;Aimee Ukasick&lt;/a&gt;, Independent Contributor&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/grokkin-the-docs/grok-definition.png&#34;
         alt=&#34;grok: to understand profoundly and intuitively&#34;/&gt; &lt;figcaption&gt;
            &lt;h4&gt;Definition courtesy of Merriam Webster online dictionary&lt;/h4&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;


&lt;h2 id=&#34;intro-observations-of-a-new-sig-docs-contributor&#34;&gt;Intro - Observations of a new SIG Docs contributor&lt;/h2&gt;

&lt;p&gt;I began contributing to the SIG Docs community in August 2019. Sometimes I feel
like I am a stranger in a strange land adapting to a new community:
investigating community organization, understanding contributor society,
learning new lessons, and incorporating new jargon. I&amp;rsquo;m an observer as well as a
contributor.&lt;/p&gt;

&lt;h2 id=&#34;observation-1&#34;&gt;Observation 01: Read the &lt;em&gt;Contribute&lt;/em&gt; pages!&lt;/h2&gt;

&lt;p&gt;I contributed code and documentation to OpenStack, OPNFV, and Acumos, so I
thought contributing to the Kubernetes documentation would be the same. I was
wrong. I should have thoroughly &lt;strong&gt;read&lt;/strong&gt; the &lt;a href=&#34;https://kubernetes.io/docs/contribute/&#34; target=&#34;_blank&#34;&gt;Contribute to Kubernetes
docs&lt;/a&gt; pages instead of skimming them.&lt;/p&gt;

&lt;p&gt;I am very familiar with the git/gerrit workflow. With those tools, a contributor clones
the &lt;code&gt;master&lt;/code&gt; repo and then creates a local branch. Kubernetes uses a different
approach, called &lt;em&gt;Fork and Pull&lt;/em&gt;. Each contributor &lt;code&gt;forks&lt;/code&gt; the master repo, and
then the contributor pushes work to their fork before creating a pull request. I
created a simple pull request (PR), following the instructions in the &lt;strong&gt;Start
contributing&lt;/strong&gt; page&amp;rsquo;s &lt;a href=&#34;https://kubernetes.io/docs/contribute/start/#submit-a-pull-request&#34; target=&#34;_blank&#34;&gt;Submit a pull
request&lt;/a&gt;
section. This section describes how to make a documentation change using the
GitHub UI. I learned that this method is fine for a change that requires a
single commit to fix. However, this method becomes complicated when you have to
make additional updates to your PR.  GitHub creates a new commit for each change
made using the GitHub UI. The Kubernetes GitHub org requires squashing commits.
The &lt;strong&gt;Start contributing&lt;/strong&gt; page didn&amp;rsquo;t mention squashing commits, so I looked at
the GitHub and git documentation. I could not squash my commits using the GitHub
UI. I had to &lt;code&gt;git fetch&lt;/code&gt; and &lt;code&gt;git checkout&lt;/code&gt; my pull request locally, squash the
commits using the command line, and then push my changes. If the &lt;strong&gt;Start
contributing&lt;/strong&gt; had mentioned squashing commits, I would have worked from a local
clone instead of using the GitHub UI.&lt;/p&gt;

&lt;h2 id=&#34;observation-2&#34;&gt;Observation 02: Reach out and ping someone&lt;/h2&gt;

&lt;p&gt;While working on my first PRs, I had questions about working from a local clone
and about keeping my fork updated from &lt;code&gt;upstream master&lt;/code&gt;. I turned to searching
the internet instead of asking on the &lt;a href=&#34;http://slack.k8s.io/&#34; target=&#34;_blank&#34;&gt;Kubernetes Slack&lt;/a&gt;
#sig-docs channel. I used the wrong process to update my fork, so I had to &lt;code&gt;git
rebase&lt;/code&gt; my PRs, which did not go well at all. As a result, I closed those PRs
and submitted new ones.  When I asked for help on the #sig-docs channel,
contributors posted useful links, what my local git config file should look
like, and the exact set of git commands to run. The process used by contributors
was different than the one defined in the &lt;strong&gt;Intermediate contributing&lt;/strong&gt; page.
I would have saved myself so much time if I had asked what GitHub workflow to
use. The more community knowledge that is documented, the easier it is for new
contributors to be productive quickly.&lt;/p&gt;

&lt;h2 id=&#34;observation-3&#34;&gt;Observation 03: Don&amp;rsquo;t let conflicting information ruin your day&lt;/h2&gt;

&lt;p&gt;The Kubernetes community has a contributor guide for
&lt;a href=&#34;https://github.com/kubernetes/community/tree/master/contributors/guide&#34; target=&#34;_blank&#34;&gt;code&lt;/a&gt;
and another one for &lt;a href=&#34;https://kubernetes.io/docs/contribute/&#34; target=&#34;_blank&#34;&gt;documentation&lt;/a&gt;. The
guides contain conflicting information on the same topic. For example, the SIG
Docs GitHub process recommends creating a local branch based on
&lt;code&gt;upstream/master&lt;/code&gt;.  The &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/guide/github-workflow.md&#34; target=&#34;_blank&#34;&gt;Kubernetes Community Contributor
Guide&lt;/a&gt;
advocates updating your fork from upstream and then creating a local branch
based on your fork. Which process should a new contributor follow? Are the two
processes interchangeable? The best place to ask questions about conflicting
information is the #sig-docs or #sig-contribex channels. I asked for
clarification about the GitHub workflows in the #sig-contribex channel.
@cblecker provided an extremely detailed response, which I used to update the
&lt;strong&gt;Intermediate contributing&lt;/strong&gt; page.&lt;/p&gt;

&lt;h2 id=&#34;observation-4&#34;&gt;Observation 04: Information may be scattered&lt;/h2&gt;

&lt;p&gt;It&amp;rsquo;s common for large open source projects to have information scattered around
various repos or duplicated between repos. Sometimes groups work in silos, and
information is not shared. Other times, a person leaves to work on a
different project without passing on specialized knowledge.
Documentation gaps exist and may never be rectified because of higher priority
items. So new contributors may have difficulty finding basic information, such
as meeting details.&lt;/p&gt;

&lt;p&gt;Attending SIG Docs meetings is a great way to become involved. However, people
have had a hard time locating the meeting URL. Most new contributors ask in the
#sig-docs channel, but I decided to locate the meeting information in the docs.
This required several clicks over multiple pages. How many new contributors miss
meetings because they can&amp;rsquo;t locate the meeting details?&lt;/p&gt;

&lt;h2 id=&#34;observation-5&#34;&gt;Observation 05: Patience is a virtue&lt;/h2&gt;

&lt;p&gt;A contributor may wait days for feedback on a larger PR. The process from
submission to final approval may take weeks instead of days. There are two
reasons for this: 1) most reviewers work part-time on SIG Docs; and 2) reviewers
want to provide meaningful reviews. &amp;ldquo;Drive-by reviewing&amp;rdquo; doesn&amp;rsquo;t happen in SIG
Docs! Reviewers check for the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Do the commit message and PR description adequately describe the change?&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Does the PR follow the guidelines in the style and content guides?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Overall, is the grammar and punctuation correct?&lt;/li&gt;
&lt;li&gt;Is the content clear, concise, and appropriate for non-native speakers?&lt;/li&gt;
&lt;li&gt;Does the content stylistically fit in with the rest of the documentation?&lt;/li&gt;
&lt;li&gt;Does the flow of the content make sense?&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Can anything be changed to make the content better, such as using a Hugo shortcode?&lt;/li&gt;
&lt;li&gt;Does the content render correctly?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Is the content technically correct?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Sometimes the review process made me feel defensive, annoyed, and frustrated. I&amp;rsquo;m
sure other contributors have felt the same way. Contributors need to be patient!
Writing excellent documentation is an iterative process. Reviewers scrutinize
PRs because they want to maintain a high level of quality in the documentation,
not because they want to annoy contributors!&lt;/p&gt;

&lt;h2 id=&#34;observation-6&#34;&gt;Observation 06: Make every word count&lt;/h2&gt;

&lt;p&gt;Non-native English speakers read and contribute to the Kubernetes documentation.
When you are writing content, use simple, direct language in clear, concise
sentences.  Every sentence you write may be translated into another language, so
remove words that don&amp;rsquo;t add substance. I admit that implementing these
guidelines is challenging at times.&lt;/p&gt;

&lt;p&gt;Issues and pull requests aren&amp;rsquo;t translated into other languages. However, you
should still follow the aforementioned guidelines when you write the description
for an issue or pull request. You should add details and background
information to an issue so the person doing triage doesn&amp;rsquo;t have to apply the
&lt;code&gt;triage/needs-information&lt;/code&gt; label. Likewise, when you create a pull request, you
should add enough information about the content change that reviewers don&amp;rsquo;t have
to figure out the reason for the pull request. Providing details in clear,
concise language speeds up the process.&lt;/p&gt;

&lt;h2 id=&#34;observation-7&#34;&gt;Observation 07: Triaging issues is more difficult than it should be&lt;/h2&gt;

&lt;p&gt;In SIG Docs, triaging issues requires the ability to distinguish between
support, bug, and feature requests not only for the documentation but also for
Kubernetes code projects. How to route, label, and prioritize issues has become
easier week by week. I&amp;rsquo;m still not 100% clear on which SIG and/or project is
responsible for which parts of the documentation. The SIGs and Working Groups
&lt;a href=&#34;https://github.com/kubernetes/community/blob/master/sig-list.md&#34; target=&#34;_blank&#34;&gt;page&lt;/a&gt; helps,
but it is not enough. At a page level in the documentation, it&amp;rsquo;s not
always obvious which SIG or project has domain expertise. The page&amp;rsquo;s front
matter sometimes list reviewers but never lists a SIG or project. Each page should
indicate who is responsible for content, so that SIG Docs triagers know where to
route issues.&lt;/p&gt;

&lt;h2 id=&#34;observation-8&#34;&gt;Observation 08: SIG Docs is understaffed&lt;/h2&gt;

&lt;p&gt;Documentation is the number one driver of software adoption&lt;sup&gt;1&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Many contributors devote a small amount of time to SIG Docs but only a handful
are trained technical writers. Few companies have hired tech writers to work on
Kubernetes docs at least half-time. That&amp;rsquo;s very disheartening for online
documentation that has had over 53 million unique page views from readers in 229
countries year to date in 2019.&lt;/p&gt;

&lt;p&gt;SIG Docs faces challenges due to lack of technical writers:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Maintaining a high quality in the Kubernetes documentation&lt;/strong&gt;:
There are over 750 pages of documentation. That&amp;rsquo;s &lt;em&gt;750 pages&lt;/em&gt; to check for
stale content on a regular basis. This involves more than running a link
checker against the &lt;code&gt;kubernetes/website&lt;/code&gt; repo. This involves people having a
technical understanding of Kubernetes, knowing which code release changes
impact documentation, and knowing where content is located in the
documentation so that &lt;em&gt;all&lt;/em&gt; impacted pages and example code files are updated
in a timely fashion. Other SIGs help with this, but based on the number of
issues created by readers, enough people aren&amp;rsquo;t working on keeping the content
fresh.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reducing the time to review and merge a PR&lt;/strong&gt;:
The larger the size of the PR, the longer it takes to get the &lt;code&gt;lgtm&lt;/code&gt; label
and eventual approval. My &lt;code&gt;size/M&lt;/code&gt; and larger PRs took from five to thirty
days to approve. Sometimes I politely poked reviewers to review again after
I had pushed updates. Other times I asked on the #sig-docs channel for &lt;em&gt;any
approver&lt;/em&gt; to take a look and approve. People are busy. People go on
vacation. People also move on to new roles that don&amp;rsquo;t involve SIG Docs and
forget to remove themselves from the reviewer and approver assignment file.
A large part of the time-to-merge problem is not having enough reviewers and
approvers. The other part is the &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/community-membership.md#reviewer&#34; target=&#34;_blank&#34;&gt;high
barrier&lt;/a&gt;
to becoming a reviewer or approver, much higher than what I&amp;rsquo;ve seen on other
open source projects. Experienced open source tech writers who want to
contribute to SIG Docs aren&amp;rsquo;t fast-tracked into approver and reviewer roles.
On one hand, that high barrier ensures that those roles are filled by folks
with a minimum level of Kubernetes documentation knowledge; on the other
hand, it might deter experienced tech writers from contributing at all, or
from a company allocating a tech writer to SIG Docs. Maybe SIG Docs should
consider deviating from the Kubernetes community requirements by lowering
the barrier to becoming a reviewer or approver, on a case-by-case basis, of
course.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ensuring consistent naming across all pages&lt;/strong&gt;:
Terms should be identical to what is used in the &lt;strong&gt;Standardized Glossary&lt;/strong&gt;. Being consistent reduces confusion.
Tracking down and fixing these occurrences is time-consuming but worthwhile for readers.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Working with the Steering Committee to create project documentation guidelines&lt;/strong&gt;:
The &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/github-management/kubernetes-repositories.md&#34; target=&#34;_blank&#34;&gt;Kubernetes Repository Guidelines&lt;/a&gt; don&amp;rsquo;t mention documentation at all. Between a
project&amp;rsquo;s GitHub docs and the Kubernetes docs, some projects have almost
duplicate content, whereas others have conflicting content. Create clear
guidelines so projects know to put roadmaps, milestones, and comprehensive
feature details in the &lt;code&gt;kubernetes/&amp;lt;project&amp;gt;&lt;/code&gt; repo and to put installation,
configuration, usage details, and tutorials in the Kubernetes docs.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Removing duplicate content&lt;/strong&gt;:
Kubernetes users install Docker, so a good example of duplicate content is
Docker installation instructions. Rather than repeat what&amp;rsquo;s in the Docker
docs, state which version of Docker works with which version of Kubernetes
and link to the Docker docs for installation. Then detail any
Kubernetes-specific configuration. That idea is the same for the container
runtimes that Kubernetes supports.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Removing third-party vendor content&lt;/strong&gt;:
This is tightly coupled to removing duplicate content. Some third-party
content consists of lists or tables detailing external products. Other
third-party content is found in the &lt;strong&gt;Tasks&lt;/strong&gt; and &lt;strong&gt;Tutorials&lt;/strong&gt; sections.
SIG Docs should not be responsible for verifying that third-party products
work with the latest version of Kubernetes. Nor should SIG Docs be
responsible for maintaining lists of training courses or cloud providers.
Additionally, the Kubernetes documentation isn&amp;rsquo;t the place to pitch vendor
products. If SIG Docs is forced to reverse its policy on not allowing
third-party content, there could be a tidal wave of
vendor-or-commercially-oriented pull requests. Maintaining that content
places an undue burden on SIG Docs.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Indicating which version of Kubernetes works with each task and tutorial&lt;/strong&gt;:
This means reviewing each task and tutorial for every release. Readers
assume if a task or tutorial is in the latest version of the docs, it works
with the latest version of Kubernetes.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Addressing issues&lt;/strong&gt;:
There are 470 open issues in the &lt;code&gt;kubernetes/website&lt;/code&gt; repo. It&amp;rsquo;s hard to keep up with all the issues that are created. We encourage
those creating simpler issues to submit PRs: some do; most do not.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Creating more detailed content&lt;/strong&gt;:
Readers
&lt;a href=&#34;https://kubernetes.io/blog/2019/10/29/kubernetes-documentation-end-user-survey&#34; target=&#34;_blank&#34;&gt;indicated&lt;/a&gt;
they would like to see more detailed content across all sections of the
documentation, including tutorials.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Kubernetes has seen unparalleled growth since its first release in 2015. Like
any fast-growing project, it has growing pains. Providing consistently
high-quality documentation is one of those pains, and one incredibly important
to an open source project. SIG Docs needs a larger core team of tech writers who
are allocated at least 50%. SIG Docs can then better achieve goals, move forward
with new content, update existing content, and address open issues in a timely fashion.&lt;/p&gt;

&lt;h2 id=&#34;observation-9&#34;&gt;Observation 09: Contributing to technical documentation projects requires, on average, more skills than developing software&lt;/h2&gt;

&lt;p&gt;When I said that to my former colleagues, the response was a healthy dose of
skepticism and lots of laughter. It seems that many developers, as well as
managers, don&amp;rsquo;t fully know what tech writers contributing to open source
projects actually do. Having done both development and technical writing for the
better part of 22 years, I&amp;rsquo;ve noticed that tech writers are valued far less than
software developers of comparative standing.&lt;/p&gt;

&lt;p&gt;SIG Docs core team members do far more than write content based on requirements:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;We use some of the same processes and tools as developers, such as the
terminal, git workflow, GitHub, and IDEs like Atom, Golang, and Visual Studio Code; we
also use documentation-specific plugins and tools.&lt;/li&gt;
&lt;li&gt;We possess a good eye for detail as well as design and organization: the big picture &lt;em&gt;and&lt;/em&gt; the little picture.&lt;/li&gt;
&lt;li&gt;We provide documentation which has a logical flow; it is not merely content on a page
but the way pages fit into sections and sections fit into the overall structure.&lt;/li&gt;
&lt;li&gt;We write content that is comprehensive and uses language that readers not fluent in English can understand.&lt;/li&gt;
&lt;li&gt;We have a firm grasp of English composition using various markup languages.&lt;/li&gt;
&lt;li&gt;We are technical, sometimes to the level of a Kubernetes admin.&lt;/li&gt;
&lt;li&gt;We read, understand, and occasionally write code.&lt;/li&gt;
&lt;li&gt;We are project managers, able to plan new work as well as assign issues to releases.&lt;/li&gt;
&lt;li&gt;We are educators and diplomats with every review we do and with every comment we leave on an issue.&lt;/li&gt;
&lt;li&gt;We use site analytics to plan work based on which pages readers access most often as well as which pages readers say are unhelpful.&lt;/li&gt;
&lt;li&gt;We are surveyors, soliciting feedback from the community on a regular basis.&lt;/li&gt;
&lt;li&gt;We analyze the documentation as a whole, deciding what content should stay and
what content should be removed based on available resources and reader needs.&lt;/li&gt;
&lt;li&gt;We have a working knowledge of Hugo and other frameworks used for
online documentation; we know how to create, use, and debug Hugo shortcodes that
enable content to be more robust than pure Markdown.&lt;/li&gt;
&lt;li&gt;We troubleshoot performance issues not only with Hugo but with Netlify.&lt;/li&gt;
&lt;li&gt;We grapple with the complex problem of API documentation.&lt;/li&gt;
&lt;li&gt;We are dedicated to providing the highest quality documentation that we can.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you have any doubts about the complexity of the Kubernetes documentation
project, watch presentations given by SIG Docs Chair Zach Corleissen:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://archive.fosdem.org/2019/schedule/event/multikuber/&#34; target=&#34;_blank&#34;&gt;Multilingual Kubernetes&lt;/a&gt; - the kubernetes.io stack, how we got there, and what it took to get there&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://youtu.be/GXkpHAruNV8&#34; target=&#34;_blank&#34;&gt;Found in Translation: Lessons from a Year of Open Source Localization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Additionally, &lt;a href=&#34;https://youtu.be/JvRd7MmAxPw&#34; target=&#34;_blank&#34;&gt;Docs as Code: The Missing Manual&lt;/a&gt;
(Jennifer Rondeau, Margaret Eker; 2016) is an excellent presentation on the
complexity of documentation projects in general.&lt;/p&gt;

&lt;p&gt;The Write the Docs &lt;a href=&#34;http://www.writethedocs.org/&#34; target=&#34;_blank&#34;&gt;website&lt;/a&gt; and &lt;a href=&#34;https://www.youtube.com/channel/UCr019846MitZUEhc6apDdcQ&#34; target=&#34;_blank&#34;&gt;YouTube
channel&lt;/a&gt; are
fantastic places to delve into the good, the bad, and the ugly of technical writing.&lt;/p&gt;

&lt;p&gt;Think what an open source project would be without talented, dedicated tech writers!&lt;/p&gt;

&lt;h2 id=&#34;observation-10&#34;&gt;Observation 10: Community is everything&lt;/h2&gt;

&lt;p&gt;The SIG Docs community, and the larger Kubernetes community, is dedicated,
intelligent, friendly, talented, fun, helpful, and a whole bunch of other
positive adjectives! People welcomed me with open arms, and not only because SIG
Docs needs more technical writers. I have never felt that my ideas and contributions were
dismissed because I was the newbie. Humility and respect go a long way.
Community members have a wealth of knowledge to share. Attend meetings, ask
questions, propose improvements, thank people, and contribute in
every way that you can!&lt;/p&gt;

&lt;p&gt;Big shout out to those who helped me, and put up with me (LOL), during my
break-in period: @zacharaysarah, @sftim, @kbhawkey, @jaypipes,  @jrondeau,
@jmangel, @bradtopol, @cody_clark, @thecrudge, @jaredb, @tengqm, @steveperry-53,
@mrbobbytables, @cblecker, and @kbarnard10.&lt;/p&gt;

&lt;h2 id=&#34;outro&#34;&gt;Outro&lt;/h2&gt;

&lt;p&gt;Do I grok SIG Docs? Not quite yet, but I do understand that SIG Docs needs more
dedicated resources to continue to be successful.&lt;/p&gt;

&lt;h2 id=&#34;citations&#34;&gt;Citations&lt;/h2&gt;

&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt; @linuxfoundation. &amp;ldquo;Megan Byrd-Sanicki, Open Source Strategist, Google @megansanicki - documentation is the #1 driver of software adoption. #ossummit.&amp;rdquo; &lt;em&gt;Twitter&lt;/em&gt;, Oct 29, 2019, 3:54 a.m., twitter.com/linuxfoundation/status/1189103201439637510.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes Documentation Survey</title>
      <link>https://kubernetes.io/blog/2019/10/29/kubernetes-documentation-end-user-survey/</link>
      <pubDate>Tue, 29 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/10/29/kubernetes-documentation-end-user-survey/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author:&lt;/strong&gt; &lt;a href=&#34;https://www.linkedin.com/in/aimee-ukasick/&#34; target=&#34;_blank&#34;&gt;Aimee Ukasick&lt;/a&gt; and SIG Docs&lt;/p&gt;

&lt;p&gt;In September, SIG Docs conducted its first survey about the &lt;a href=&#34;https://kubernetes.io/docs/&#34; target=&#34;_blank&#34;&gt;Kubernetes
documentation&lt;/a&gt;. We&amp;rsquo;d like to thank the CNCF&amp;rsquo;s Kim
McMahon for helping us create the survey and access the results.&lt;/p&gt;

&lt;h1 id=&#34;key-takeaways&#34;&gt;Key takeaways&lt;/h1&gt;

&lt;p&gt;Respondents would like more example code, more detailed content, and more
diagrams in the Concepts, Tasks, and Reference sections.&lt;/p&gt;

&lt;p&gt;74% of respondents would like the Tutorials section to contain advanced content.&lt;/p&gt;

&lt;p&gt;69.70% said the Kubernetes documentation is the first place they look for
information about Kubernetes.&lt;/p&gt;

&lt;h1 id=&#34;survey-methodology-and-respondents&#34;&gt;Survey methodology and respondents&lt;/h1&gt;

&lt;p&gt;We conducted the survey in English. The survey was only available for 4 days due
to time constraints. We announced the survey on Kubernetes mailing lists, in
Kubernetes Slack channels, on Twitter, and in Kube Weekly. There were 23
questions, and respondents took an average of 4 minutes to complete the survey.&lt;/p&gt;

&lt;h2 id=&#34;quick-facts-about-respondents&#34;&gt;Quick facts about respondents:&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;48.48% are experienced Kubernetes users, 26.26% expert, and 25.25% beginner&lt;/li&gt;
&lt;li&gt;57.58% use Kubernetes in both administrator and developer roles&lt;/li&gt;
&lt;li&gt;64.65% have been using the Kubernetes documentation for more than 12 months&lt;/li&gt;
&lt;li&gt;95.96% read the documentation in English&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;question-and-response-highlights&#34;&gt;Question and response highlights&lt;/h1&gt;

&lt;h2 id=&#34;why-people-access-the-kubernetes-documentation&#34;&gt;Why people access the Kubernetes documentation&lt;/h2&gt;

&lt;p&gt;The majority of respondents stated that they access the documentation for the Concepts.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-sig-docs-survey/Q9-k8s-docs-use.png&#34;
         alt=&#34;Why respondents access the Kubernetes documentation&#34;/&gt; 
&lt;/figure&gt;


&lt;p&gt;This deviates only slightly from what we see in Google Analytics: of the top 10
most viewed pages this year, #1 is the kubectl cheatsheet in the Reference section,
followed overwhelmingly by pages in the Concepts section.&lt;/p&gt;

&lt;h2 id=&#34;satisfaction-with-the-documentation&#34;&gt;Satisfaction with the documentation&lt;/h2&gt;

&lt;p&gt;We asked respondents to record their level of satisfaction with the detail in
the Concepts, Tasks, Reference, and Tutorials sections:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Concepts: 47.96% Moderately Satisfied&lt;/li&gt;
&lt;li&gt;Tasks: 50.54% Moderately Satisfied&lt;/li&gt;
&lt;li&gt;Reference: 40.86% Very Satisfied&lt;/li&gt;
&lt;li&gt;Tutorial: 47.25% Moderately Satisfied&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;how-sig-docs-can-improve-each-documentation-section&#34;&gt;How SIG Docs can improve each documentation section&lt;/h2&gt;

&lt;p&gt;We asked how we could improve each section, providing respondents with
selectable answers as well as a text field. The clear majority would like more
example code, more detailed content, more diagrams, and advanced tutorials:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-text&#34; data-lang=&#34;text&#34;&gt;- Personally, would like to see more analogies to help further understanding.
- Would be great if corresponding sections of code were explained too
- Expand on the concepts to bring them together - they&amp;#39;re a bucket of separate eels moving in different directions right now
- More diagrams, and more example code&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Respondents used the &amp;ldquo;Other&amp;rdquo; text box to record areas causing frustration:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-text&#34; data-lang=&#34;text&#34;&gt;- Keep concepts up to date and accurate
- Keep task topics up to date and accurate. Human testing.
- Overhaul the examples. Many times the output of commands shown is not actual.
- I&amp;#39;ve never understood how to navigate or interpret the reference section
- Keep the tutorials up to date, or remove them&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&#34;how-sig-docs-can-improve-the-documentation-overall&#34;&gt;How SIG Docs can improve the documentation overall&lt;/h2&gt;

&lt;p&gt;We asked respondents how we can improve the Kubernetes documentation
overall. Some took the opportunity to tell us we are doing a good job:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-text&#34; data-lang=&#34;text&#34;&gt;- For me, it is the best documented open source project.
- Keep going!
- I find the documentation to be excellent.
- You [are] doing a great job. For real.&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Other respondents provided feedback on the content:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-text&#34; data-lang=&#34;text&#34;&gt;-  ...But since we&amp;#39;re talking about docs, more is always better. More
advanced configuration examples would be, to me, the way to go. Like a Use Case page for each
configuration topic with beginner to advanced example scenarios. Something like that would be
awesome....
- More in-depth examples and use cases would be great. I often feel that the Kubernetes
documentation scratches the surface of a topic, which might be great for new users, but it leaves
more experienced users without much &amp;#34;official&amp;#34; guidance on how to implement certain things.
- More production like examples in the resource sections (notably secrets) or links to production like
examples
- It would be great to see a very clear &amp;#34;Quick Start&amp;#34; A-&amp;gt;Z up and running like many other tech
projects. There are a handful of almost-quick-starts, but no single guidance. The result is
information overkill.&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;A few respondents provided technical suggestions:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-text&#34; data-lang=&#34;text&#34;&gt;- Make table columns sortable and filterable using a ReactJS or Angular component.
- For most, I think creating documentation with Hugo - a system for static site generation - is not
appropriate. There are better systems for documenting large software project. Specifically, I would
like to see k8s switch to Sphinx for documentation. It has an excellent built-in search, it is easy to
learn if you know markdown, it is widely adopted by other projects (e.g. every software project in
readthedocs.io, linux kernel, docs.python.org etc).&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Overall, respondents provided constructive criticism focusing on the need for
advanced use cases as well as more in-depth examples, guides, and walkthroughs.&lt;/p&gt;

&lt;h1 id=&#34;where-to-see-more&#34;&gt;Where to see more&lt;/h1&gt;

&lt;p&gt;Survey results summary, charts, and raw data are available in &lt;code&gt;kubernetes/community&lt;/code&gt; sig-docs &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-docs/survey&#34; target=&#34;_blank&#34;&gt;survey&lt;/a&gt; directory.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Contributor Summit San Diego Schedule Announced!</title>
      <link>https://kubernetes.io/blog/2019/10/10/contributor-summit-san-diego-schedule/</link>
      <pubDate>Thu, 10 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/10/10/contributor-summit-san-diego-schedule/</guid>
      <description>
        
        
        &lt;p&gt;Authors: Josh Berkus (Red Hat), Paris Pittman (Google), Jonas Rosland (VMware)&lt;/p&gt;

&lt;p&gt;tl;dr A week ago we announced that &lt;a href=&#34;https://events19.linuxfoundation.org/events/kubernetes-contributor-summit-north-america-2019/register/&#34; target=&#34;_blank&#34;&gt;registration is open&lt;/a&gt; for the contributor
summit , and we&amp;rsquo;re now live with &lt;a href=&#34;https://events19.linuxfoundation.org/events/kubernetes-contributor-summit-north-america-2019/program/schedule/&#34; target=&#34;_blank&#34;&gt;the full Contributor Summit schedule!&lt;/a&gt;
Grab your spot while tickets are still available. There is currently a waitlist
for new contributor workshop.  (&lt;a href=&#34;https://events19.linuxfoundation.org/events/kubernetes-contributor-summit-north-america-2019/register/&#34; target=&#34;_blank&#34;&gt;Register here!&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;There are many great sessions planned for the Contributor Summit, spread across
five rooms of current contributor content in addition to the new contributor
workshops. Since this is an upstream contributor summit and we don&amp;rsquo;t often meet,
being a globally distributed team, most of these sessions are discussions or
hands-on labs, not just presentations.  We want folks to learn and have a
good time meeting their OSS teammates.&lt;/p&gt;

&lt;p&gt;Unconference tracks are returning from last year with sessions to be chosen
Monday morning. These are ideal for the latest hot topics and specific
discussions that contributors want to have. In previous years, we&amp;rsquo;ve covered
flaky tests, cluster lifecycle, KEPs (Kubernetes Enhancement Proposals), mentoring,
security, and more.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2019-10-10-contributor-summit-san-diego-schedule/DSCF0806.jpg&#34; alt=&#34;Unconference&#34; /&gt;&lt;/p&gt;

&lt;p&gt;While the schedule contains difficult decisions in every timeslot, we&amp;rsquo;ve picked
a few below to give you a taste of what you&amp;rsquo;ll hear, see, and participate in, at
the summit:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://sched.co/VvMc&#34; target=&#34;_blank&#34;&gt;Vision&lt;/a&gt;&lt;/strong&gt;: SIG-Architecture will be sharing their vision of where we&amp;rsquo;re going
with Kubernetes development for the next year and beyond.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://sched.co/VvMj&#34; target=&#34;_blank&#34;&gt;Security&lt;/a&gt;&lt;/strong&gt;: Tim Allclair and CJ Cullen will present on the current state of
Kubernetes security. In another security talk, Vallery Lancey will lead a
discussion about making our platform secure by default.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://sched.co/Vv6Z&#34; target=&#34;_blank&#34;&gt;Prow&lt;/a&gt;&lt;/strong&gt;: Interested in working with Prow and contributing to Test-Infra, but
not sure where to start?  Rob Keilty will help you get a Prow test environment
running on your laptop.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://sched.co/VvNa&#34; target=&#34;_blank&#34;&gt;Git&lt;/a&gt;&lt;/strong&gt;: Staff from GitHub will be collaborating with Christoph Blecker to share
practical Git tips for Kubernetes contributors.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://sched.co/VutA&#34; target=&#34;_blank&#34;&gt;Reviewing&lt;/a&gt;&lt;/strong&gt;: Tim Hockin will share the secrets of becoming a great code
reviewer, and Jordan Liggitt will conduct a live API review so that you can do
one, or at least pass one.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://sched.co/VvNJ&#34; target=&#34;_blank&#34;&gt;End Users&lt;/a&gt;&lt;/strong&gt;: Several end users from the CNCF partner ecosystem, invited by
Cheryl Hung, will hold a Q&amp;amp;A with contributors to strengthen our feedback loop.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://sched.co/Vux2&#34; target=&#34;_blank&#34;&gt;Docs&lt;/a&gt;&lt;/strong&gt;: As always, SIG-Docs will run a three-hour contributing-to-documentation
workshop.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We&amp;rsquo;re also giving out awards to contributors who distinguished themselves in 2019,
and there will be a huge Meet &amp;amp; Greet for new contributors to find their SIG
(and for existing contributors to ask about their PRs) at the end of the day on
Monday.&lt;/p&gt;

&lt;p&gt;Hope to see you all there, and &lt;a href=&#34;https://events19.linuxfoundation.org/events/kubernetes-contributor-summit-north-america-2019/register/&#34; target=&#34;_blank&#34;&gt;make sure you register!&lt;/a&gt;
&lt;a href=&#34;http://git.k8s.io/community/events/events-team&#34; target=&#34;_blank&#34;&gt;San Diego team&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: 2019 Steering Committee Election Results</title>
      <link>https://kubernetes.io/blog/2019/10/03/2019-steering-committee-election-results/</link>
      <pubDate>Thu, 03 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/10/03/2019-steering-committee-election-results/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt;: Bob Killen (University of Michigan), Jorge Castro (VMware),
Brian Grant (Google), and Ihor Dvoretskyi (CNCF)&lt;/p&gt;

&lt;p&gt;The &lt;a href=&#34;https://git.k8s.io/community/events/elections/2019&#34; target=&#34;_blank&#34;&gt;2019 Steering Committee Election&lt;/a&gt; is a landmark milestone for the
Kubernetes project. The initial bootstrap committee is graduating to emeritus
and the committee has now shrunk to its final allocation of seven seats. All
members of the Steering Committee are now fully elected by the Kubernetes
Community.&lt;/p&gt;

&lt;p&gt;Moving forward elections will elect either 3 or 4 people to the committee for
two-year terms.&lt;/p&gt;

&lt;h2 id=&#34;results&#34;&gt;&lt;strong&gt;Results&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;The Kubernetes Steering Committee Election is now complete and the following
candidates came ahead to secure two-year terms that start immediately
(in alphabetical order by GitHub handle):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Christoph Blecker (&lt;a href=&#34;https://github.com/cblecker&#34; target=&#34;_blank&#34;&gt;@cblecker&lt;/a&gt;), Red Hat&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Derek Carr (&lt;a href=&#34;https://github.com/derekwaynecarr&#34; target=&#34;_blank&#34;&gt;@derekwaynecarr&lt;/a&gt;), Red Hat&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Nikhita Raghunath (&lt;a href=&#34;https://github.com/nikhita&#34; target=&#34;_blank&#34;&gt;@nikhita&lt;/a&gt;), Loodse&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Paris Pittman (&lt;a href=&#34;https://github.com/parispittman&#34; target=&#34;_blank&#34;&gt;@parispittman&lt;/a&gt;)&lt;/strong&gt;, &lt;strong&gt;Google&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;They join Aaron Crickenberger (&lt;a href=&#34;https://github.com/spiffxp&#34; target=&#34;_blank&#34;&gt;@spiffxp&lt;/a&gt;), Google; Davanum Srinivas (&lt;a href=&#34;https://github.com/dims&#34; target=&#34;_blank&#34;&gt;@dims&lt;/a&gt;),
VMware; and Timothy St. Clair (&lt;a href=&#34;https://github.com/timothysc&#34; target=&#34;_blank&#34;&gt;@timothysc&lt;/a&gt;), VMware, to round out the committee.
The seats held by Aaron, Davanum, and Timothy will be up for election around
this time next year.&lt;/p&gt;

&lt;h2 id=&#34;big-thanks&#34;&gt;Big Thanks!&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Thanks to the initial bootstrap committee for establishing the initial
project governance and overseeing a multi-year transition period:

&lt;ul&gt;
&lt;li&gt;Joe Beda (&lt;a href=&#34;https://github.com/jbeda&#34; target=&#34;_blank&#34;&gt;@jbeda&lt;/a&gt;), VMware&lt;/li&gt;
&lt;li&gt;Brendan Burns (&lt;a href=&#34;https://github.com/brendandburns&#34; target=&#34;_blank&#34;&gt;@brendandburns&lt;/a&gt;), Microsoft&lt;/li&gt;
&lt;li&gt;Clayton Coleman (&lt;a href=&#34;https://github.com/smarterclayton&#34; target=&#34;_blank&#34;&gt;@smarterclayton&lt;/a&gt;), Red Hat&lt;/li&gt;
&lt;li&gt;Brian Grant (&lt;a href=&#34;https://github.com/bgrant0607&#34; target=&#34;_blank&#34;&gt;@bgrant0607&lt;/a&gt;), Google&lt;/li&gt;
&lt;li&gt;Tim Hockin (&lt;a href=&#34;https://github.com/thockin&#34; target=&#34;_blank&#34;&gt;@thockin&lt;/a&gt;), Google&lt;/li&gt;
&lt;li&gt;Sarah Novotny (&lt;a href=&#34;https://github.com/sarahnovotny&#34; target=&#34;_blank&#34;&gt;@sarahnovotny&lt;/a&gt;), Microsoft&lt;/li&gt;
&lt;li&gt;Brandon Philips (&lt;a href=&#34;https://github.com/philips&#34; target=&#34;_blank&#34;&gt;@philips&lt;/a&gt;), Red Hat&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;And also thanks to the other Emeritus Steering Committee Members. Your
prior service is appreciated by the community:

&lt;ul&gt;
&lt;li&gt;Quinton Hoole (&lt;a href=&#34;https://github.com/quinton-hoole&#34; target=&#34;_blank&#34;&gt;@quinton-hoole&lt;/a&gt;), Huawei&lt;/li&gt;
&lt;li&gt;Michelle Noorali (&lt;a href=&#34;https://github.com/michelleN&#34; target=&#34;_blank&#34;&gt;@michelleN&lt;/a&gt;), Microsoft&lt;/li&gt;
&lt;li&gt;Phillip Wittrock (&lt;a href=&#34;https://github.com/pwittrock&#34; target=&#34;_blank&#34;&gt;@pwittrock&lt;/a&gt;), Google&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Thanks to the candidates that came forward to run for election. May we always
have a strong set of people who want to push the community forward like yours
in every election.&lt;/li&gt;
&lt;li&gt;Thanks to all 377 voters who cast a ballot.&lt;/li&gt;
&lt;li&gt;And last but not least…Thanks to Cornell University for hosting &lt;a href=&#34;https://civs.cs.cornell.edu/&#34; target=&#34;_blank&#34;&gt;CIVS&lt;/a&gt;!&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;get-involved-with-the-steering-committee&#34;&gt;Get Involved with the Steering Committee&lt;/h2&gt;

&lt;p&gt;You can follow along with Steering Committee &lt;a href=&#34;https://github.com/kubernetes/steering/projects/1&#34; target=&#34;_blank&#34;&gt;backlog items&lt;/a&gt; and weigh in by
filing an issue or creating a PR against their &lt;a href=&#34;https://github.com/kubernetes/steering&#34; target=&#34;_blank&#34;&gt;repo&lt;/a&gt;. They meet bi-weekly on
&lt;a href=&#34;https://github.com/kubernetes/steering&#34; target=&#34;_blank&#34;&gt;Wednesdays at 8pm UTC&lt;/a&gt; and regularly attend Meet Our Contributors.  They can
also be contacted at their public mailing list &lt;a href=&#34;mailto:steering@kubernetes.io&#34; target=&#34;_blank&#34;&gt;steering@kubernetes.io&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Steering Committee Meetings:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PL69nYSiGNLP1yP1B_nd9-drjoxp0Q14qM&#34; target=&#34;_blank&#34;&gt;YouTube Playlist&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Contributor Summit San Diego Registration Open!</title>
      <link>https://kubernetes.io/blog/2019/09/24/san-diego-contributor-summit/</link>
      <pubDate>Tue, 24 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/09/24/san-diego-contributor-summit/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Authors: Paris Pittman (Google), Jeffrey Sica (Red Hat), Jonas Rosland (VMware)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://events.linuxfoundation.org/events/kubernetes-contributor-summit-north-america-2019/&#34; target=&#34;_blank&#34;&gt;Contributor Summit San Diego 2019 Event Page&lt;/a&gt;&lt;br /&gt;
Registration is now open and in record time, we’ve hit capacity for the
&lt;em&gt;new contributor workshop&lt;/em&gt; session of the event! Waitlist is now available.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Sunday, November 17&lt;/strong&gt;&lt;br /&gt;
Evening Contributor Celebration:&lt;br /&gt;
&lt;a href=&#34;https://quartyardsd.com/&#34; target=&#34;_blank&#34;&gt;QuartYard&lt;/a&gt;*&lt;br /&gt;
Address: 1301 Market Street, San Diego, CA 92101&lt;br /&gt;
Time: 6:00PM - 9:00PM&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Monday, November 18&lt;/strong&gt;&lt;br /&gt;
All Day Contributor Summit:&lt;br /&gt;
&lt;a href=&#34;https://www.marriott.com/hotels/travel/sandt-marriott-marquis-san-diego-marina/?scid=bb1a189a-fec3-4d19-a255-54ba596febe2&#34; target=&#34;_blank&#34;&gt;Marriott Marquis San Diego Marina&lt;/a&gt;&lt;br /&gt;
Address: 333 W Harbor Dr, San Diego, CA 92101&lt;br /&gt;
Time: 9:00AM - 5:00PM&lt;/p&gt;

&lt;p&gt;While the Kubernetes project is only five years old, we’re already going into our
9th Contributor Summit this November in San Diego before KubeCon + CloudNativeCon.
The rapid increase is thanks to adding European and Asian Contributor Summits to
the North American events we’ve done previously. We will continue to run Contributor
Summits across the globe, as it is important that our contributor base grows in
all forms of diversity.&lt;/p&gt;

&lt;p&gt;Kubernetes has a large distributed remote contributing team, from &lt;a href=&#34;https://k8s.devstats.cncf.io/d/8/company-statistics-by-repository-group?orgId=1&amp;amp;var-period=y&amp;amp;var-metric=contributions&amp;amp;var-repogroup_name=All&amp;amp;var-companies=All&#34; target=&#34;_blank&#34;&gt;individuals and
organizations&lt;/a&gt; all over the world. The Contributor Summits give the community three
chances a year to get together, work on community topics, and have hallway track
time. The upcoming San Diego summit is expected to bring over 450 attendees, and
will contain multiple tracks with something for everyone. The focus will be around
contributor growth and sustainability. We&amp;rsquo;re going to stop here with capacity for
future summits; we want this event to offer value to individuals and the project.
We&amp;rsquo;ve heard from past summit attendee feedback that getting work done, learning,
and meeting folks face to face is a priority. By capping attendance and offering
the contributor gatherings in more locations, it will help us achieve those goals.&lt;/p&gt;

&lt;p&gt;This summit is unique as we’ve taken big moves on sustaining ourselves, the
contributor experience events team. Taking a page from the release team’s playbook,
we have added additional core team and shadow roles making it a natural mentoring
(watching+doing) relationship. The shadows are expected to fill another role at
one of the three events in 2020, and core team members to take the lead.
In preparation for this team, we’ve open sourced our &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/events/events-team&#34; target=&#34;_blank&#34;&gt;rolebooks, guidelines,
best practices&lt;/a&gt; and opened up our &lt;a href=&#34;https://docs.google.com/document/d/1oLXv5_rM4f645jlXym_Vd7AUq7x6DV-O87E6tcW1sjU/edit?usp=sharing&#34; target=&#34;_blank&#34;&gt;meetings&lt;/a&gt; and &lt;a href=&#34;https://github.com/orgs/kubernetes/projects/21&#34; target=&#34;_blank&#34;&gt;project board&lt;/a&gt;. Our team makes up
many parts of the Kubernetes project and takes care of making sure all voices
are represented.&lt;/p&gt;

&lt;p&gt;Are you at KubeCon + CloudNativeCon but can’t make it to the summit? Check out
the &lt;a href=&#34;https://kccncna19.sched.com/overview/type/Maintainer+Track+Sessions?iframe=yes&#34; target=&#34;_blank&#34;&gt;SIG Intro and Deep Dive sessions&lt;/a&gt; during KubeCon + CloudNativeCon to
participate in Q&amp;amp;A and hear what’s up with each Special interest Group (SIG).
We’ll also record all of Contributor Summit’s presentation sessions, take notes
in discussions, and share it back with you, after the event is complete.&lt;/p&gt;

&lt;p&gt;We hope to see you all at Kubernetes Contributor Summit San Diego, make sure you
head over and &lt;a href=&#34;https://events.linuxfoundation.org/events/kubernetes-contributor-summit-north-america-2019/&#34; target=&#34;_blank&#34;&gt;register right now&lt;/a&gt;! This event will sell out - here’s your warning.
:smiley:&lt;/p&gt;

&lt;p&gt;Check out past blogs on &lt;a href=&#34;https://kubernetes.io/blog/2019/03/20/a-look-back-and-whats-in-store-for-kubernetes-contributor-summits/&#34; target=&#34;_blank&#34;&gt;persona building around our events&lt;/a&gt; and the &lt;a href=&#34;https://kubernetes.io/blog/2019/06/25/recap-of-kubernetes-contributor-summit-barcelona-2019/&#34; target=&#34;_blank&#34;&gt;Barcelona summit story&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2019-09-24-san-diego-contributor-summit/IMG_2588.JPG&#34; alt=&#34;Group Picture in 2018&#34; /&gt;&lt;/p&gt;

&lt;p&gt;*=QuartYard has a huge stage! Want to perform something in front of your contributor peers? Reach out to us! community@kubernetes.io&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes 1.16: Custom Resources, Overhauled Metrics, and Volume Extensions</title>
      <link>https://kubernetes.io/blog/2019/09/18/kubernetes-1-16-release-announcement/</link>
      <pubDate>Wed, 18 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/09/18/kubernetes-1-16-release-announcement/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; &lt;a href=&#34;https://github.com/kubernetes/sig-release/blob/master/releases/release-1.16/release_team.md&#34; target=&#34;_blank&#34;&gt;Kubernetes 1.16 Release Team&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We’re pleased to announce the delivery of Kubernetes 1.16, our third release of 2019! Kubernetes 1.16 consists of 31 enhancements: 8 enhancements moving to stable, 8 enhancements in beta, and 15 enhancements in alpha.&lt;/p&gt;

&lt;h1 id=&#34;major-themes&#34;&gt;Major Themes&lt;/h1&gt;

&lt;h2 id=&#34;custom-resources&#34;&gt;Custom resources&lt;/h2&gt;

&lt;p&gt;CRDs are in widespread use as a Kubernetes extensibility mechanism and have been available in beta since the 1.7 release. The 1.16 release marks the graduation of CRDs to general availability (GA).&lt;/p&gt;

&lt;h2 id=&#34;overhauled-metrics&#34;&gt;Overhauled metrics&lt;/h2&gt;

&lt;p&gt;Kubernetes has previously made extensive use of a global metrics registry to register metrics to be exposed. By implementing a metrics registry, metrics are registered in more transparent means. Previously, Kubernetes metrics have been excluded from any kind of stability requirements.&lt;/p&gt;

&lt;h2 id=&#34;volume-extension&#34;&gt;Volume Extension&lt;/h2&gt;

&lt;p&gt;There are quite a few enhancements in this release that pertain to volumes and volume modifications. Volume resizing support in CSI specs is moving to beta which allows for any CSI spec volume plugin to be resizable.&lt;/p&gt;

&lt;h1 id=&#34;significant-changes-to-the-kubernetes-api&#34;&gt;Significant Changes to the Kubernetes API&lt;/h1&gt;

&lt;p&gt;As the Kubernetes API has evolved, we have promoted some API resources to &lt;em&gt;stable&lt;/em&gt;, others have been reorganized to different groups. We deprecate older versions of a resource and make newer versions available in accordance with the &lt;a href=&#34;https://kubernetes.io/docs/concepts/overview/kubernetes-api/#api-versioning&#34; target=&#34;_blank&#34;&gt;API versioning policy&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;An example of this is the &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/deployment/&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;Deployment&lt;/code&gt;&lt;/a&gt; resource. This was introduced under the &lt;code&gt;extensions/v1beta1&lt;/code&gt; group in 1.6 and as the project changed has been promoted to &lt;code&gt;extensions/v1beta2&lt;/code&gt;, &lt;code&gt;apps/v1beta2&lt;/code&gt; and finally promoted to &lt;code&gt;stable&lt;/code&gt; and moved to &lt;code&gt;apps/v1&lt;/code&gt; in 1.9.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s important to note that until this release the project has not stopped serving any of the previous versions of the any of the deprecated resources.&lt;/p&gt;

&lt;p&gt;This means that folks interacting with the Kubernetes API have not been &lt;em&gt;required&lt;/em&gt; to move to the new version of any of the deprecated API objects.&lt;/p&gt;

&lt;p&gt;In 1.16 if you submit a &lt;code&gt;Deployment&lt;/code&gt; to the API server and specify &lt;code&gt;extensions/v1beta1&lt;/code&gt; as the API group it will be rejected with:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;error: unable to recognize &amp;quot;deployment&amp;quot;: no matches for kind &amp;quot;Deployment&amp;quot; in version &amp;quot;extensions/v1beta1&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With this release we are taking a very important step in the maturity of the Kubernetes API, and are no longer serving the deprecated APIs. Our earlier post &lt;a href=&#34;https://kubernetes.io/blog/2019/07/18/api-deprecations-in-1-16/&#34; target=&#34;_blank&#34;&gt;Deprecated APIs Removed In 1.16: Here’s What You Need To Know&lt;/a&gt; tells you more, including which resources are affected.&lt;/p&gt;

&lt;h1 id=&#34;additional-enhancements&#34;&gt;Additional Enhancements&lt;/h1&gt;

&lt;h2 id=&#34;custom-resources-reach-general-availability&#34;&gt;Custom Resources Reach General Availability&lt;/h2&gt;

&lt;p&gt;CRDs have become the basis for extensions in the Kubernetes ecosystem. Started as a ground-up redesign of the ThirdPartyResources prototype, they have finally reached GA in 1.16 with apiextensions.k8s.io/v1, as the hard-won lessons of API evolution in Kubernetes have been integrated. As we transition to GA, the focus is on data consistency for API clients.&lt;/p&gt;

&lt;p&gt;As you upgrade to the GA API, you’ll notice that several of the previously optional guard rails have become required and/or default behavior. Things like structural schemas, pruning unknown fields, validation, and protecting the *.k8s.io group are important for ensuring the longevity of your APIs and are now much harder to accidentally miss. Defaulting is another important part of API evolution and that support will be on by default for CRD.v1. The combination of these, along with CRD conversion mechanisms are enough to build stable APIs that evolve over time, the same way that native Kubernetes resources have changed without breaking backward-compatibility.&lt;/p&gt;

&lt;p&gt;Updates to the CRD API won’t end here. We have ideas for features like arbitrary subresources, API group migration, and maybe a more efficient serialization protocol, but the changes from here are expected to be optional and complementary in nature to what’s already here in the GA API. Happy operator writing!&lt;/p&gt;

&lt;p&gt;Details on how to work with custom resources can be found &lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/&#34; target=&#34;_blank&#34;&gt;in the Kubernetes documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;opening-doors-with-windows-enhancements&#34;&gt;Opening Doors With Windows Enhancements&lt;/h2&gt;

&lt;h3 id=&#34;beta-enhancing-the-workload-identity-options-for-windows-containers&#34;&gt;Beta: Enhancing the workload identity options for Windows containers&lt;/h3&gt;

&lt;p&gt;Active Directory Group Managed Service Account (GMSA) support is graduating to beta and certain annotations that were introduced with the alpha support are being deprecated. GMSA is a specific type of Active Directory account that enables Windows containers to carry an identity across the network and communicate with other resources. Windows containers can now gain authenticated access to external resources. In addition, GMSA provides automatic password management, simplified service principal name (SPN) management, and the ability to delegate the management to other administrators across multiple servers.&lt;/p&gt;

&lt;p&gt;Adding support for RunAsUserName as an alpha release. The RunAsUserName is a string specifying the windows identity (or username) in Windows to run the entrypoint of the container and is a part of the newly introduced windowsOptions component of the securityContext (WindowsSecurityContextOptions).&lt;/p&gt;

&lt;h3 id=&#34;alpha-improvements-to-setup-node-join-experience-with-kubeadm&#34;&gt;Alpha: Improvements to setup &amp;amp; node join experience with kubeadm&lt;/h3&gt;

&lt;p&gt;Introducing alpha support for kubeadm, enabling Kubernetes users to easily join (and reset) Windows worker nodes to an existing cluster the same way they do for Linux nodes. Users can utilize kubeadm to prepare and add a Windows node to cluster. When the operations are complete, the node will be in a Ready state and able to run Windows containers. In addition, we will also provide a set of Windows-specific scripts to enable the installation of prerequisites and CNIs ahead of joining the node to the cluster.&lt;/p&gt;

&lt;h3 id=&#34;alpha-introducing-support-for-container-storage-interface-csi&#34;&gt;Alpha: Introducing support for Container Storage Interface (CSI)&lt;/h3&gt;

&lt;p&gt;Introducing CSI plugin support for out-of-tree providers, enabling Windows nodes in a Kubernetes cluster to leverage persistent storage capabilities for Windows-based workloads. This significantly expands the storage options of Windows workloads, adding onto a list that included FlexVolume and in-tree storage plugins. This capability is achieved through a host OS proxy that enables the execution of privileged operations on the Windows node on behalf of containers.&lt;/p&gt;

&lt;h2 id=&#34;introducing-endpoint-slices&#34;&gt;Introducing Endpoint Slices&lt;/h2&gt;

&lt;p&gt;The release of Kubernetes 1.16 includes an exciting new alpha feature: the EndpointSlice API. This API provides a scalable and extensible alternative to the &lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.16/#endpoints-v1-core&#34; target=&#34;_blank&#34;&gt;Endpoints&lt;/a&gt; resource, which dates back to the very first versions of Kubernetes. Behind the scenes, Endpoints play a big role in network routing within Kubernetes. Each Service endpoint is tracked within these resources - kube-proxy uses them for generating proxy rules that allow pods to communicate with each other so easily in Kubernetes, and many ingress controllers use them to route HTTP traffic directly to pods.&lt;/p&gt;

&lt;h3 id=&#34;providing-greater-scalability&#34;&gt;Providing Greater Scalability&lt;/h3&gt;

&lt;p&gt;A key goal for EndpointSlices is to enable greater scalability for Kubernetes Services. With the existing Endpoints API, a single instance must include network endpoints representing all pods matching a Service. As Services start to scale to thousands of pods, the corresponding Endpoints resources become quite large. Simply adding or removing one endpoint from a Service at this scale can be quite costly. As the Endpoints instance is updated, every piece of code watching Endpoints will need to be sent a full copy of the resource. With kube-proxy running on every node in a cluster, a copy needs to be sent to every single node. At a small scale, this is not an issue, but it becomes increasingly noticeable as clusters get larger.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2019-09-18-kubernetes-1-16-release-announcement/endpoint-slices.png&#34; alt=&#34;Endpoints to Endpoint Slice&#34; /&gt;&lt;/p&gt;

&lt;p&gt;With EndpointSlices, network endpoints for a Service are split into multiple instances, significantly decreasing the amount of data required for updates at scale. By default, EndpointSlices are limited to 100 endpoints each.&lt;/p&gt;

&lt;p&gt;For example, let’s take a cluster with 10,000 Service endpoints spread over 5,000 nodes. A single Pod update would result in approximately 5GB transmitted with the Endpoints API (that’s enough to fill a DVD). This becomes increasingly significant given how frequently Endpoints can change during events like rolling updates on Deployments. The same update will be much more efficient with EndpointSlices since each one includes only a tiny portion of the total number of Service endpoints. Instead of transferring a big Endpoints object to each node, only the small EndpointSlice that’s been changed has to be transferred. In this example, EndpointSlices would decrease data transferred by approximately 100x.&lt;/p&gt;

&lt;table&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Endpoints &lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Endpoint Slices&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;# of resources
   &lt;/td&gt;
   &lt;td&gt;&lt;em&gt;1&lt;/em&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;em&gt;20k / 100 = 200&lt;/em&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;# of network endpoints stored
   &lt;/td&gt;
   &lt;td&gt;&lt;em&gt;1 * 20k = 20k&lt;/em&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;em&gt;200 * 100 = 20k&lt;/em&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;size of each resource
   &lt;/td&gt;
   &lt;td&gt;&lt;em&gt;20k * const = ~2.0 MB&lt;/em&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;em&gt; 100 * const = ~10 kB&lt;/em&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;watch event data transferred
   &lt;/td&gt;
   &lt;td&gt;&lt;em&gt;~2.0MB * 5k = 10GB&lt;/em&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;em&gt;~10kB * 5k = 50MB&lt;/em&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h3 id=&#34;providing-greater-extensibility&#34;&gt;Providing Greater Extensibility&lt;/h3&gt;

&lt;p&gt;A second goal for EndpointSlices was to provide a resource that would be highly extensible and useful across a wide variety of use cases. One of the key additions with EndpointSlices involves a new topology attribute. By default, this will be populated with the existing topology labels used throughout Kubernetes indicating attributes such as region and zone. Of course, this field can be populated with custom labels as well for more specialized use cases.&lt;/p&gt;

&lt;p&gt;EndpointSlices also include greater flexibility for address types. Each contains a list of addresses. An initial use case for multiple addresses would be to support dual-stack endpoints with both IPv4 and IPv6 addresses. As an example, here’s a simple EndpointSlice showing how one could be represented:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: discovery.k8s.io/v1alpha
kind: EndpointSlice
metadata:
  name: example-abc
  labels:
    kubernetes.io/service-name: example
addressType: IP
ports:
  - name: http
    protocol: TCP
    port: 80
endpoints:
  - addresses:
    - &amp;quot;10.1.2.3&amp;quot;
    - &amp;quot;2001:db8::1234:5678&amp;quot;
    topology:
      kubernetes.io/hostname: node-1
      topology.kubernetes.io/zone: us-west2-a
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;more-about-endpoint-slices&#34;&gt;More About Endpoint Slices&lt;/h3&gt;

&lt;p&gt;EndpointSlices are an alpha feature in Kubernetes 1.16 and not enabled by default. The Endpoints API will continue to be enabled by default, but we’re working to move the largest Endpoints consumers to the new EndpointSlice API. Notably, kube-proxy in Kubernetes 1.16 includes alpha support for EndpointSlices.&lt;/p&gt;

&lt;p&gt;The official Kubernetes documentation contains more information about EndpointSlices as well as how to enable them in your cluster. There’s also a &lt;a href=&#34;https://www.youtube.com/watch?v=Y5JOCCbJ_Fg&#34; target=&#34;_blank&#34;&gt;great KubeCon talk&lt;/a&gt; that provides more background on the initial rationale for developing this API.&lt;/p&gt;

&lt;h4 id=&#34;notable-feature-updates&#34;&gt;Notable Feature Updates&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/693&#34; target=&#34;_blank&#34;&gt;Topology Manager&lt;/a&gt;, a new Kubelet component, aims to co-ordinate resource assignment decisions to provide optimized resource allocations.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs//concepts/services-networking/dual-stack/&#34; target=&#34;_blank&#34;&gt;IPv4/IPv6 dual-stack&lt;/a&gt; enables the allocation of both IPv4 and IPv6 addresses to Pods and Services.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/blob/master/keps/sig-cloud-provider/20190422-cloud-controller-manager-migration.md&#34; target=&#34;_blank&#34;&gt;Extensions&lt;/a&gt; for Cloud Controller Manager Migration.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;availability&#34;&gt;Availability&lt;/h2&gt;

&lt;p&gt;Kubernetes 1.16 is available for &lt;a href=&#34;https://github.com/kubernetes/kubernetes/releases/tag/v1.16.0&#34; target=&#34;_blank&#34;&gt;download on GitHub&lt;/a&gt;. To get started with Kubernetes, check out these &lt;a href=&#34;https://kubernetes.io/docs/tutorials/&#34; target=&#34;_blank&#34;&gt;interactive tutorials&lt;/a&gt;. You can also easily install 1.16 using &lt;a href=&#34;https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/&#34; target=&#34;_blank&#34;&gt;kubeadm&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;release-team&#34;&gt;Release Team&lt;/h2&gt;

&lt;p&gt;This release is made possible through the efforts of hundreds of individuals who contributed both technical and non-technical content. Special thanks to the &lt;a href=&#34;http://bit.ly/k8s116-team&#34; target=&#34;_blank&#34;&gt;release team&lt;/a&gt; led by Lachlan Evenson, Principal Program Manager at Microsoft. The 32 individuals on the release team coordinated many aspects of the release, from documentation to testing, validation, and feature completeness.&lt;/p&gt;

&lt;p&gt;As the Kubernetes community has grown, our release process represents an amazing demonstration of collaboration in open source software development. Kubernetes continues to gain new users at a rapid pace. This growth creates a positive feedback cycle where more contributors commit code creating a more vibrant ecosystem. Kubernetes has had over &lt;a href=&#34;https://k8s.devstats.cncf.io/d/24/overall-project-statistics?orgId=1&#34; target=&#34;_blank&#34;&gt;32,000 individual contributors&lt;/a&gt; to date and an active community of more than 66,000 people.&lt;/p&gt;

&lt;h2 id=&#34;release-mascot&#34;&gt;Release Mascot&lt;/h2&gt;

&lt;p&gt;The Kubernetes 1.16 release crest was loosely inspired by the Apollo 16 mission crest. It represents the hard work of the release-team and the community alike and is an ode to the challenges and fun times we shared as a team throughout the release cycle. Many thanks to Ronan Flynn-Curran of Microsoft for creating this magnificent piece.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2019-09-18-kubernetes-1-16-release-announcement/mascot.png&#34; alt=&#34;Kubernetes 1.16 Release Mascot&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;kubernetes-updates&#34;&gt;Kubernetes Updates&lt;/h1&gt;

&lt;h2 id=&#34;project-velocity&#34;&gt;Project Velocity&lt;/h2&gt;

&lt;p&gt;The CNCF has continued refining DevStats, an ambitious project to visualize the myriad contributions that go into the project. &lt;a href=&#34;https://k8s.devstats.cncf.io&#34; target=&#34;_blank&#34;&gt;K8s DevStats&lt;/a&gt; illustrates the breakdown of contributions from major company contributors, as well as an impressive set of preconfigured reports on everything from individual contributors to pull request lifecycle times. This past year, &lt;a href=&#34;https://k8s.devstats.cncf.io/d/9/companies-table?orgId=1&amp;amp;var-period_name=Last%20year&amp;amp;var-metric=contributions&#34; target=&#34;_blank&#34;&gt;1,147 different companies and over 3,149 individuals&lt;/a&gt; contribute to Kubernetes each month. &lt;a href=&#34;https://k8s.devstats.cncf.io/d/11/companies-contributing-in-repository-groups?orgId=1&amp;amp;var-period=m&amp;amp;var-repogroup_name=All&#34; target=&#34;_blank&#34;&gt;Check out DevStats&lt;/a&gt; to learn more about the overall velocity of the Kubernetes project and community.&lt;/p&gt;

&lt;h2 id=&#34;ecosystem&#34;&gt;Ecosystem&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;The Kubernetes project leadership created the Security Audit Working Group to oversee the very first third-part &lt;a href=&#34;https://www.cncf.io/blog/2019/08/06/open-sourcing-the-kubernetes-security-audit/&#34; target=&#34;_blank&#34;&gt;Kubernetes security audit&lt;/a&gt;, in an effort to improve the overall security of the ecosystem.&lt;/li&gt;
&lt;li&gt;The &lt;a href=&#34;https://www.cncf.io/announcement/2019/07/09/the-cloud-native-computing-foundation-announces-the-kubernetes-certified-service-providers-program-has-reached-100-participants/&#34; target=&#34;_blank&#34;&gt;Kubernetes Certified Service Providers&lt;/a&gt; program (KCSP) reached 100 member companies, ranging from the largest multinational cloud, enterprise software, and consulting companies to tiny startups.&lt;/li&gt;
&lt;li&gt;The first &lt;a href=&#34;https://www.cncf.io/blog/2019/08/29/announcing-the-cncf-kubernetes-project-journey-report/&#34; target=&#34;_blank&#34;&gt;Kubernetes Project Journey Report&lt;/a&gt; was released, showcasing the massive growth of the project.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;kubecon-cloudnativecon&#34;&gt;KubeCon + CloudNativeCon&lt;/h2&gt;

&lt;p&gt;The Cloud Native Computing Foundation’s flagship conference gathers adopters and technologists from leading open source and cloud native communities in San Diego, California from November 18-21, 2019. Join Kubernetes, Prometheus, Envoy, CoreDNS, containerd, Fluentd, OpenTracing, gRPC, CNI, Jaeger, Notary, TUF, Vitess, NATS, Linkerd, Helm, Rook, Harbor, etcd, Open Policy Agent, CRI-O, and TiKV as the community gathers for four days to further the education and advancement of cloud native computing. &lt;a href=&#34;https://www.cncf.io/community/kubecon-cloudnativecon-events/&#34; target=&#34;_blank&#34;&gt;Register today&lt;/a&gt;!&lt;/p&gt;

&lt;h2 id=&#34;webinar&#34;&gt;Webinar&lt;/h2&gt;

&lt;p&gt;Join members of the Kubernetes 1.16 release team on Oct 22, 2019 to learn about the major features in this release. Register &lt;a href=&#34;https://zoom.us/webinar/register/9015681469655/WN_JTLYA0DMRD6Mnm2f64KYMg&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;get-involved&#34;&gt;Get Involved&lt;/h2&gt;

&lt;p&gt;The simplest way to get involved with Kubernetes is by joining one of the many &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/sig-list.md&#34; target=&#34;_blank&#34;&gt;Special Interest Groups&lt;/a&gt; (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/communication&#34; target=&#34;_blank&#34;&gt;community meeting&lt;/a&gt;, and through the channels below. Thank you for your continued feedback and support.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Follow us on Twitter &lt;a href=&#34;https://twitter.com/kubernetesio&#34; target=&#34;_blank&#34;&gt;@Kubernetesio&lt;/a&gt; for latest updates&lt;/li&gt;
&lt;li&gt;Join the community discussion on &lt;a href=&#34;https://discuss.kubernetes.io/&#34; target=&#34;_blank&#34;&gt;Discuss&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Join the community on &lt;a href=&#34;http://slack.k8s.io/&#34; target=&#34;_blank&#34;&gt;Slack&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Post questions (or answer questions) on &lt;a href=&#34;http://stackoverflow.com/questions/tagged/kubernetes&#34; target=&#34;_blank&#34;&gt;Stack Overflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Share your Kubernetes &lt;a href=&#34;https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform&#34; target=&#34;_blank&#34;&gt;story&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Announcing etcd 3.4</title>
      <link>https://kubernetes.io/blog/2019/08/30/announcing-etcd-3-4/</link>
      <pubDate>Fri, 30 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/08/30/announcing-etcd-3-4/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Gyuho Lee (Amazon Web Services, @&lt;a href=&#34;https://github.com/gyuho&#34; target=&#34;_blank&#34;&gt;gyuho&lt;/a&gt;), Jingyi Hu (Google, @&lt;a href=&#34;https://github.com/jingyih&#34; target=&#34;_blank&#34;&gt;jingyih&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;etcd 3.4 focuses on stability, performance and ease of operation, with features like pre-vote and non-voting member and improvements to storage backend and client balancer.&lt;/p&gt;

&lt;p&gt;Please see &lt;a href=&#34;https://github.com/etcd-io/etcd/blob/master/CHANGELOG-3.4.md&#34; target=&#34;_blank&#34;&gt;CHANGELOG&lt;/a&gt; for full lists of changes.&lt;/p&gt;

&lt;h2 id=&#34;better-storage-backend&#34;&gt;Better Storage Backend&lt;/h2&gt;

&lt;p&gt;etcd v3.4 includes a number of performance improvements for large scale Kubernetes workloads.&lt;/p&gt;

&lt;p&gt;In particular, etcd experienced performance issues with a large number of concurrent read transactions even when there is no write (e.g. &lt;code&gt;“read-only range request ... took too long to execute”&lt;/code&gt;). Previously, the storage backend commit operation on pending writes blocks incoming read transactions, even when there was no pending write. Now, the commit &lt;a href=&#34;https://github.com/etcd-io/etcd/pull/9296&#34; target=&#34;_blank&#34;&gt;does not block reads&lt;/a&gt; which improve long-running read transaction performance.&lt;/p&gt;

&lt;p&gt;We further made &lt;a href=&#34;https://github.com/etcd-io/etcd/pull/10523&#34; target=&#34;_blank&#34;&gt;backend read transactions fully concurrent&lt;/a&gt;. Previously, ongoing long-running read transactions block writes and upcoming reads. With this change, write throughput is increased by 70% and P99 write latency is reduced by 90% in the presence of long-running reads. We also ran &lt;a href=&#34;https://prow.k8s.io/view/gcs/kubernetes-jenkins/logs/ci-kubernetes-e2e-gce-scale-performance/1130745634945503235&#34; target=&#34;_blank&#34;&gt;Kubernetes 5000-node scalability test on GCE&lt;/a&gt; with this change and observed similar improvements. For example, in the very beginning of the test where there are a lot of long-running “LIST pods”, the P99 latency of “POST clusterrolebindings” is &lt;a href=&#34;https://github.com/etcd-io/etcd/pull/10523#issuecomment-499262001&#34; target=&#34;_blank&#34;&gt;reduced by 97.4%&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;More improvements have been made to lease storage. We enhanced &lt;a href=&#34;https://github.com/etcd-io/etcd/pull/9418&#34; target=&#34;_blank&#34;&gt;lease expire/revoke performance&lt;/a&gt; by storing lease objects more efficiently, and made &lt;a href=&#34;https://github.com/etcd-io/etcd/pull/9229&#34; target=&#34;_blank&#34;&gt;lease look-up operation non-blocking&lt;/a&gt; with current lease grant/revoke operation. And etcd v3.4 introduces &lt;a href=&#34;https://github.com/etcd-io/etcd/pull/9924&#34; target=&#34;_blank&#34;&gt;lease checkpoint&lt;/a&gt; as an experimental feature to persist remaining time-to-live values through consensus. This ensures short-lived lease objects are not auto-renewed after leadership election. This also prevents lease object pile-up when the time-to-live value is relatively large (e.g. &lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/65497&#34; target=&#34;_blank&#34;&gt;1-hour TTL never expired in Kubernetes use case&lt;/a&gt;).&lt;/p&gt;

&lt;h2 id=&#34;improved-raft-voting-process&#34;&gt;Improved Raft Voting Process&lt;/h2&gt;

&lt;p&gt;etcd server implements &lt;a href=&#34;https://raft.github.io&#34; target=&#34;_blank&#34;&gt;Raft consensus algorithm&lt;/a&gt; for data replication. Raft is a leader-based protocol. Data is replicated from leader to follower; a follower forwards proposals to a leader, and the leader decides what to commit or not. Leader persists and replicates an entry, once it has been agreed by the quorum of cluster. The cluster members elect a single leader, and all other members become followers. The elected leader periodically sends heartbeats to its followers to maintain its leadership, and expects responses from each follower to keep track of its progress.&lt;/p&gt;

&lt;p&gt;In its simplest form, a Raft leader steps down to a follower when it receives a message with higher terms without any further cluster-wide health checks. This behavior can affect the overall cluster availability.&lt;/p&gt;

&lt;p&gt;For instance, a flaky (or rejoining) member drops in and out, and starts campaign. This member ends up with higher terms, ignores all incoming messages with lower terms, and sends out messages with higher terms. When the leader receives this message of a higher term, it reverts back to follower.&lt;/p&gt;

&lt;p&gt;This becomes more disruptive when there’s a network partition. Whenever the partitioned node regains its connectivity, it can possibly trigger the leader re-election. To address this issue, etcd Raft introduces a new node state pre-candidate with the &lt;a href=&#34;https://github.com/etcd-io/etcd/pull/9352&#34; target=&#34;_blank&#34;&gt;pre-vote feature&lt;/a&gt;. The pre-candidate first asks other servers whether it&amp;rsquo;s up-to-date enough to get votes. Only if it can get votes from the majority, it increments its term and starts an election. This extra phase improves the robustness of leader election in general. And helps the leader remain stable as long as it maintains its connectivity with the quorum of its peers.&lt;/p&gt;

&lt;p&gt;Similarly, etcd availability can be affected when a restarting node has not received the leader heartbeats in time (e.g. due to slow network), which triggers the leader election. Previously, etcd fast-forwards election ticks on server start, with only one tick left for leader election. For example, when the election timeout is 1-second, the follower only waits 100ms for leader contacts before starting an election. This speeds up initial server start, by not having to wait for the election timeouts (e.g. election is triggered in 100ms instead of 1-second). Advancing election ticks is also useful for cross datacenter deployments with larger election timeouts. However, in many cases, the availability is more critical than the speed of initial leader election. To ensure better availability with rejoining nodes, etcd now &lt;a href=&#34;https://github.com/etcd-io/etcd/pull/9415&#34; target=&#34;_blank&#34;&gt;adjusts election ticks&lt;/a&gt; with more than one tick left, thus more time for the leader to prevent a disruptive restart.&lt;/p&gt;

&lt;h2 id=&#34;raft-non-voting-member-learner&#34;&gt;Raft Non-Voting Member, Learner&lt;/h2&gt;

&lt;p&gt;The challenge with membership reconfiguration is that it often leads to quorum size changes, which are prone to cluster unavailabilities. Even if it does not alter the quorum, clusters with membership change are more likely to experience other underlying problems. To improve the reliability and confidence of reconfiguration, a new role - learner is introduced in etcd 3.4 release.&lt;/p&gt;

&lt;p&gt;A new etcd member joins the cluster with no initial data, requesting all historical updates from the leader until it catches up to the leader’s logs. This means the leader’s network is more likely to be overloaded, blocking or dropping leader heartbeats to followers. In such cases, a follower may experience election-timeout and start a new leader election. That is, a cluster with a new member is more vulnerable to leader election. Both leader election and the subsequent update propagation to the new member are prone to causing periods of cluster unavailability (see &lt;em&gt;Figure 1&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2019-08-30-announcing-etcd-3.4/figure-1.png&#34; alt=&#34;learner-figure-1&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The worst case is a misconfigured membership add. Membership reconfiguration in etcd is a two-step process: &lt;code&gt;etcdctl member add&lt;/code&gt; with peer URLs, and starting a new etcd to join the cluster. That is, &lt;code&gt;member add&lt;/code&gt; command is applied whether the peer URL value is invalid or not. If the first step is to apply the invalid URLs and change the quorum size, it is possible that the cluster already loses the quorum until the new node connects. Since the node with invalid URLs will never become online and there’s no leader, it is impossible to revert the membership change (see &lt;em&gt;Figure 2&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2019-08-30-announcing-etcd-3.4/figure-2.png&#34; alt=&#34;learner-figure-2&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This becomes more complicated when there are partitioned nodes (see the &lt;a href=&#34;https://github.com/etcd-io/etcd/blob/master/Documentation/learning/design-learner.md&#34; target=&#34;_blank&#34;&gt;design document&lt;/a&gt; for more).&lt;/p&gt;

&lt;p&gt;In order to address such failure modes, etcd introduces a &lt;a href=&#34;https://github.com/etcd-io/etcd/issues/10537&#34; target=&#34;_blank&#34;&gt;new node state “Learner”&lt;/a&gt;, which joins the cluster as a non-voting member until it catches up to leader’s logs. This means the learner still receives all updates from leader, while it does not count towards the quorum, which is used by the leader to evaluate peer activeness. The learner only serves as a standby node until promoted. This relaxed requirements for quorum provides the better availability during membership reconfiguration and operational safety (see &lt;em&gt;Figure 3&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2019-08-30-announcing-etcd-3.4/figure-3.png&#34; alt=&#34;learner-figure-3&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We will further improve learner robustness, and explore auto-promote mechanisms for easier and more reliable operation. Please read our &lt;a href=&#34;https://github.com/etcd-io/etcd/blob/master/Documentation/learning/design-learner.md&#34; target=&#34;_blank&#34;&gt;learner design documentation&lt;/a&gt; and &lt;a href=&#34;https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/runtime-configuration.md#add-a-new-member-as-learner&#34; target=&#34;_blank&#34;&gt;runtime-configuration document&lt;/a&gt; for user guides.&lt;/p&gt;

&lt;h2 id=&#34;new-client-balancer&#34;&gt;New Client Balancer&lt;/h2&gt;

&lt;p&gt;etcd is designed to tolerate various system and network faults. By design, even if one node goes down, the cluster “appears” to be working normally, by providing one logical cluster view of multiple servers. But, this does not guarantee the liveness of the client. Thus, etcd client has implemented a different set of intricate protocols to guarantee its correctness and high availability under faulty conditions.&lt;/p&gt;

&lt;p&gt;Historically, etcd client balancer heavily relied on old gRPC interface: every gRPC dependency upgrade broke client behavior. A majority of development and debugging efforts were devoted to fixing those client behavior changes. As a result, its implementation has become overly complicated with bad assumptions on server connectivity. The primary goal was to &lt;a href=&#34;https://github.com/etcd-io/etcd/pull/9860&#34; target=&#34;_blank&#34;&gt;simplify balancer failover logic in etcd v3.4 client&lt;/a&gt;; instead of maintaining a list of unhealthy endpoints, which may be stale, simply roundrobin to the next endpoint whenever client gets disconnected from the current endpoint. It does not assume endpoint status. Thus, no more complicated status tracking is needed.&lt;/p&gt;

&lt;p&gt;Furthermore, the new client now creates its own credential bundle to &lt;a href=&#34;https://github.com/etcd-io/etcd/pull/10911&#34; target=&#34;_blank&#34;&gt;fix balancer failover against secure endpoints&lt;/a&gt;. This resolves the &lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/72102&#34; target=&#34;_blank&#34;&gt;year-long bug&lt;/a&gt;, where kube-apiserver loses its connectivity to etcd cluster when the first etcd server becomes unavailable.&lt;/p&gt;

&lt;p&gt;Please see &lt;a href=&#34;https://github.com/etcd-io/etcd/blob/master/Documentation/learning/design-client.md&#34; target=&#34;_blank&#34;&gt;client balancer design documentation&lt;/a&gt; for more.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: OPA Gatekeeper: Policy and Governance for Kubernetes</title>
      <link>https://kubernetes.io/blog/2019/08/06/opa-gatekeeper-policy-and-governance-for-kubernetes/</link>
      <pubDate>Tue, 06 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/08/06/opa-gatekeeper-policy-and-governance-for-kubernetes/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Rita Zhang (Microsoft), Max Smythe (Google), Craig Hooper (Commonwealth Bank AU), Tim Hinrichs (Styra), Lachie Evenson (Microsoft), Torin Sandall (Styra)&lt;/p&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/open-policy-agent/gatekeeper&#34; target=&#34;_blank&#34;&gt;Open Policy Agent Gatekeeper&lt;/a&gt; project can be leveraged to help enforce policies and strengthen governance in your Kubernetes environment. In this post, we will walk through the goals, history, and current state of the project.&lt;/p&gt;

&lt;p&gt;The following recordings from the Kubecon EU 2019 sessions are a great starting place in working with Gatekeeper:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://youtu.be/Yup1FUc2Qn0&#34; target=&#34;_blank&#34;&gt;Intro: Open Policy Agent Gatekeeper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://youtu.be/n94_FNhuzy4&#34; target=&#34;_blank&#34;&gt;Deep Dive: Open Policy Agent&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;motivations&#34;&gt;Motivations&lt;/h2&gt;

&lt;p&gt;If your organization has been operating Kubernetes, you probably have been looking for ways to control what end-users can do on the cluster and ways to ensure that clusters are in compliance with company policies. These policies may be there to meet governance and legal requirements or to enforce best practices and organizational conventions. With Kubernetes, how do you ensure compliance without sacrificing development agility and operational independence?&lt;/p&gt;

&lt;p&gt;For example, you can enforce policies like:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;All images must be from approved repositories&lt;/li&gt;
&lt;li&gt;All ingress hostnames must be globally unique&lt;/li&gt;
&lt;li&gt;All pods must have resource limits&lt;/li&gt;
&lt;li&gt;All namespaces must have a label that lists a point-of-contact&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Kubernetes allows decoupling policy decisions from the API server by means of &lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/&#34; target=&#34;_blank&#34;&gt;admission controller webhooks&lt;/a&gt; to intercept admission requests before they are persisted as objects in Kubernetes. &lt;a href=&#34;https://github.com/open-policy-agent/gatekeeper&#34; target=&#34;_blank&#34;&gt;Gatekeeper&lt;/a&gt; was created to enable users to customize admission control via configuration, not code and to bring awareness of the cluster’s state, not just the single object under evaluation at admission time. Gatekeeper is a customizable admission webhook for Kubernetes that enforces policies executed by the &lt;a href=&#34;https://www.openpolicyagent.org&#34; target=&#34;_blank&#34;&gt;Open Policy Agent (OPA)&lt;/a&gt;, a policy engine for Cloud Native environments hosted by CNCF.&lt;/p&gt;

&lt;h2 id=&#34;evolution&#34;&gt;Evolution&lt;/h2&gt;

&lt;p&gt;Before we dive into the current state of Gatekeeper, let’s take a look at how the Gatekeeper project has evolved.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Gatekeeper v1.0 - Uses OPA as the admission controller with the kube-mgmt sidecar enforcing configmap-based policies. It provides validating and mutating admission control. Donated by Styra.&lt;/li&gt;
&lt;li&gt;Gatekeeper v2.0 - Uses Kubernetes policy controller as the admission controller with OPA and kube-mgmt sidecars enforcing configmap-based policies. It provides validating and mutating admission control and audit functionality. Donated by Microsoft.

&lt;ul&gt;
&lt;li&gt;Gatekeeper v3.0 - The admission controller is integrated with the &lt;a href=&#34;https://github.com/open-policy-agent/frameworks/tree/master/constraint&#34; target=&#34;_blank&#34;&gt;OPA Constraint Framework&lt;/a&gt; to enforce CRD-based policies and allow declaratively configured policies to be reliably shareable. Built with kubebuilder, it provides validating and, eventually, mutating (to be implemented) admission control and audit functionality. This enables the creation of policy templates for &lt;a href=&#34;https://www.openpolicyagent.org/docs/latest/how-do-i-write-policies/&#34; target=&#34;_blank&#34;&gt;Rego&lt;/a&gt; policies, creation of policies as CRDs, and storage of audit results on policy CRDs. This project is a collaboration between Google, Microsoft, Red Hat, and Styra.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2019-08-06-opa-gatekeeper/v3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;gatekeeper-v3-0-features&#34;&gt;Gatekeeper v3.0 Features&lt;/h2&gt;

&lt;p&gt;Now let’s take a closer look at the current state of Gatekeeper and how you can leverage all the latest features. Consider an organization that wants to ensure all objects in a cluster have departmental information provided as part of the object’s labels. How can you do this with Gatekeeper?&lt;/p&gt;

&lt;h3 id=&#34;validating-admission-control&#34;&gt;Validating Admission Control&lt;/h3&gt;

&lt;p&gt;Once all the Gatekeeper components have been &lt;a href=&#34;https://github.com/open-policy-agent/gatekeeper&#34; target=&#34;_blank&#34;&gt;installed&lt;/a&gt; in your cluster, the API server will trigger the Gatekeeper admission webhook to process the admission request whenever a resource in the cluster is created, updated, or deleted.&lt;/p&gt;

&lt;p&gt;During the validation process, Gatekeeper acts as a bridge between the API server and OPA. The API server will enforce all policies executed by OPA.&lt;/p&gt;

&lt;h3 id=&#34;policies-and-constraints&#34;&gt;Policies and Constraints&lt;/h3&gt;

&lt;p&gt;With the integration of the OPA Constraint Framework, a Constraint is a declaration that its author wants a system to meet a given set of requirements. Each Constraint is written with Rego, a declarative query language used by OPA to enumerate instances of data that violate the expected state of the system. All Constraints are evaluated as a logical AND. If one Constraint is not satisfied, then the whole request is rejected.&lt;/p&gt;

&lt;p&gt;Before defining a Constraint, you need to create a Constraint Template that allows people to declare new Constraints. Each template describes both the Rego logic that enforces the Constraint and the schema for the Constraint, which includes the schema of the CRD and the parameters that can be passed into a Constraint, much like arguments to a function.&lt;/p&gt;

&lt;p&gt;For example, here is a Constraint template CRD that requires certain labels to be present on an arbitrary object.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;templates.gatekeeper.sh/v1beta1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;ConstraintTemplate&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;k8srequiredlabels&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;crd:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;names:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;K8sRequiredLabels&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;listKind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;K8sRequiredLabelsList&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;plural:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;k8srequiredlabels&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;singular:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;k8srequiredlabels&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;validation:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# Schema for the `parameters` field&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;openAPIV3Schema:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;properties:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;labels:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;array&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;items:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;string&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;targets:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;target:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;admission.k8s.gatekeeper.sh&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;rego:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44;font-style:italic&#34;&gt;|
&lt;/span&gt;&lt;span style=&#34;color:#b44;font-style:italic&#34;&gt;        package k8srequiredlabels&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;deny[{&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;msg&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;msg,&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;details&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;{&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;missing_labels&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;missing}}]&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;{&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;provided&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;:=&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;{label&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;|&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;input.review.object.metadata.labels[label]}&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;required&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;:=&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;{label&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;|&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;label&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;:=&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;input.parameters.labels[_]}&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;missing&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;:=&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;required&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;provided&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;count(missing)&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&amp;gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;0&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;msg&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;:=&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;sprintf(&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;you must provide labels: %v&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;[missing])&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Once a Constraint template has been deployed in the cluster, an admin can now create individual Constraint CRDs as defined by the Constraint template. For example, here is a Constraint CRD that requires the label &lt;code&gt;hr&lt;/code&gt; to be present on all namespaces.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;constraints.gatekeeper.sh/v1beta1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;K8sRequiredLabels&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;ns-must-have-hr&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;match:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;kinds:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;apiGroups:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;]&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;kinds:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;Namespace&amp;#34;&lt;/span&gt;]&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;parameters:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;labels:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;hr&amp;#34;&lt;/span&gt;]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Similarly, another Constraint CRD that requires the label &lt;code&gt;finance&lt;/code&gt; to be present on all namespaces can easily be created from the same Constraint template.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;constraints.gatekeeper.sh/v1beta1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;K8sRequiredLabels&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;ns-must-have-finance&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;match:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;kinds:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;apiGroups:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;]&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;kinds:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;Namespace&amp;#34;&lt;/span&gt;]&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;parameters:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;labels:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;finance&amp;#34;&lt;/span&gt;]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;As you can see, with the Constraint framework, we can reliably share Regos via the Constraint templates, define the scope of enforcement with the match field, and provide user-defined parameters to the Constraints to create customized behavior for each Constraint.&lt;/p&gt;

&lt;h3 id=&#34;audit&#34;&gt;Audit&lt;/h3&gt;

&lt;p&gt;The audit functionality enables periodic evaluations of replicated resources against the Constraints enforced in the cluster to detect pre-existing misconfigurations. Gatekeeper stores audit results as &lt;code&gt;violations&lt;/code&gt; listed in the &lt;code&gt;status&lt;/code&gt; field of the relevant Constraint.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;constraints.gatekeeper.sh/v1beta1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;K8sRequiredLabels&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;ns-must-have-hr&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;match:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;kinds:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;apiGroups:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;]&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;kinds:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;Namespace&amp;#34;&lt;/span&gt;]&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;parameters:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;labels:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;hr&amp;#34;&lt;/span&gt;]&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;status:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;auditTimestamp:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;2019-08-06T01:46:13Z&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;byPod:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;enforced:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;true&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;id:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;gatekeeper-controller-manager&lt;span style=&#34;color:#666&#34;&gt;-0&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;violations:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;enforcementAction:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;deny&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Namespace&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;message:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;you must provide labels: {&amp;#34;hr&amp;#34;}&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;default&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;enforcementAction:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;deny&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Namespace&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;message:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;you must provide labels: {&amp;#34;hr&amp;#34;}&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;gatekeeper-system&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;enforcementAction:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;deny&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Namespace&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;message:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;you must provide labels: {&amp;#34;hr&amp;#34;}&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;kube-public&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;enforcementAction:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;deny&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Namespace&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;message:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;you must provide labels: {&amp;#34;hr&amp;#34;}&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;kube-system&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id=&#34;data-replication&#34;&gt;Data Replication&lt;/h3&gt;

&lt;p&gt;Audit requires replication of Kubernetes resources into OPA before they can be evaluated against the enforced Constraints. Data replication is also required by Constraints that need access to objects in the cluster other than the object under evaluation. For example, a Constraint that enforces uniqueness of ingress hostname must have access to all other ingresses in the cluster.&lt;/p&gt;

&lt;p&gt;To configure Kubernetes data to be replicated, create a sync config resource with the resources to be replicated into OPA. For example, the below configuration replicates all namespace and pod resources to OPA.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;config.gatekeeper.sh/v1alpha1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Config&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;config&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;namespace:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;gatekeeper-system&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;sync:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;syncOnly:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;group:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;version:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;v1&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;Namespace&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;group:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;version:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;v1&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;Pod&amp;#34;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&#34;planned-for-future&#34;&gt;Planned for Future&lt;/h2&gt;

&lt;p&gt;The community behind the Gatekeeper project will be focusing on providing mutating admission control to support mutation scenarios (for example: annotate objects automatically with departmental information when creating a new resource), support external data to inject context external to the cluster into the admission decisions, support dry run to see impact of a policy on existing resources in the cluster before enforcing it, and more audit functionalities.&lt;/p&gt;

&lt;p&gt;If you are interested in learning more about the project, check out the &lt;a href=&#34;https://github.com/open-policy-agent/gatekeeper&#34; target=&#34;_blank&#34;&gt;Gatekeeper&lt;/a&gt; repo. If you are interested in helping define the direction of Gatekeeper, join the &lt;a href=&#34;https://openpolicyagent.slack.com/messages/CDTN970AX&#34; target=&#34;_blank&#34;&gt;#kubernetes-policy&lt;/a&gt; channel on OPA Slack, and join our &lt;a href=&#34;https://docs.google.com/document/d/1A1-Q-1OMw3QODs1wT6eqfLTagcGmgzAJAjJihiO3T48/edit&#34; target=&#34;_blank&#34;&gt;weekly meetings&lt;/a&gt; to discuss development, issues, use cases, etc.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Get started with Kubernetes (using Python)</title>
      <link>https://kubernetes.io/blog/2019/07/23/get-started-with-kubernetes-using-python/</link>
      <pubDate>Tue, 23 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/07/23/get-started-with-kubernetes-using-python/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Jason Haley (Independent Consultant)&lt;/p&gt;

&lt;p&gt;So, you know you want to run your application in Kubernetes but don’t know where to start. Or maybe you’re getting started but still don’t know what you don’t know. In this blog you’ll walk through how to containerize an application and get it running in Kubernetes.&lt;/p&gt;

&lt;p&gt;This walk-through assumes you are a developer or at least comfortable with the command line (preferably bash shell).&lt;/p&gt;

&lt;h2 id=&#34;what-we-ll-do&#34;&gt;What we’ll do&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Get the code and run the application locally&lt;/li&gt;
&lt;li&gt;Create an image and run the application in Docker&lt;/li&gt;
&lt;li&gt;Create a deployment and run the application in Kubernetes&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;A Kubernetes service - I&amp;rsquo;m using &lt;a href=&#34;https://www.docker.com/products/kubernetes&#34; target=&#34;_blank&#34;&gt;Docker Desktop with Kubernetes&lt;/a&gt; in this walkthrough, but you can use one of the others. See &lt;a href=&#34;https://kubernetes.io/docs/setup/&#34; target=&#34;_blank&#34;&gt;Getting Started&lt;/a&gt; for a full listing.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.python.org/&#34; target=&#34;_blank&#34;&gt;Python 3.7&lt;/a&gt; installed&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://git-scm.com/downloads&#34; target=&#34;_blank&#34;&gt;Git&lt;/a&gt; installed&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;containerizing-an-application&#34;&gt;Containerizing an application&lt;/h2&gt;

&lt;p&gt;In this section you’ll take some source code, verify it runs locally, and then create a Docker image of the application. The sample application used is a very simple Flask web application; if you want to test it locally, you’ll need Python installed. Otherwise, you can skip to the &amp;ldquo;Create a Dockerfile&amp;rdquo; section.&lt;/p&gt;

&lt;h3 id=&#34;get-the-application-code&#34;&gt;Get the application code&lt;/h3&gt;

&lt;p&gt;Use git to clone the repository to your local machine:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/JasonHaley/hello-python.git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Change to the app directory:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd hello-python/app
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are only two files in this directory. If you look at the main.py file, you’ll see the application prints out a hello message. You can learn more about Flask on the &lt;a href=&#34;http://flask.pocoo.org/&#34; target=&#34;_blank&#34;&gt;Flask website&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from flask import Flask
app = Flask(__name__)

@app.route(&amp;quot;/&amp;quot;)
def hello():
    return &amp;quot;Hello from Python!&amp;quot;

if __name__ == &amp;quot;__main__&amp;quot;:
    app.run(host=&#39;0.0.0.0&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The requirements.txt file contains the list of packages needed by the main.py and will be used by &lt;a href=&#34;https://pip.pypa.io/en/stable/&#34; target=&#34;_blank&#34;&gt;pip&lt;/a&gt; to install the Flask library.&lt;/p&gt;

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;Note:&lt;/strong&gt; When you start writing more advanced Python, you&amp;rsquo;ll find it&amp;rsquo;s not always recommended to use &lt;code&gt;pip install&lt;/code&gt; and may want to use &lt;code&gt;virtualenv&lt;/code&gt; (or &lt;code&gt;pyenv&lt;/code&gt;) to install your dependencies in a virtual environment.&lt;/div&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;run-locally&#34;&gt;Run locally&lt;/h3&gt;

&lt;p&gt;Manually run the installer and application using the following commands:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt
python main.py
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will start a development web server hosting your application, which you will be able to see by navigating to &lt;a href=&#34;http://localhost:5000&#34; target=&#34;_blank&#34;&gt;http://localhost:5000&lt;/a&gt;. Because port 5000 is the default port for the development server, we didn’t need to specify it.&lt;/p&gt;

&lt;h3 id=&#34;create-a-dockerfile&#34;&gt;Create a Dockerfile&lt;/h3&gt;

&lt;p&gt;Now that you have verified the source code works, the first step in containerizing the application is to create a Dockerfile.&lt;/p&gt;

&lt;p&gt;In the hello-python/app directory, create a file named Dockerfile with the following contents and save it:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;FROM python:3.7

RUN mkdir /app
WORKDIR /app
ADD . /app/
RUN pip install -r requirements.txt

EXPOSE 5000
CMD [&amp;quot;python&amp;quot;, &amp;quot;/app/main.py&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This file is a set of instructions Docker will use to build the image. For this simple application, Docker is going to:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Get the official &lt;a href=&#34;https://hub.docker.com/_/python/&#34; target=&#34;_blank&#34;&gt;Python Base Image&lt;/a&gt; for version 3.7 from Docker Hub.&lt;/li&gt;
&lt;li&gt;In the image, create a directory named app.&lt;/li&gt;
&lt;li&gt;Set the working directory to that new app directory.&lt;/li&gt;
&lt;li&gt;Copy the local directory’s contents to that new folder into the image.&lt;/li&gt;
&lt;li&gt;Run the pip installer (just like we did earlier) to pull the requirements into the image.&lt;/li&gt;
&lt;li&gt;Inform Docker the container listens on port 5000.&lt;/li&gt;
&lt;li&gt;Configure the starting command to use when the container starts.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;create-an-image&#34;&gt;Create an image&lt;/h3&gt;

&lt;p&gt;At your command line or shell, in the hello-python/app directory, build the image with the following command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker build -f Dockerfile -t hello-python:latest .
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;Note:&lt;/strong&gt; I&amp;rsquo;m using the :latest tag in this example, if you are not familiar with what it is you may want to read &lt;a href=&#34;https://container-solutions.com/docker-latest-confusion/&#34; target=&#34;_blank&#34;&gt;Docker: The latest Confusion&lt;/a&gt;.&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;This will perform those seven steps listed above and create the image. To verify the image was created, run the following command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker image ls
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/get-started-with-kubernetes-using-python/docker-image-ls.png&#34; alt=&#34;Docker image listing&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The application is now containerized, which means it can now run in Docker and Kubernetes!&lt;/p&gt;

&lt;h2 id=&#34;running-in-docker&#34;&gt;Running in Docker&lt;/h2&gt;

&lt;p&gt;Before jumping into Kubernetes, let’s verify it works in Docker.
Run the following command to have Docker run the application in a container and map it to port 5001:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run -p 5001:5000 hello-python
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now navigate to &lt;a href=&#34;http://localhost:5001&#34; target=&#34;_blank&#34;&gt;http://localhost:5001&lt;/a&gt;, and you should see the “Hello form Python!” message.&lt;/p&gt;

&lt;h3 id=&#34;more-info&#34;&gt;More info&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.docker.com/get-started/&#34; target=&#34;_blank&#34;&gt;Get started with Docker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.docker.com/develop/develop-images/dockerfile_best-practices/&#34; target=&#34;_blank&#34;&gt;Best practices for writing Dockerfiles&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.docker.com/sites/default/files/Docker_CheatSheet_08.09.2016_0.pdf&#34; target=&#34;_blank&#34;&gt;Docker Cheat Sheet&lt;/a&gt; (pdf)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;running-in-kubernetes&#34;&gt;Running in Kubernetes&lt;/h2&gt;

&lt;p&gt;You are finally ready to get the application running in Kubernetes. Because you have a web application, you will create a service and a deployment.&lt;/p&gt;

&lt;p&gt;First verify your kubectl is configured. At the command line, type the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl version
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you don’t see a reply with a Client and Server version, you’ll need to &lt;a href=&#34;https://kubernetes.io/docs/tasks/tools/install-kubectl/&#34; target=&#34;_blank&#34;&gt;install&lt;/a&gt; and configure it.&lt;/p&gt;

&lt;p&gt;If you are running on Windows or Mac, make sure it is using the Docker for Desktop context by running the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config use-context docker-for-desktop
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now you are working with Kubernetes! You can see the node by typing:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl get nodes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now let’s have it run the application. Create a file named deployment.yaml and add the following contents to it and then save it:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;v1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Service&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;hello-python-service&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;selector:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;app:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;hello-python&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;ports:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;protocol:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;TCP&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;port:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;6000&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;targetPort:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;5000&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;LoadBalancer&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;---&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;apps/v1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Deployment&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;hello-python&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;selector:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;matchLabels:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;app:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;hello-python&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;replicas:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;4&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;template:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;labels:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;app:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;hello-python&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;containers:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;hello-python&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;image:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;hello-python:latest&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;imagePullPolicy:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Never&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;ports:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;containerPort:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;5000&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This YAML file is the instructions to Kubernetes for what you want running. It is telling Kubernetes the following:
* You want a load-balanced service exposing port 6000
* You want four instances of the hello-python container running&lt;/p&gt;

&lt;p&gt;Use kubectl to send the YAML file to Kubernetes by running the following command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl apply -f deployment.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can see the pods are running if you execute the following command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl get pods
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/get-started-with-kubernetes-using-python/kubectl-get-pods.png&#34; alt=&#34;Pod listing&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Now navigate to &lt;a href=&#34;http://localhost:6000&#34; target=&#34;_blank&#34;&gt;http://localhost:6000&lt;/a&gt;, and you should see the “Hello form Python!” message.&lt;/p&gt;

&lt;p&gt;That’s it! The application is now running in Kubernetes!&lt;/p&gt;

&lt;h3 id=&#34;more-info-1&#34;&gt;More Info&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://azure.microsoft.com/en-us/topic/kubernetes/&#34; target=&#34;_blank&#34;&gt;Learn Kubernetes Basics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/kubectl/cheatsheet/&#34; target=&#34;_blank&#34;&gt;kubectl Cheat Sheet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/kubectl/docker-cli-to-kubectl/&#34; target=&#34;_blank&#34;&gt;kubectl for Docker Users&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;

&lt;p&gt;In this walk-through, we containerized an application, and got it running in Docker and in Kubernetes. This simple application only scratches the surface of what’s possible (and what you’ll need to learn).&lt;/p&gt;

&lt;h3 id=&#34;next-steps&#34;&gt;Next steps&lt;/h3&gt;

&lt;p&gt;If you are just getting started and this walk-through was useful to you, then the following resources should be good next steps for you to further expand your Kubernetes knowledge:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=1xo-0gCVhTU&#34; target=&#34;_blank&#34;&gt;Introduction to Microservices, Docker, and Kubernetes&lt;/a&gt; - 55-minute video by James Quigley

&lt;ul&gt;
&lt;li&gt;This is a great place to start because it provides more information than I could here.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/PacktPublishing/Containerize-your-Apps-with-Docker-and-Kubernetes&#34; target=&#34;_blank&#34;&gt;Containerize your Apps with Docker and Kubernetes&lt;/a&gt; - free e-book by Dr Gabriel N Schenker

&lt;ul&gt;
&lt;li&gt;This is my favorite book on Docker and Kubernetes.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://aka.ms/LearnKubernetes&#34; target=&#34;_blank&#34;&gt;Kubernetes Learning Path: 50 days from zero to hero with Kubernetes&lt;/a&gt; - on Microsoft’s site

&lt;ul&gt;
&lt;li&gt;This is a 10-page pdf that has tons of links to videos (with Brendan Burns), documentation sites, and a really good workshop for Azure Kubernetes Service.
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;how-to-enable-kubernetes-in-docker-desktop&#34;&gt;How to enable Kubernetes in Docker Desktop&lt;/h2&gt;

&lt;p&gt;Once you have Docker Desktop installed, open the Settings:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/get-started-with-kubernetes-using-python/docker-settings-menu.png&#34; alt=&#34;Docker settings menu&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Select the &lt;strong&gt;Kubernetes&lt;/strong&gt; menu item on the left and verify that the &lt;strong&gt;Enable Kubernetes&lt;/strong&gt; is checked. If it isn’t, &lt;strong&gt;check it&lt;/strong&gt; and click the &lt;strong&gt;Apply&lt;/strong&gt; button at the bottom right:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/get-started-with-kubernetes-using-python/kubernetes-tab.png&#34; alt=&#34;Kubernetes tab&#34; /&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Deprecated APIs Removed In 1.16: Here’s What You Need To Know</title>
      <link>https://kubernetes.io/blog/2019/07/18/api-deprecations-in-1-16/</link>
      <pubDate>Thu, 18 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/07/18/api-deprecations-in-1-16/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Vallery Lancey (Lyft)&lt;/p&gt;

&lt;p&gt;As the Kubernetes API evolves, APIs are periodically reorganized or upgraded.
When APIs evolve, the old API is deprecated and eventually removed.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;v1.16&lt;/strong&gt; release will stop serving the following deprecated API versions in favor of newer and more stable API versions:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;NetworkPolicy in the &lt;strong&gt;extensions/v1beta1&lt;/strong&gt; API version is no longer served

&lt;ul&gt;
&lt;li&gt;Migrate to use the &lt;strong&gt;networking.k8s.io/v1&lt;/strong&gt; API version, available since v1.8.
Existing persisted data can be retrieved/updated via the new version.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;PodSecurityPolicy in the &lt;strong&gt;extensions/v1beta1&lt;/strong&gt; API version

&lt;ul&gt;
&lt;li&gt;Migrate to use the &lt;strong&gt;policy/v1beta1&lt;/strong&gt; API, available since v1.10.
Existing persisted data can be retrieved/updated via the new version.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;DaemonSet in the &lt;strong&gt;extensions/v1beta1&lt;/strong&gt; and &lt;strong&gt;apps/v1beta2&lt;/strong&gt; API versions is no longer served

&lt;ul&gt;
&lt;li&gt;Migrate to use the &lt;strong&gt;apps/v1&lt;/strong&gt; API version, available since v1.9.
Existing persisted data can be retrieved/updated via the new version.&lt;/li&gt;
&lt;li&gt;Notable changes:

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;spec.templateGeneration&lt;/code&gt; is removed&lt;/li&gt;
&lt;li&gt;&lt;code&gt;spec.selector&lt;/code&gt; is now required and immutable after creation; use the existing template labels as the selector for seamless upgrades&lt;/li&gt;
&lt;li&gt;&lt;code&gt;spec.updateStrategy.type&lt;/code&gt; now defaults to &lt;code&gt;RollingUpdate&lt;/code&gt; (the default in &lt;code&gt;extensions/v1beta1&lt;/code&gt; was &lt;code&gt;OnDelete&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Deployment in the &lt;strong&gt;extensions/v1beta1&lt;/strong&gt;, &lt;strong&gt;apps/v1beta1&lt;/strong&gt;, and &lt;strong&gt;apps/v1beta2&lt;/strong&gt; API versions is no longer served

&lt;ul&gt;
&lt;li&gt;Migrate to use the &lt;strong&gt;apps/v1&lt;/strong&gt; API version, available since v1.9.
Existing persisted data can be retrieved/updated via the new version.&lt;/li&gt;
&lt;li&gt;Notable changes:

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;spec.rollbackTo&lt;/code&gt; is removed&lt;/li&gt;
&lt;li&gt;&lt;code&gt;spec.selector&lt;/code&gt; is now required and immutable after creation; use the existing template labels as the selector for seamless upgrades&lt;/li&gt;
&lt;li&gt;&lt;code&gt;spec.progressDeadlineSeconds&lt;/code&gt; now defaults to &lt;code&gt;600&lt;/code&gt; seconds (the default in &lt;code&gt;extensions/v1beta1&lt;/code&gt; was no deadline)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;spec.revisionHistoryLimit&lt;/code&gt; now defaults to &lt;code&gt;10&lt;/code&gt; (the default in &lt;code&gt;apps/v1beta1&lt;/code&gt; was &lt;code&gt;2&lt;/code&gt;, the default in &lt;code&gt;extensions/v1beta1&lt;/code&gt; was to retain all)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;maxSurge&lt;/code&gt; and &lt;code&gt;maxUnavailable&lt;/code&gt; now default to &lt;code&gt;25%&lt;/code&gt; (the default in &lt;code&gt;extensions/v1beta1&lt;/code&gt; was &lt;code&gt;1&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;StatefulSet in the &lt;strong&gt;apps/v1beta1&lt;/strong&gt; and &lt;strong&gt;apps/v1beta2&lt;/strong&gt; API versions is no longer served

&lt;ul&gt;
&lt;li&gt;Migrate to use the &lt;strong&gt;apps/v1&lt;/strong&gt; API version, available since v1.9.
Existing persisted data can be retrieved/updated via the new version.&lt;/li&gt;
&lt;li&gt;Notable changes:

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;spec.selector&lt;/code&gt; is now required and immutable after creation; use the existing template labels as the selector for seamless upgrades&lt;/li&gt;
&lt;li&gt;&lt;code&gt;spec.updateStrategy.type&lt;/code&gt; now defaults to &lt;code&gt;RollingUpdate&lt;/code&gt; (the default in &lt;code&gt;apps/v1beta1&lt;/code&gt; was &lt;code&gt;OnDelete&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;ReplicaSet in the &lt;strong&gt;extensions/v1beta1&lt;/strong&gt;, &lt;strong&gt;apps/v1beta1&lt;/strong&gt;, and &lt;strong&gt;apps/v1beta2&lt;/strong&gt; API versions is no longer served

&lt;ul&gt;
&lt;li&gt;Migrate to use the &lt;strong&gt;apps/v1&lt;/strong&gt; API version, available since v1.9.
Existing persisted data can be retrieved/updated via the new version.&lt;/li&gt;
&lt;li&gt;Notable changes:

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;spec.selector&lt;/code&gt; is now required and immutable after creation; use the existing template labels as the selector for seamless upgrades&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The &lt;strong&gt;v1.22&lt;/strong&gt; release will stop serving the following deprecated API versions in favor of newer and more stable API versions:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Ingress in the &lt;strong&gt;extensions/v1beta1&lt;/strong&gt; API version will no longer be served

&lt;ul&gt;
&lt;li&gt;Migrate to use the &lt;strong&gt;networking.k8s.io/v1beta1&lt;/strong&gt; API version, available since v1.14.
Existing persisted data can be retrieved/updated via the new version.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;what-to-do&#34;&gt;What To Do&lt;/h1&gt;

&lt;p&gt;Kubernetes 1.16 is due to be released in September 2019, so be sure to audit
your configuration and integrations now!&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Change YAML files to reference the newer APIs&lt;/li&gt;
&lt;li&gt;Update custom integrations and controllers to call the newer APIs&lt;/li&gt;
&lt;li&gt;Update third party tools (ingress controllers, continuous delivery systems)
to call the newer APIs&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Migrating to the new Ingress API will only require changing the API path - the
API fields remain the same. However, migrating other resources (EG Deployments)
will require some updates based on changed fields. You can use the
&lt;code&gt;kubectl convert&lt;/code&gt; command to automatically convert an existing object:
&lt;code&gt;kubectl convert -f &amp;lt;file&amp;gt; --output-version &amp;lt;group&amp;gt;/&amp;lt;version&amp;gt;&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;For example, to convert
an older Deployment to apps/v1, you can run:
&lt;code&gt;kubectl convert -f ./my-deployment.yaml --output-version apps/v1&lt;/code&gt;
Note that this may use non-ideal default values. To learn more about a specific
resource, check the Kubernetes &lt;a href=&#34;https://kubernetes.io/docs/reference/#api-reference&#34; target=&#34;_blank&#34;&gt;api reference&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You can test your clusters by starting an apiserver with the above resources
disabled, to simulate the upcoming removal. Add the following flag to the
apiserver startup arguments:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;--runtime-config=apps/v1beta1=false,apps/v1beta2=false,extensions/v1beta1/daemonsets=false,extensions/v1beta1/deployments=false,extensions/v1beta1/replicasets=false,extensions/v1beta1/networkpolicies=false,extensions/v1beta1/podsecuritypolicies=false&lt;/code&gt;&lt;/p&gt;

&lt;h1 id=&#34;want-to-know-more&#34;&gt;Want To Know More?&lt;/h1&gt;

&lt;p&gt;Deprecations are announced in the Kubernetes release notes. You can see these
announcements in
&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.14.md#deprecations&#34; target=&#34;_blank&#34;&gt;1.14&lt;/a&gt;
and &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.15.md#deprecations-and-removals&#34; target=&#34;_blank&#34;&gt;1.15&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You can read more &lt;a href=&#34;https://kubernetes.io/docs/reference/using-api/deprecation-policy/#deprecating-parts-of-the-api&#34; target=&#34;_blank&#34;&gt;in our deprecation policy document&lt;/a&gt;
about the deprecation policies for Kubernetes APIs, and other Kubernetes components.
Deprecation policies vary by component (for example, the primary APIs vs.
admin CLIs) and by maturity (alpha, beta, or GA).&lt;/p&gt;

&lt;p&gt;These details were also &lt;a href=&#34;https://groups.google.com/forum/#!topic/kubernetes-dev/je0rjyfTVyc&#34; target=&#34;_blank&#34;&gt;previously announced&lt;/a&gt;
on the kubernetes-dev mailing list, along with the releases of Kubernetes 1.14
and 1.15. From Jordan Liggitt:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;In case you missed it in the 1.15.0 release notes, the timelines for deprecated resources in the extensions/v1beta1, apps/v1beta1, and apps/v1beta2 API groups to no longer be served by default have been updated:

* NetworkPolicy resources will no longer be served from extensions/v1beta1 by default in v1.16. Migrate to the networking.k8s.io/v1 API, available since v1.8. Existing persisted data can be retrieved/updated via the networking.k8s.io/v1 API.
* PodSecurityPolicy resources will no longer be served from extensions/v1beta1 by default in v1.16. Migrate to the policy/v1beta1 API, available since v1.10. Existing persisted data can be retrieved/updated via the policy/v1beta1 API.
* DaemonSet, Deployment, StatefulSet, and ReplicaSet resources will no longer be served from extensions/v1beta1, apps/v1beta1, or apps/v1beta2 by default in v1.16. Migrate to the apps/v1 API, available since v1.9. Existing persisted data can be retrieved/updated via the apps/v1 API.

To start a v1.15.0 API server with these resources disabled to flush out dependencies on these deprecated APIs, and ensure your application/manifests will work properly against the v1.16 release, use the following --runtime-config argument:

--runtime-config=apps/v1beta1=false,apps/v1beta2=false,extensions/v1beta1/daemonsets=false,extensions/v1beta1/deployments=false,extensions/v1beta1/replicasets=false,extensions/v1beta1/networkpolicies=false,extensions/v1beta1/podsecuritypolicies=false
&lt;/code&gt;&lt;/pre&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Recap of Kubernetes Contributor Summit Barcelona 2019</title>
      <link>https://kubernetes.io/blog/2019/06/25/recap-of-kubernetes-contributor-summit-barcelona-2019/</link>
      <pubDate>Tue, 25 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/06/25/recap-of-kubernetes-contributor-summit-barcelona-2019/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Jonas Rosland (VMware)&lt;/p&gt;

&lt;p&gt;First of all, &lt;strong&gt;THANK YOU&lt;/strong&gt; to everyone who made the Kubernetes Contributor Summit in Barcelona possible. We had an amazing team of volunteers tasked with planning and executing the event, and it was so much fun meeting and talking to all new and current contributors during the main event and the pre-event celebration.&lt;/p&gt;

&lt;p&gt;Contributor Summit in Barcelona kicked off KubeCon + CloudNativeCon in a big way as it was the &lt;strong&gt;largest contributor summit&lt;/strong&gt; to date with 331 people signed up, and only 9 didn&amp;rsquo;t pick up their badges!&lt;/p&gt;

&lt;h2 id=&#34;contributor-celebration&#34;&gt;Contributor Celebration&lt;/h2&gt;

&lt;p&gt;Sunday evening before the main event we held a &lt;strong&gt;Contributor Celebration&lt;/strong&gt;, which was very well attended. We hope that all new and current contributors felt welcome and enjoyed the food, the music, and the company.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://live.staticflickr.com/65535/46981485515_561bb324b2_z.jpg&#34; alt=&#34;contributor-celebration2&#34; /&gt;
&lt;img src=&#34;https://live.staticflickr.com/65535/46981484655_8122564557_z.jpg&#34; alt=&#34;contributor-celebration&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;new-contributor-workshops&#34;&gt;New Contributor Workshops&lt;/h2&gt;

&lt;p&gt;We had over &lt;strong&gt;130 people registered&lt;/strong&gt; for the New Contributor Workshops. This year the workshops were divided into &lt;em&gt;101-level content&lt;/em&gt; for people who were not familiar with contributing to an open source project, and &lt;em&gt;201-level content&lt;/em&gt; for those who were.&lt;/p&gt;

&lt;p&gt;The workshops contained overviews of what SIGs are, deep-dives into the codebase, test builds of the Kubernetes project, and real contributions.&lt;/p&gt;

&lt;p&gt;Did you miss something during the workshops? We now have them &lt;a href=&#34;https://www.youtube.com/playlist?list=PL69nYSiGNLP2WTJ6P8sQenhf0RY-JqF5L&#34; target=&#34;_blank&#34;&gt;published on YouTube&lt;/a&gt;, with added closed captioning!&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://live.staticflickr.com/65535/47897543541_a57d3b9ac9_z.jpg&#34; alt=&#34;img&#34; /&gt;
&lt;img src=&#34;https://live.staticflickr.com/65535/47108247174_5d60b60846_z.jpg&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;sig-face-to-face&#34;&gt;SIG Face-to-Face&lt;/h2&gt;

&lt;p&gt;We also tried a new thing for Barcelona, the SIG Face-to-Face meetings. We had &lt;strong&gt;over 170 people&lt;/strong&gt; registered to attend the 11 SIG and one subproject meetings throughout the day, going over what they&amp;rsquo;re working on and what they want to do in the near future.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://live.staticflickr.com/65535/47108254124_248a80ef1a_z.jpg&#34; alt=&#34;img&#34; /&gt;
&lt;img src=&#34;https://live.staticflickr.com/65535/47108250434_88afdf930a_z.jpg&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;sig-meet-and-greet&#34;&gt;SIG Meet and Greet&lt;/h2&gt;

&lt;p&gt;At the end of the summit, both new and current contributors had a chance to sit down with SIG chairs and members. The goal of this was to make sure that contributors got to know even more individuals in the project, hear what some of the SIGs actually do, and sign up to be a part of them and learn more.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://live.staticflickr.com/65535/47108248464_97eb2bbbb6_k.jpg&#34; alt=&#34;img&#34; /&gt;
&lt;img src=&#34;https://live.staticflickr.com/65535/47845452032_a3d478beb9_k.jpg&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;join-us&#34;&gt;Join us!&lt;/h2&gt;

&lt;p&gt;Interested in attending the Contributor Summit in San Diego? &lt;a href=&#34;https://events.linuxfoundation.org/events/contributor-summit-north-america-2019/&#34; target=&#34;_blank&#34;&gt;You can get more information on our event page&lt;/a&gt;, sign up and we will notify you when registration opens.&lt;/p&gt;

&lt;h2 id=&#34;thanks&#34;&gt;Thanks!&lt;/h2&gt;

&lt;p&gt;Again, thank you to everyone for making this an amazing event, and we&amp;rsquo;re looking forward to seeing you next time!&lt;/p&gt;

&lt;p&gt;To our Barcelona crew, you ROCK! 🥁&lt;/p&gt;

&lt;p&gt;Paris Pittman, Bob Killen, Guinevere Saenger, Tim Pepper, Deb Giles, Ihor Dvoretskyi, Jorge Castro, Noah Kantrowitz, Dawn Foster, Ruben Orduz, Josh Berkus, Kiran Mova, Bart Smykla, Rostislav Georgiev, Jeffrey Sica, Rael Garcia, Silvia Moura Pina, Arnaud Meukam, Jason DeTiberius, Andy Goldstein, Suzanne Ambiel, Jonas Rosland&lt;/p&gt;

&lt;p&gt;You can see many more pictures from the event &lt;a href=&#34;https://www.flickr.com/photos/143247548@N03/sets/72157680323974628&#34; target=&#34;_blank&#34;&gt;over on CNCF&amp;rsquo;s Flickr&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Automated High Availability in kubeadm v1.15: Batteries Included But Swappable</title>
      <link>https://kubernetes.io/blog/2019/06/24/automated-high-availability-in-kubeadm-v1.15-batteries-included-but-swappable/</link>
      <pubDate>Mon, 24 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/06/24/automated-high-availability-in-kubeadm-v1.15-batteries-included-but-swappable/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Lucas Käldström, &lt;a href=&#34;https://github.com/luxas&#34; target=&#34;_blank&#34;&gt;@luxas&lt;/a&gt;, SIG Cluster Lifecycle co-chair &amp;amp; kubeadm subproject owner, Weaveworks&lt;/li&gt;
&lt;li&gt;Fabrizio Pandini, &lt;a href=&#34;https://github.com/fabriziopandini&#34; target=&#34;_blank&#34;&gt;@fabriziopandini&lt;/a&gt;, kubeadm subproject owner, Independent&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/&#34; target=&#34;_blank&#34;&gt;kubeadm&lt;/a&gt; is a tool that enables Kubernetes administrators
to quickly and easily bootstrap minimum viable clusters that are fully compliant with
&lt;a href=&#34;https://github.com/cncf/k8s-conformance/blob/master/terms-conditions/Certified_Kubernetes_Terms.md&#34; target=&#34;_blank&#34;&gt;Certified Kubernetes&lt;/a&gt; guidelines.
It’s been under active development by &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-cluster-lifecycle&#34; target=&#34;_blank&#34;&gt;SIG Cluster Lifecycle&lt;/a&gt;
since 2016 and graduated it from beta to
&lt;a href=&#34;https://kubernetes.io/blog/2018/12/04/production-ready-kubernetes-cluster-creation-with-kubeadm/&#34; target=&#34;_blank&#34;&gt;generally available (GA) at the end of 2018&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;After this important milestone, the kubeadm team is now focused on the stability of the core feature set and working on
maturing existing features.&lt;/p&gt;

&lt;p&gt;With this post, we are introducing the improvements made in the v1.15 release of kubeadm.&lt;/p&gt;

&lt;h2 id=&#34;the-scope-of-kubeadm&#34;&gt;The scope of kubeadm&lt;/h2&gt;

&lt;p&gt;kubeadm is focused on performing the actions necessary to get a minimum viable, secure cluster up and running in a
user-friendly way. kubeadm&amp;rsquo;s scope is limited to the local machine’s filesystem and the Kubernetes API, and it is
intended to be a &lt;em&gt;composable building block for higher-level tools&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The core of the kubeadm interface is quite simple: new control plane nodes are created by you running
&lt;strong&gt;&lt;code&gt;kubeadm init&lt;/code&gt;&lt;/strong&gt;, worker nodes are joined to the control plane by you running
&lt;strong&gt;&lt;code&gt;kubeadm join&lt;/code&gt;&lt;/strong&gt;. Also included are common utilities for managing already bootstrapped
clusters, such as control plane upgrades, token and certificate renewal.&lt;/p&gt;

&lt;p&gt;To keep kubeadm lean, focused, and vendor/infrastructure agnostic, the following tasks are out of scope:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Infrastructure provisioning&lt;/li&gt;
&lt;li&gt;Third-party networking&lt;/li&gt;
&lt;li&gt;Non-critical add-ons, e.g. monitoring, logging, and visualization&lt;/li&gt;
&lt;li&gt;Specific cloud provider integrations&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Those tasks are addressed by other SIG Cluster Lifecycle projects, such as the
&lt;a href=&#34;https://github.com/kubernetes-sigs/cluster-api&#34; target=&#34;_blank&#34;&gt;Cluster API&lt;/a&gt; for infrastructure provisioning and management.&lt;/p&gt;

&lt;p&gt;Instead, kubeadm covers only the common denominator in every Kubernetes cluster: the
&lt;a href=&#34;https://kubernetes.io/docs/concepts/#kubernetes-control-plane&#34; target=&#34;_blank&#34;&gt;control plane&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2019-06-24-kubeadm-ha-v115/overview.png&#34; alt=&#34;Cluster Lifecycle Layers&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;what-s-new-in-kubeadm-v1-15&#34;&gt;What’s new in kubeadm v1.15?&lt;/h2&gt;

&lt;h3 id=&#34;high-availability-to-beta&#34;&gt;High Availability to Beta&lt;/h3&gt;

&lt;p&gt;We are delighted to announce that automated support for High Availability clusters is graduating to &lt;strong&gt;Beta&lt;/strong&gt; in kubeadm v1.15. Let’s give a great shout out to all the contributors that helped in this effort and to the early adopter users for the great feedback received so far!&lt;/p&gt;

&lt;p&gt;But how does automated High Availability work in kubeadm?&lt;/p&gt;

&lt;p&gt;The great news is that you can use the familiar &lt;code&gt;kubeadm init&lt;/code&gt; or &lt;code&gt;kubeadm join&lt;/code&gt; workflow for creating high availability cluster as well, with the only difference that you have to pass the &lt;code&gt;--control-plane&lt;/code&gt; flag to &lt;code&gt;kubeadm join&lt;/code&gt; when adding more control plane nodes.&lt;/p&gt;

&lt;p&gt;A 3-minute screencast of this feature is here:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://asciinema.org/a/252343&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://asciinema.org/a/252343.svg&#34; alt=&#34;asciicast&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In a nutshell:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Set up a Load Balancer.&lt;/strong&gt; You need an &lt;em&gt;external load balancer&lt;/em&gt;; providing this however, is out of scope of kubeadm.

&lt;ul&gt;
&lt;li&gt;The community will provide a set of reference implementations for this task though&lt;/li&gt;
&lt;li&gt;HAproxy, Envoy, or a similar Load Balancer from a cloud provider work well&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Run kubeadm init&lt;/strong&gt; on the first control plane node, with small modifications:

&lt;ul&gt;
&lt;li&gt;Create a &lt;a href=&#34;https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/#config-file&#34; target=&#34;_blank&#34;&gt;kubeadm Config File&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;In the config file, set the &lt;code&gt;controlPlaneEndpoint&lt;/code&gt; field to where your Load Balancer can be reached at.&lt;/li&gt;
&lt;li&gt;Run init, with the &lt;code&gt;--upload-certs&lt;/code&gt; flag like this: &lt;code&gt;sudo kubeadm init --config=kubeadm-config.yaml --upload-certs&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Run kubeadm join &amp;ndash;control-plane&lt;/strong&gt; at any time when you want to expand the set of control plane nodes&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Both control-plane- and normal nodes can be joined in any order, at any time&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The command to run will be given by &lt;code&gt;kubeadm init&lt;/code&gt; above, and is of the form:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubeadm join [LB endpoint] \
--token ... \                                                                                               
--discovery-token-ca-cert-hash sha256:... \                                                             
--control-plane --certificate-key ...  
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For those interested in the details, there are many things that make this functionality possible. Most notably:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Automated certificate transfer&lt;/strong&gt;. kubeadm implements an automatic certificate copy feature to automate the distribution of all the certificate authorities/keys that must be shared across all the control-planes nodes in order to get your cluster to work. This feature can be activated by passing  &lt;code&gt;--upload-certs&lt;/code&gt; to  &lt;code&gt;kubeadm init&lt;/code&gt;; see &lt;a href=&#34;https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/&#34; target=&#34;_blank&#34;&gt;configure and deploy an HA control plane&lt;/a&gt; for more details. This is an explicit opt-in feature, you can also distribute the certificates manually in your preferred way. &lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Dynamically-growing etcd cluster&lt;/strong&gt;. When you&amp;rsquo;re not providing an external etcd cluster, kubeadm automatically adds a new etcd member, running as a static pod. All the etcd members are joined in a “stacked” etcd cluster that grows together with your high availability control-plane &lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Concurrent joining&lt;/strong&gt;. Similarly to what already implemented for worker nodes, you join control-plane nodes whenever, in any order, or even in parallel. &lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Upgradable&lt;/strong&gt;. The kubeadm upgrade workflow was improved in order to properly handle the HA scenario, and, after starting the upgrade with &lt;code&gt;kubeadm upgrade apply&lt;/code&gt; as usual, users can now complete the upgrade process by using &lt;code&gt;kubeadm upgrade node&lt;/code&gt; both on the remaining control-plane nodes and worker nodes&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Finally, it is also worthy to notice that an entirely new test suite has been created specifically for ensuring High Availability in kubeadm will stay stable over time.&lt;/p&gt;

&lt;h3 id=&#34;certificate-management&#34;&gt;Certificate Management&lt;/h3&gt;

&lt;p&gt;Certificate management has become more simple and robust in kubeadm v1.15.&lt;/p&gt;

&lt;p&gt;If you perform Kubernetes version upgrades regularly, kubeadm will now take care of keeping your cluster up to date and reasonably secure by &lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/#automatic-certificate-renewal&#34; target=&#34;_blank&#34;&gt;automatically rotating all your certificates&lt;/a&gt; at &lt;code&gt;kubeadm upgrade&lt;/code&gt; time.&lt;/p&gt;

&lt;p&gt;If instead, you prefer to renew your certificates manually, you can opt out from the automatic certificate renewal by passing &lt;code&gt;--certificate-renewal=false&lt;/code&gt; to &lt;code&gt;kubeadm upgrade&lt;/code&gt; commands. Then you can perform &lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/#manual-certificate-renewal&#34; target=&#34;_blank&#34;&gt;manual certificate renewal&lt;/a&gt; with the &lt;code&gt;kubeadm alpha certs renew&lt;/code&gt; command.&lt;/p&gt;

&lt;p&gt;But there is more.&lt;/p&gt;

&lt;p&gt;A new command &lt;code&gt;kubeadm alpha certs check-expiration&lt;/code&gt; was introduced to allow users to
&lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/#check-certificate-expiration&#34; target=&#34;_blank&#34;&gt;check certificate expiration&lt;/a&gt;. The output is similar to this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;CERTIFICATE                EXPIRES                  RESIDUAL TIME   EXTERNALLY MANAGED
admin.conf                 May 15, 2020 13:03 UTC   364d            false
apiserver                  May 15, 2020 13:00 UTC   364d            false
apiserver-etcd-client      May 15, 2020 13:00 UTC   364d            false
apiserver-kubelet-client   May 15, 2020 13:00 UTC   364d            false
controller-manager.conf    May 15, 2020 13:03 UTC   364d            false
etcd-healthcheck-client    May 15, 2020 13:00 UTC   364d            false
etcd-peer                  May 15, 2020 13:00 UTC   364d            false
etcd-server                May 15, 2020 13:00 UTC   364d            false
front-proxy-client         May 15, 2020 13:00 UTC   364d            false
scheduler.conf             May 15, 2020 13:03 UTC   364d            false&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You should expect also more work around certificate management in kubeadm in the next releases, with the introduction of ECDSA keys and with improved support for CA key rotation. Additionally, the commands staged under &lt;code&gt;kubeadm alpha&lt;/code&gt; are expected to move top-level soon.&lt;/p&gt;

&lt;h3 id=&#34;improved-configuration-file-format&#34;&gt;Improved Configuration File Format&lt;/h3&gt;

&lt;p&gt;You can argue that there are hardly two Kubernetes clusters that are configured equally, and hence there is a need to customize how the cluster is set up depending on the environment. One way of configuring a component is via flags. However, this has some scalability limitations:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Hard to maintain.&lt;/strong&gt; When a component’s flag set grows over 30+ flags, configuring it becomes really painful.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Complex upgrades&lt;/strong&gt;. When flags are removed, deprecated or changed, you need to upgrade of the binary at the same time as the arguments.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Key-value limited&lt;/strong&gt;. There are simply many types of configuration you can’t express with the  &lt;code&gt;--key=value&lt;/code&gt; syntax.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Imperative&lt;/strong&gt;. In contrast to Kubernetes API objects themselves that are declaratively specified, flag arguments are imperative by design.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is a key problem for Kubernetes components in general, as some components have 150+ flags. With kubeadm we’re pioneering the ComponentConfig effort, and providing users with a small set of flags, but most importantly, a &lt;strong&gt;declarative and versioned configuration file&lt;/strong&gt; for advance use-cases. We call this &lt;em&gt;ComponentConfig&lt;/em&gt;. It has the following characteristics:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Upgradable&lt;/strong&gt;: You can upgrade the binary, and still use the existing, older schema. Automatic migrations.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Programmable&lt;/strong&gt;. Configuration expressed in JSON/YAML allows for consistent, and programmable manipulation&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Expressible&lt;/strong&gt;. Advanced patterns of configuration can be used and applied.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Declarative&lt;/strong&gt;. OpenAPI information can easily be exposed / used for doc generation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In kubeadm v1.15, we have improved the structure and are releasing the new &lt;strong&gt;v1beta2&lt;/strong&gt; format. Important to note that the existing &lt;strong&gt;v1beta1&lt;/strong&gt; format released in v1.13 will still continue to work for several releases. This means you can upgrade kubeadm to v1.15, and still use your existing v1beta1 configuration files. When you’re ready to take advantage of the improvements made in v1beta2, you can perform an automatic schema migration using the &lt;code&gt;kubeadm config migrate&lt;/code&gt; command.&lt;/p&gt;

&lt;p&gt;During the course of the year, we’re looking forward to graduate the schema to General Availability &lt;code&gt;v1&lt;/code&gt;.` If you’re interested in this effort, you can also join &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/wg-component-standard&#34; target=&#34;_blank&#34;&gt;WG Component Standard&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;what-s-next&#34;&gt;What’s next?&lt;/h2&gt;

&lt;h3 id=&#34;2019-plans&#34;&gt;2019 plans&lt;/h3&gt;

&lt;p&gt;We are focusing our efforts around graduating the configuration file format to GA (&lt;code&gt;kubeadm.k8s.io/v1&lt;/code&gt;)`, graduating this super-easy High Availability flow to stable, and providing better tools around rotating certificates needed for running the cluster automatically.&lt;/p&gt;

&lt;p&gt;In addition to these three key milestones of our charter, we want to improve the following areas:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Support joining Windows nodes to a kubeadm cluster (with end-to-end tests)&lt;/li&gt;
&lt;li&gt;Improve the upstream CI signal, mainly for HA and upgrades&lt;/li&gt;
&lt;li&gt;Consolidate how Kubernetes artifacts are built and installed&lt;/li&gt;
&lt;li&gt;Utilize Kustomize to allow for advanced, layered and declarative configuration&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We make no guarantees that these deliverables will ship this year though, as this is a community effort. If you want to see these things happen, please join our SIG and start contributing! The ComponentConfig issues in particular need more attention.&lt;/p&gt;

&lt;h3 id=&#34;kubeadm-now-has-a-logo&#34;&gt;kubeadm now has a logo!&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/dankohn&#34; target=&#34;_blank&#34;&gt;Dan Kohn&lt;/a&gt; offered CNCF’s help with creating a logo for kubeadm in this cycle.
&lt;a href=&#34;https://github.com/alexcontini&#34; target=&#34;_blank&#34;&gt;Alex Contini&lt;/a&gt; created 19 (!) different logo options for the community to vote on. The public poll
was active for around a week, and we got 386 answers. The winning option got 17.4% of the votes. In other words, now we have an
official logo!&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2019-06-24-kubeadm-ha-v115/logo.png&#34; alt=&#34;kubeadm&#39;s logo&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;contributing&#34;&gt;Contributing&lt;/h2&gt;

&lt;p&gt;If this all sounds exciting, &lt;strong&gt;join us&lt;/strong&gt;!&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-cluster-lifecycle&#34; target=&#34;_blank&#34;&gt;SIG Cluster Lifecycle&lt;/a&gt; has many different
subprojects, where kubeadm is one of them. In the following picture you can see that there are many pieces in the
puzzle, and we have a lot still to do.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2019-06-24-kubeadm-ha-v115/projects.png&#34; alt=&#34;SIG Cluster Lifecycle Projects&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Some handy links if you want to start contribute:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;You can watch the SIG Cluster Lifecycle &lt;a href=&#34;https://www.youtube.com/watch?v=Bof9aveB3rA&#34; target=&#34;_blank&#34;&gt;New Contributor Onboarding&lt;/a&gt; session on YouTube.&lt;/li&gt;
&lt;li&gt;Look out for “good first issue”, “help wanted” and “sig/cluster-lifecycle” labeled issues in our repositories
(e.g. &lt;a href=&#34;https://github.com/kubernetes/kubeadm&#34; target=&#34;_blank&#34;&gt;kubernetes/kubeadm&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Join &lt;a href=&#34;https://kubernetes.slack.com/messages/sig-cluster-lifecycle&#34; target=&#34;_blank&#34;&gt;#sig-cluster-lifecycle&lt;/a&gt;, &lt;a href=&#34;https://kubernetes.slack.com/messages/kubeadm&#34; target=&#34;_blank&#34;&gt;#kubeadm&lt;/a&gt;, &lt;a href=&#34;https://kubernetes.slack.com/messages/cluster-api&#34; target=&#34;_blank&#34;&gt;#cluster-api&lt;/a&gt;, &lt;a href=&#34;https://kubernetes.slack.com/messages/minikube&#34; target=&#34;_blank&#34;&gt;#minikube&lt;/a&gt;, &lt;a href=&#34;https://kubernetes.slack.com/messages/kind&#34; target=&#34;_blank&#34;&gt;#kind&lt;/a&gt;, etc. in Slack&lt;/li&gt;
&lt;li&gt;Join our public, bi-weekly SIG Cluster Lifecycle Zoom meeting at Tuesdays 9am PT

&lt;ul&gt;
&lt;li&gt;Check out the &lt;a href=&#34;https://docs.google.com/document/d/1Gmc7LyCIL_148a9Tft7pdhdee0NBHdOfHS1SAF0duI4/edit&#34; target=&#34;_blank&#34;&gt;Meeting Notes&lt;/a&gt; to join&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Join our public, weekly kubeadm Office Hours Zoom meeting at Wednesdays 9am PT

&lt;ul&gt;
&lt;li&gt;Check out the &lt;a href=&#34;https://docs.google.com/document/d/130_kiXjG7graFNSnIAgtMS1G8zPDwpkshgfRYS0nggo/edit&#34; target=&#34;_blank&#34;&gt;Meeting Notes&lt;/a&gt; to join&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Check out the &lt;a href=&#34;https://youtu.be/bA2M41J4wvg&#34; target=&#34;_blank&#34;&gt;SIG Cluster Lifecycle Intro&lt;/a&gt; or the
&lt;a href=&#34;https://youtu.be/spXSSIbZTqM&#34; target=&#34;_blank&#34;&gt;kubeadm Deep Dive&lt;/a&gt; sessions from KubeCon Barcelona&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;thank-you&#34;&gt;Thank You&lt;/h3&gt;

&lt;p&gt;This release wouldn’t have been possible without the help of the great people that have been contributing to SIG Cluster Lifecycle
and kubeadm. We would like to thank all the kubeadm contributors and companies making it possible for their developers to work
on Kubernetes!&lt;/p&gt;

&lt;p&gt;In particular, we would like to thank the &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/cmd/kubeadm/OWNERS&#34; target=&#34;_blank&#34;&gt;kubeadm subproject owners&lt;/a&gt; that made this possible:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Tim St. Clair , &lt;a href=&#34;https://github.com/timothysc&#34; target=&#34;_blank&#34;&gt;@timothysc&lt;/a&gt;, SIG Cluster Lifecycle co-chair, VMware&lt;/li&gt;
&lt;li&gt;Lucas Käldström, &lt;a href=&#34;https://github.com/luxas&#34; target=&#34;_blank&#34;&gt;@luxas&lt;/a&gt;, SIG Cluster Lifecycle co-chair, Weaveworks&lt;/li&gt;
&lt;li&gt;Fabrizio Pandini, &lt;a href=&#34;https://github.com/fabriziopandini&#34; target=&#34;_blank&#34;&gt;@fabriziopandini&lt;/a&gt;, Independent&lt;/li&gt;
&lt;li&gt;Lubomir I. Ivanov, &lt;a href=&#34;https://github.com/neolit123&#34; target=&#34;_blank&#34;&gt;@neolit123&lt;/a&gt;, VMware&lt;/li&gt;
&lt;li&gt;Rostislav M. Georgiev, &lt;a href=&#34;https://github.com/rosti&#34; target=&#34;_blank&#34;&gt;@rosti&lt;/a&gt;, VMware&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Introducing Volume Cloning Alpha for Kubernetes</title>
      <link>https://kubernetes.io/blog/2019/06/21/introducing-volume-cloning-alpha-for-kubernetes/</link>
      <pubDate>Fri, 21 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/06/21/introducing-volume-cloning-alpha-for-kubernetes/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: John Griffith (Red Hat)&lt;/p&gt;

&lt;p&gt;Kubernetes v1.15 introduces alpha support for volume cloning. This feature allows you to create new volumes using the contents of existing volumes in the user&amp;rsquo;s namespace using the Kubernetes API.&lt;/p&gt;

&lt;h2 id=&#34;what-is-a-clone&#34;&gt;What is a Clone?&lt;/h2&gt;

&lt;p&gt;Many storage systems provide the ability to create a &amp;ldquo;clone&amp;rdquo; of a volume.  A clone is a duplicate of an existing volume that is its own unique volume on the system, but the data on the source is duplicated to the destination (clone).  A clone is similar to a snapshot in that it&amp;rsquo;s a point in time copy of a volume, however rather than creating a new snapshot object from a volume, we&amp;rsquo;re instead creating a new independent volume, sometimes thought of as pre-populating the newly created volume.&lt;/p&gt;

&lt;h2 id=&#34;why-add-cloning-to-kubernetes&#34;&gt;Why add cloning to Kubernetes&lt;/h2&gt;

&lt;p&gt;The Kubernetes volume plugin system already provides a powerful abstraction that automates the provisioning, attaching, and mounting of block and file storage.&lt;/p&gt;

&lt;p&gt;Underpinning all these features is the Kubernetes goal of workload portability: Kubernetes aims to create an abstraction layer between distributed systems applications and underlying clusters so that applications can be agnostic to the specifics of the cluster they run on and application deployment requires no specific storage device knowledge.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-storage&#34; target=&#34;_blank&#34;&gt;Kubernetes Storage SIG&lt;/a&gt; identified clone operations as critical functionality for many stateful workloads. For example, a database administrator may want to duplicate a database volume and create another instance of an existing database.&lt;/p&gt;

&lt;p&gt;By providing a standard way to trigger clone operations in the Kubernetes API, Kubernetes users can now handle use cases like this without having to go around the Kubernetes API (and manually executing storage system specific operations).  While cloning is similar in behavior to creating a snapshot of a volume, then creating a volume from the snapshot, a clone operation is more streamlined and is more efficient for many backend devices.&lt;/p&gt;

&lt;p&gt;Kubernetes users are now empowered to incorporate clone operations in a cluster agnostic way into their tooling and policy with the comfort of knowing that it will work against arbitrary Kubernetes clusters regardless of the underlying storage.&lt;/p&gt;

&lt;h2 id=&#34;kubernetes-api-and-cloning&#34;&gt;Kubernetes API and Cloning&lt;/h2&gt;

&lt;p&gt;The cloning feature in Kubernetes is enabled via the &lt;code&gt;PersistentVolumeClaim.DataSource&lt;/code&gt; field.  Prior to v1.15 the only valid object type permitted for use as a dataSource was a &lt;code&gt;VolumeSnapshot&lt;/code&gt;.  The cloning feature extends the allowed &lt;code&gt;PersistentVolumeclaim.DataSource.Kind&lt;/code&gt; field to not only allow &lt;code&gt;VolumeSnapshot&lt;/code&gt; but also &lt;code&gt;PersistentVolumeClaim&lt;/code&gt;.  The existing behavior is not changed.&lt;/p&gt;

&lt;p&gt;There are no new objects introduced to enable cloning. Instead, the existing dataSource field in the PersistentVolumeClaim object is expanded to be able to accept the name of an existing PersistentVolumeClaim in the same namespace.  It is important to note that from a users perspective a clone is just another PersistentVolume and PersistentVolumeClaim, the only difference being that that PersistentVolume is being populated with the contents of another PersistentVolume at creation time.  After creation it behaves exactly like any other Kubernetes PersistentVolume and adheres to the same behaviors and rules.&lt;/p&gt;

&lt;h2 id=&#34;which-volume-plugins-support-kubernetes-cloning&#34;&gt;Which volume plugins support Kubernetes Cloning?&lt;/h2&gt;

&lt;p&gt;Kubernetes supports three types of volume plugins: in-tree, Flex, and &lt;a href=&#34;https://github.com/container-storage-interface/spec/blob/master/spec.md&#34; target=&#34;_blank&#34;&gt;Container Storage Interface&lt;/a&gt; (CSI). See &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md&#34; target=&#34;_blank&#34;&gt;Kubernetes Volume Plugin FAQ&lt;/a&gt; for details.&lt;/p&gt;

&lt;p&gt;Cloning is only supported for CSI drivers (not for in-tree or Flex). To use the Kubernetes cloning feature, ensure that a CSI Driver that implements cloning is deployed on your cluster.
For a list of CSI drivers that currently support cloning see the &lt;a href=&#34;https://kubernetes-csi.github.io/docs/drivers.html&#34; target=&#34;_blank&#34;&gt;CSI Drivers doc&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;kubernetes-cloning-requirements&#34;&gt;Kubernetes Cloning Requirements&lt;/h2&gt;

&lt;p&gt;Before using Kubernetes Volume Cloning, you must:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Ensure a CSI driver implementing Cloning is deployed and running on your Kubernetes cluster.&lt;/li&gt;
&lt;li&gt;Enable the Kubernetes Volume Cloning feature via new Kubernetes feature gate (disabled by default for alpha):

&lt;ul&gt;
&lt;li&gt;Set the following flag on the API server binary: &lt;code&gt;--feature-gates=VolumePVCDataSource=true&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;The source and destination claims must be in the same namespace.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;creating-a-clone-with-kubernetes&#34;&gt;Creating a clone with Kubernetes&lt;/h2&gt;

&lt;p&gt;To provision a new volume pre-populated with data from an existing Kubernetes Volume, use the dataSource field in the &lt;code&gt;PersistentVolumeClaim&lt;/code&gt;.  There are three parameters:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;name - name of the &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; object to use as source&lt;/li&gt;
&lt;li&gt;kind - must be &lt;code&gt;PersistentVolumeClaim&lt;/code&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;apiGroup - must be &lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: PersistentVolumeClaim
metadata:
name: pvc-clone
Namespace: demo-namespace
spec:
storageClassName: csi-storageclass
dataSource:
name: src-pvc
kind: PersistentVolumeClaim 
apiGroup: &amp;quot;&amp;quot;
accessModes:
- ReadWriteOnce
resources:
requests:
  storage: 1Gi # NOTE this capacity must be specified and must be &amp;gt;= the capacity of the source volume
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When the &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; object is created, it will trigger provisioning of a new volume that is pre-populated with data from the specified &lt;code&gt;dataSource&lt;/code&gt; volume.  It is the sole responsbility of the CSI Plugin to implement the cloning of volumes.&lt;/p&gt;

&lt;h2 id=&#34;as-a-storage-vendor-how-do-i-add-support-for-cloning-to-my-csi-driver&#34;&gt;As a storage vendor, how do I add support for cloning to my CSI driver?&lt;/h2&gt;

&lt;p&gt;For more information on how to implement cloning in your CSI Plugin, reference the &lt;a href=&#34;https://kubernetes-csi.github.io/docs/developing.html&#34; target=&#34;_blank&#34;&gt;developing a CSI driver for Kubernetes&lt;/a&gt; section of the CSI docs.&lt;/p&gt;

&lt;h2 id=&#34;what-are-the-limitations-of-alpha&#34;&gt;What are the limitations of alpha?&lt;/h2&gt;

&lt;p&gt;The alpha implementation of cloning for Kubernetes has the following limitations:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Does not support cloning volumes across different namespaces&lt;/li&gt;
&lt;li&gt;Does not support cloning volumes across different storage classes (backends)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;future&#34;&gt;Future&lt;/h2&gt;

&lt;p&gt;Depending on feedback and adoption, the Kubernetes team plans to push the CSI cloning implementation to beta in 1.16.&lt;/p&gt;

&lt;p&gt;A common question that users have regarding cloning is &amp;ldquo;what about cross namespace clones&amp;rdquo;.  As we&amp;rsquo;ve mentioned, the current release requires that source and destination be in the same namespace.  There are however efforts underway to propose a namespace transfer API, future versions of Kubernetes may provide the ability to transfer volume resources from one namespace to another.  This feature is still under discussion and design, and may or may not be available in a future release.&lt;/p&gt;

&lt;h2 id=&#34;how-can-i-learn-more&#34;&gt;How can I learn more?&lt;/h2&gt;

&lt;p&gt;You can find additional documentation on the cloning feature in the &lt;a href=&#34;https://k8s.io/docs/concepts/storage/volume-pvc-datasource.md&#34; target=&#34;_blank&#34;&gt;storage concept docs&lt;/a&gt; and also the &lt;a href=&#34;https://kubernetes-csi.github.io/docs/volume-cloning.html&#34; target=&#34;_blank&#34;&gt;CSI docs&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;how-do-i-get-involved&#34;&gt;How do I get involved?&lt;/h2&gt;

&lt;p&gt;This project, like all of Kubernetes, is the result of hard work by many contributors from diverse backgrounds working together.&lt;/p&gt;

&lt;p&gt;We offer a huge thank you to all the contributors in Kubernetes Storage SIG and CSI community who helped review the design and implementation of the project, including but not limited to the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Saad Ali (&lt;a href=&#34;https://github.com/saadali&#34; target=&#34;_blank&#34;&gt;saadali&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Tim Hockin (&lt;a href=&#34;https://github.com/thockin&#34; target=&#34;_blank&#34;&gt;thockin&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Jan Šafránek (&lt;a href=&#34;https://github.com/jsafrane&#34; target=&#34;_blank&#34;&gt;jsafrane&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Michelle Au (&lt;a href=&#34;https://github.com/msau42&#34; target=&#34;_blank&#34;&gt;msau42&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Xing Yang (&lt;a href=&#34;https://github.com/xing-yang&#34; target=&#34;_blank&#34;&gt;xing-yang&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you’re interested in getting involved with the design and development of CSI or any part of the Kubernetes Storage system, join the &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-storage&#34; target=&#34;_blank&#34;&gt;Kubernetes Storage Special Interest Group&lt;/a&gt; (SIG). We’re rapidly growing and always welcome new contributors.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Future of CRDs: Structural Schemas</title>
      <link>https://kubernetes.io/blog/2019/06/20/crd-structural-schema/</link>
      <pubDate>Thu, 20 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/06/20/crd-structural-schema/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Stefan Schimanski (Red Hat)&lt;/p&gt;

&lt;p&gt;CustomResourceDefinitions were introduced roughly two years ago as the primary way to extend the Kubernetes API with custom resources. From the beginning they stored arbitrary JSON data, with the exception that &lt;code&gt;kind&lt;/code&gt;, &lt;code&gt;apiVersion&lt;/code&gt; and &lt;code&gt;metadata&lt;/code&gt; had to follow the Kubernetes API conventions. In Kubernetes 1.8 CRDs gained the ability to define an optional OpenAPI v3 based validation schema.&lt;/p&gt;

&lt;p&gt;By the nature of OpenAPI specifications though—only describing what must be there, not what shouldn’t, and by being potentially incomplete specifications—the Kubernetes API server never knew the complete structure of CustomResource instances. As a consequence, kube-apiserver—until today—stores all JSON data received in an API request (if it validates against the OpenAPI spec). This especially includes anything that is not specified in the OpenAPI schema.&lt;/p&gt;

&lt;h2 id=&#34;the-story-of-malicious-unspecified-data&#34;&gt;The story of malicious, unspecified data&lt;/h2&gt;

&lt;p&gt;To understand this, we assume a CRD for maintenance jobs by the operations team, running each night as a service user:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;operations/v1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;MaintenanceNightlyJob&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;shell:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44;font-style:italic&#34;&gt;&amp;gt;
&lt;/span&gt;&lt;span style=&#34;color:#b44;font-style:italic&#34;&gt;    grep backdoor /etc/passwd || &lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;echo&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;“backdoor:76asdfh76:/bin/bash”&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&amp;gt;&amp;gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/etc/passwd&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;||&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;true&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;machines:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;[“az1-master1”,”az1-master2”,”az2-master3”]&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;privileged:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;true&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The privileged field is not specified by the operations team. Their controller does not know it, and their validating admission webhook does not know about it either. Nevertheless, kube-apiserver persists this suspicious, but unknown field without ever validating it.&lt;/p&gt;

&lt;p&gt;When run in the night, this job never fails, but because the service user is not able to write &lt;code&gt;/etc/passwd&lt;/code&gt;, it will also not cause any harm.&lt;/p&gt;

&lt;p&gt;The maintenance team needs support for privileged jobs. It adds the &lt;code&gt;privileged&lt;/code&gt; support, but is super careful to implement authorization for privileged jobs by only allowing those to be created by very few people in the company. That malicious job though has long been persisted to etcd. The next night arrives and the malicious job is executed.&lt;/p&gt;

&lt;h2 id=&#34;towards-complete-knowledge-of-the-data-structure&#34;&gt;Towards complete knowledge of the data structure&lt;/h2&gt;

&lt;p&gt;This example shows that we cannot trust CustomResource data in etcd. Without having complete knowledge about the JSON structure, the kube-apsierver cannot do anything to prevent persistence of unknown data.&lt;/p&gt;

&lt;p&gt;Kubernetes 1.15 introduces the concept of a (complete) structural OpenAPI schema—an OpenAPI schema with a certain shape, more in a second—which will fill this knowledge gap.&lt;/p&gt;

&lt;p&gt;If the provided OpenAPI validation schema provided by the CRD author is not structural, violations are reported in a &lt;code&gt;NonStructural&lt;/code&gt; condition in the CRD.&lt;/p&gt;

&lt;p&gt;A structural schema for CRDs in &lt;code&gt;apiextensions.k8s.io/v1beta1&lt;/code&gt; will not be required. But we plan to require structural schemas for every CRD created in &lt;code&gt;apiextensions.k8s.io/v1&lt;/code&gt;, targeted for 1.16.&lt;/p&gt;

&lt;p&gt;But now let us see what a structural schema looks like.&lt;/p&gt;

&lt;h2 id=&#34;structural-schema&#34;&gt;Structural Schema&lt;/h2&gt;

&lt;p&gt;The &lt;strong&gt;core of a structural schema&lt;/strong&gt; is an OpenAPI v3 schema made out of&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;properties&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;items&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;additionalProperties&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;type&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nullable&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;title&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;descriptions&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In addition, all types must be non-empty, and in each sub-schema only one of &lt;code&gt;properties&lt;/code&gt;, &lt;code&gt;additionalProperties&lt;/code&gt; or &lt;code&gt;items&lt;/code&gt; may be used.&lt;/p&gt;

&lt;p&gt;Here is an example of our &lt;code&gt;MaintenanceNightlyJob&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;object&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;properties:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;object&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;properties&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;command:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;string&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;shell:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;string&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;machines:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;array&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;items:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;string&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This schema is structural because we only use the permitted OpenAPI constructs, and we specify each type.&lt;/p&gt;

&lt;p&gt;Note that we leave out &lt;code&gt;apiVersion&lt;/code&gt;, &lt;code&gt;kind&lt;/code&gt; and &lt;code&gt;metadata&lt;/code&gt;. These are implicitly defined for each object.&lt;/p&gt;

&lt;p&gt;Starting from this structural core of our schema, we might enhance it for value validation purposes with nearly all other OpenAPI constructs, with only a few restrictions, for example:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;object&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;properties:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;object&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;properties&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;command:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;string&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;minLength:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;                          &lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# value validation&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;shell:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;string&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;minLength:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;                          &lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# value validation&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;machines:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;array&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;items:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;string&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;pattern:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;“^[a-z0&lt;span style=&#34;color:#666&#34;&gt;-9&lt;/span&gt;]+(-[a-z0&lt;span style=&#34;color:#666&#34;&gt;-9&lt;/span&gt;]+)&lt;span style=&#34;color:#080&#34;&gt;*$”&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# value validation&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;oneOf:&lt;span style=&#34;color:#bbb&#34;&gt;                                    &lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# value validation&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;required:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;[“command”]&lt;span style=&#34;color:#bbb&#34;&gt;                   &lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# value validation&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;required:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;[“shell”]&lt;span style=&#34;color:#bbb&#34;&gt;                     &lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# value validation&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;required:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;[“spec”]&lt;span style=&#34;color:#bbb&#34;&gt;                            &lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# value validation&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Some notable restrictions for these additional value validations:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;the last 5 of the core constructs are not allowed: &lt;code&gt;additionalProperties&lt;/code&gt;, &lt;code&gt;type&lt;/code&gt;, &lt;code&gt;nullable&lt;/code&gt;, &lt;code&gt;title&lt;/code&gt;, &lt;code&gt;description&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;every properties field mentioned, must also show up in the core (without the blue value validations).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As you can see also logical constraints using &lt;code&gt;oneOf&lt;/code&gt;, &lt;code&gt;allOf&lt;/code&gt;, &lt;code&gt;anyOf&lt;/code&gt;, &lt;code&gt;not&lt;/code&gt; are allowed.&lt;/p&gt;

&lt;p&gt;To sum up, an OpenAPI schema is structural if&lt;br/&gt;&lt;br/&gt;
1. it has the core as defined above out of &lt;code&gt;properties&lt;/code&gt;, &lt;code&gt;items&lt;/code&gt;, &lt;code&gt;additionalProperties&lt;/code&gt;, &lt;code&gt;type&lt;/code&gt;, &lt;code&gt;nullable&lt;/code&gt;, &lt;code&gt;title&lt;/code&gt;, &lt;code&gt;description&lt;/code&gt;,&lt;br/&gt;
2. all types are defined,&lt;br/&gt;
3. the core is extended with value validation following the constraints:&lt;br/&gt;
   (i) inside of value validations no &lt;code&gt;additionalProperties&lt;/code&gt;, &lt;code&gt;type&lt;/code&gt;, &lt;code&gt;nullable&lt;/code&gt;, &lt;code&gt;title&lt;/code&gt;, &lt;code&gt;description&lt;/code&gt;&lt;br/&gt;
   (ii) all fields mentioned in value validation are specified in the core.&lt;/p&gt;

&lt;p&gt;Let us modify our example spec slightly, to make it non-structural:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;properties:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;object&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;properties&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;command:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;string&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;minLength:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;shell:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;string&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;minLength:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;machines:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;array&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;items:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;string&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;pattern:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;“^[a-z0&lt;span style=&#34;color:#666&#34;&gt;-9&lt;/span&gt;]+(-[a-z0&lt;span style=&#34;color:#666&#34;&gt;-9&lt;/span&gt;]+)&lt;span style=&#34;color:#080&#34;&gt;*$”&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;oneOf:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;properties:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;command:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;string&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;required:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;[“command”]&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;properties:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;shell:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;string&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;required:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;[“shell”]&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;not:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;properties:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;privileged:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;{}&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;required:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;[“spec”]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This spec is non-structural for many reasons:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;type: object&lt;/code&gt; at the root is missing (rule 2).&lt;/li&gt;
&lt;li&gt;inside of &lt;code&gt;oneOf&lt;/code&gt; it is not allowed to use &lt;code&gt;type&lt;/code&gt; (rule 3-i).&lt;/li&gt;
&lt;li&gt;inside of &lt;code&gt;not&lt;/code&gt; the property &lt;code&gt;privileged&lt;/code&gt; is mentioned, but it is not specified in the core (rule 3-ii).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now that we know what a structural schema is, and what is not, let us take a look at our attempt above to forbid &lt;code&gt;privileged&lt;/code&gt; as a field. While we have seen that this is not possible in a structural schema, the good news is that we don’t have to explicitly attempt to forbid unwanted fields in advance.&lt;/p&gt;

&lt;h2 id=&#34;pruning-don-t-preserve-unknown-fields&#34;&gt;Pruning – don’t preserve unknown fields&lt;/h2&gt;

&lt;p&gt;In &lt;code&gt;apiextensions.k8s.io/v1&lt;/code&gt; pruning will be the default, with ways to opt-out of it. Pruning in &lt;code&gt;apiextensions.k8s.io/v1beta1&lt;/code&gt; is enabled via&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;apiextensions/v1beta1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;CustomResourceDefinition&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;…&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;preserveUnknownFields:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;false&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Pruning can only be enabled if the global schema or the schemas of all versions are structural.&lt;/p&gt;

&lt;p&gt;If pruning is enabled, the pruning algorithm&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;assumes that the schema is complete, i.e. every field is mentioned and not-mentioned fields can be dropped&lt;/li&gt;
&lt;li&gt;is run on&lt;br/&gt;
(i) data received via an API request&lt;br/&gt;
(ii) after conversion and admission requests&lt;br/&gt;
(iii) when reading from etcd (using the schema version of the data in etcd).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As we don’t specify &lt;code&gt;privileged&lt;/code&gt; in our structural example schema, the malicious field is pruned from before persisting to etcd:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;operations/v1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;MaintenanceNightlyJob&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;shell:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44;font-style:italic&#34;&gt;&amp;gt;
&lt;/span&gt;&lt;span style=&#34;color:#b44;font-style:italic&#34;&gt;    grep backdoor /etc/passwd || &lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;echo&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;“backdoor:76asdfh76:/bin/bash”&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&amp;gt;&amp;gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/etc/passwd&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;||&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;true&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;machines:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;[“az1-master1”,”az1-master2”,”az2-master3”]&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# pruned: privileged: true&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&#34;extensions&#34;&gt;Extensions&lt;/h2&gt;

&lt;p&gt;While most Kubernetes-like APIs can be expressed with a structural schema, there are a few exceptions, notably &lt;code&gt;intstr.IntOrString&lt;/code&gt;, &lt;code&gt;runtime.RawExtension&lt;/code&gt;s and pure JSON fields.&lt;/p&gt;

&lt;p&gt;Because we want CRDs to make use of these types as well, we introduce the following OpenAPI vendor extensions to the permitted core constructs:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;x-kubernetes-embedded-resource: true&lt;/code&gt; — specifies that this is an &lt;code&gt;runtime.RawExtension&lt;/code&gt;-like field, with a Kubernetes resource with apiVersion, kind and metadata. The consequence is that those 3 fields are not pruned and are automatically validated.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;x-kubernetes-int-or-string: true&lt;/code&gt; — specifies that this is either an integer or a string. No types must be specified, but&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;oneOf:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;integer&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;string&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;is permitted, though optional.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;x-kubernetes-preserve-unknown-fields: true&lt;/code&gt; — specifies that the pruning algorithm should not prune any field. This can be combined with &lt;code&gt;x-kubernetes-embedded-resource&lt;/code&gt;. Note that within a nested &lt;code&gt;properties&lt;/code&gt; or &lt;code&gt;additionalProperties&lt;/code&gt; OpenAPI schema the pruning starts again.
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One can use &lt;code&gt;x-kubernetes-preserve-unknown-fields: true&lt;/code&gt; at the root of the schema (and inside any &lt;code&gt;properties&lt;/code&gt;, &lt;code&gt;additionalProperties&lt;/code&gt;) to get the traditional CRD behaviour that nothing is pruned, despite setting &lt;code&gt;spec.preserveUnknownProperties: false&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;With this we conclude the discussion of the structural schema in Kubernetes 1.15 and beyond. To sum up:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;structural schemas are optional in &lt;code&gt;apiextensions.k8s.io/v1beta1&lt;/code&gt;. Non-structural CRDs will keep working as before.&lt;/li&gt;
&lt;li&gt;pruning (enabled via &lt;code&gt;spec.preserveUnknownProperties: false&lt;/code&gt;) requires a structural schema.&lt;/li&gt;
&lt;li&gt;structural schema violations are signalled via the &lt;code&gt;NonStructural&lt;/code&gt; condition in the CRD.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Structural schemas are the future of CRDs. &lt;code&gt;apiextensions.k8s.io/v1&lt;/code&gt; will require them. But&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;type:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;object&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;x-kubernetes-preserve-unknown-fields:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;true&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;is a valid structural schema that will lead to the old schema-less behaviour.&lt;/p&gt;

&lt;p&gt;Any new feature for CRDs starting from Kubernetes 1.15 will require to have a structural schema:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;publishing of OpenAPI validation schemas and therefore support for kubectl client-side validation, and &lt;code&gt;kubectl explain&lt;/code&gt; support (beta in Kubernetes 1.15)&lt;/li&gt;
&lt;li&gt;CRD conversion (beta in Kubernetes 1.15)&lt;/li&gt;
&lt;li&gt;CRD defaulting (alpha in Kubernetes 1.15)&lt;/li&gt;
&lt;li&gt;Server-side apply (alpha in Kubernetes 1.15, CRD support pending).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Of course &lt;a href=&#34;https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/#specifying-a-structural-schema&#34; target=&#34;_blank&#34;&gt;structural schemas&lt;/a&gt; are also described in the Kubernetes documentation for the 1.15 release.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes 1.15: Extensibility and Continuous Improvement</title>
      <link>https://kubernetes.io/blog/2019/06/19/kubernetes-1-15-release-announcement/</link>
      <pubDate>Wed, 19 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/06/19/kubernetes-1-15-release-announcement/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; The 1.15 &lt;a href=&#34;https://github.com/kubernetes/sig-release/blob/master/releases/release-1.15/release_team.md&#34; target=&#34;_blank&#34;&gt;Release Team&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We’re pleased to announce the delivery of Kubernetes 1.15, our second release of 2019! Kubernetes 1.15 consists of 25 enhancements: 2 moving to stable, 13 in beta, and 10 in alpha. The main themes of this release are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Continuous Improvement

&lt;ul&gt;
&lt;li&gt;Project sustainability is not just about features. Many SIGs have been working on improving test coverage, ensuring the basics stay reliable, and stability of the core feature set and working on maturing existing features and cleaning up the backlog.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Extensibility

&lt;ul&gt;
&lt;li&gt;The community has been asking for continuing support of extensibility, so this cycle features more work around CRDs and API Machinery. Most of the enhancements in this cycle were from SIG API Machinery and related areas.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let’s dive into the key features of this release:&lt;/p&gt;

&lt;h2 id=&#34;extensibility-around-core-kubernetes-apis&#34;&gt;Extensibility around core Kubernetes APIs&lt;/h2&gt;

&lt;p&gt;The theme of the new developments around CustomResourceDefinitions is data consistency and native behaviour. A user should not notice whether the interaction is with a CustomResource or with a Golang-native resource. With big steps we are working towards a GA release of CRDs and GA of admission webhooks in one of the next releases.&lt;/p&gt;

&lt;p&gt;In this direction, we have rethought our OpenAPI based validation schemas in CRDs and from 1.15 on we check each schema against a restriction called “structural schema”. This basically enforces non-polymorphic and complete typing of each field in a CustomResource. We are going to require structural schemas in the future, especially for all new features including those listed below, and list violations in a &lt;code&gt;NonStructural&lt;/code&gt; condition. Non-structural schemas keep working for the time being in the v1beta1 API group. But any serious CRD application is urged to migrate to structural schemas in the foreseeable future.&lt;/p&gt;

&lt;p&gt;Details about what makes a schema structural will be published in a blog post on kubernetes.io later this week, and it is of course &lt;a href=&#34;https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/#specifying-a-structural-schema&#34;&gt;documented in the Kubernetes documentation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;beta: CustomResourceDefinition Webhook Conversion&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;CustomResourceDefinitions support multiple versions as beta since 1.14. With Kubernetes 1.15, they gain the ability to convert between different versions on-the-fly, just like users are used to from native resources for long term. Conversions for CRDs are implemented via webhooks, deployed inside the cluster by the cluster admin. This feature is promoted to beta in Kubernetes 1.15, lifting CRDs to a completely new level for serious CRD applications.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;beta: CustomResourceDefinition OpenAPI Publishing&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;OpenAPI specs for native types have been served at &lt;code&gt;/openapi/v2&lt;/code&gt; by kube-apiserver for a long time, and they are consumed by a number of components, notably kubectl client-side validation, kubectl explain and OpenAPI based client generators.&lt;/p&gt;

&lt;p&gt;OpenAPI publishing for CRDs will be available with Kubernetes 1.15 as beta, yet again only for structural schemas.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;beta: CustomResourceDefinitions Pruning&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Pruning is the automatic removal of unknown fields in objects sent to a Kubernetes API. A field is unknown if it is not specified in the OpenAPI validation schema. This is both a data consistency and security relevant feature. It enforces that only data structures specified by the CRD developer are persisted to etcd. This is the behaviour of native resources, and will be available for CRDs as well, starting as beta in Kubernetes 1.15.&lt;/p&gt;

&lt;p&gt;Pruning is activated via &lt;code&gt;spec.preserveUnknownFields: false&lt;/code&gt; in the CustomResourceDefinition. A future apiextensions.k8s.io/v1 variant of CRDs will enforce pruning (with a possible, but explicitly necessary opt-out).&lt;/p&gt;

&lt;p&gt;Pruning requires that CRD developer provides complete, structural validation schemas, either top-level or for all versions of the CRD.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;alpha: CustomResourceDefinition Defaulting&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;CustomResourceDefinitions get support for defaulting. Defaults are specified using the &lt;code&gt;default&lt;/code&gt; keyword in the OpenAPI validation schema. Defaults are set for unspecified field in an object sent to the API, and when reading from etcd.&lt;/p&gt;

&lt;p&gt;Defaulting will be available as alpha in Kubernetes 1.15 for structural schemas.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;beta: Admission Webhook Reinvocation &amp;amp; Improvements&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Mutating and validating admission webhooks become more and more mainstream for projects extending the Kubernetes API. Until now mutating webhooks were only called once, in alphabetical order. An earlier run webhook cannot react on the output of webhooks called later in the chain. With Kubernetes 1.15 this will change:&lt;/p&gt;

&lt;p&gt;Mutating webhooks can opt-in into at least one re-invocation by specifying &lt;code&gt;reinvocationPolicy: IfNeeded&lt;/code&gt;. If a later mutating webhook modifies the object, the earlier webhook will get a second chance.&lt;/p&gt;

&lt;p&gt;This requires that webhooks have an idem-potent-like behaviour which can cope with this second invocation.&lt;/p&gt;

&lt;p&gt;It is not planned to add another round of invocations such that webhook authors still have to be careful about the changes to admitted objects they implement. Finally the validating webhooks are called to verify that promised invariants are fulfilled.&lt;/p&gt;

&lt;p&gt;There are more smaller changes to admission webhook, notably &lt;code&gt;objectSelector&lt;/code&gt; to exclude objects with certain labels from admission, arbitrary port (not only 443) for the webhook server.&lt;/p&gt;

&lt;h2 id=&#34;cluster-lifecycle-stability-and-usability-improvements&#34;&gt;Cluster Lifecycle Stability and Usability Improvements&lt;/h2&gt;

&lt;p&gt;Work on making Kubernetes installation, upgrade and configuration even more robust has been a major focus for this cycle for SIG Cluster Lifecycle (see our last &lt;a href=&#34;https://docs.google.com/presentation/d/1QUOsQxfEfHlMq4lPjlK2ewQHsr9peEKymDw5_XwZm8Q/edit?usp=sharing&#34; target=&#34;_blank&#34;&gt;Community Update&lt;/a&gt;). Bug fixes across bare metal tooling and production-ready user stories, such as the high availability use cases have been given priority for 1.15.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;kubeadm&lt;/strong&gt;, the cluster lifecycle building block, continues to receive features and stability work required for bootstrapping production clusters efficiently. kubeadm has promoted high availability (HA) capability to beta, allowing users to use the familiar &lt;code&gt;kubeadm init&lt;/code&gt; and &lt;code&gt;kubeadm join&lt;/code&gt; commands to &lt;a href=&#34;https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/&#34;&gt;configure and deploy an HA control plane&lt;/a&gt;. An entire new test suite has been created specifically for ensuring these features will stay stable over time.&lt;/p&gt;

&lt;p&gt;Certificate management has become more robust in 1.15, with kubeadm now seamlessly rotating all your certificates (on upgrades) before they expire. Check the &lt;a href=&#34;https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-alpha/&#34;&gt;kubeadm documentation&lt;/a&gt; for information on how to manage your certificates.&lt;/p&gt;

&lt;p&gt;The kubeadm configuration file API is moving from v1beta1 to v1beta2 in 1.15.&lt;/p&gt;

&lt;p&gt;Finally, let’s celebrate that kubeadm now &lt;a href=&#34;https://github.com/kubernetes/kubeadm/issues/1588&#34; target=&#34;_blank&#34;&gt;has its own logo&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2019-06-19-kubernetes-1-15-release-announcement/kubeadm-logo.png&#34; alt=&#34;kubeadm official logo&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;continued-improvement-of-csi&#34;&gt;Continued improvement of CSI&lt;/h2&gt;

&lt;p&gt;In Kubernetes v1.15, SIG Storage continued work to &lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/625&#34; target=&#34;_blank&#34;&gt;enable migration of in-tree volume plugins&lt;/a&gt; to Container Storage Interface (CSI). SIG Storage worked on bringing CSI to feature parity with in-tree functionality, including functionality like resizing, inline volumes, and more. SIG Storage introduces new alpha functionality in CSI that doesn&amp;rsquo;t exist in the Kubernetes Storage subsystem yet, like volume cloning.&lt;/p&gt;

&lt;p&gt;Volume cloning enables users to specify another PVC as a &amp;ldquo;DataSource&amp;rdquo; when provisioning a new volume. If the underlying storage system supports this functionality and implements the &amp;ldquo;CLONE_VOLUME&amp;rdquo; capability in its CSI driver, then the new volume becomes a clone of the source volume.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Additional Notable Feature Updates&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Support for go modules in Kubernetes Core&lt;/li&gt;
&lt;li&gt;Continued preparation on cloud provider extraction and code organization. The cloud provider code has been moved to &lt;a href=&#34;https://github.com/kubernetes/legacy-cloud-providers&#34; target=&#34;_blank&#34;&gt;kubernetes/legacy-cloud-providers&lt;/a&gt; for easier removal later and external consumption.&lt;/li&gt;
&lt;li&gt;Kubectl &lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/515&#34; target=&#34;_blank&#34;&gt;get and describe&lt;/a&gt; now work with extensions&lt;/li&gt;
&lt;li&gt;Nodes now support &lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/606&#34; target=&#34;_blank&#34;&gt;third party monitoring plugins&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;A new &lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/624&#34; target=&#34;_blank&#34;&gt;Scheduling Framework&lt;/a&gt; for schedule plugins is now Alpha&lt;/li&gt;
&lt;li&gt;ExecutionHook API &lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/962&#34; target=&#34;_blank&#34;&gt;designed to trigger hook commands&lt;/a&gt; in the containers for different use cases is now Alpha.&lt;/li&gt;
&lt;li&gt;Continued deprecation of extensions/v1beta1, apps/v1beta1, and apps/v1beta2 APIs; these extensions will be retired in 1.16!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Check the &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.15.md#kubernetes-v115-release-notes&#34; target=&#34;_blank&#34;&gt;release notes&lt;/a&gt; for a complete list of notable features and fixes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Availability&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Kubernetes 1.15 is available for &lt;a href=&#34;https://github.com/kubernetes/kubernetes/releases/tag/v1.15.0&#34; target=&#34;_blank&#34;&gt;download on GitHub&lt;/a&gt;. To get started with Kubernetes, check out these &lt;a href=&#34;https://kubernetes.io/docs/tutorials/&#34; target=&#34;_blank&#34;&gt;interactive tutorials&lt;/a&gt;. You can also easily install 1.15 using &lt;a href=&#34;https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/&#34; target=&#34;_blank&#34;&gt;kubeadm&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Features Blog Series&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If you’re interested in exploring these features more in depth, check back this week and the next for our Days of Kubernetes series where we’ll highlight detailed walkthroughs of the following features:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Future of CRDs: Structural Schemas&lt;/li&gt;
&lt;li&gt;Introducing Volume Cloning Alpha for Kubernetes&lt;/li&gt;
&lt;li&gt;Automated High Availability in Kubeadm&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Release team&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This release is made possible through the efforts of hundreds of individuals who contributed both technical and non-technical content. Special thanks to the &lt;a href=&#34;https://git.k8s.io/sig-release/releases/release-1.15/release_team.md&#34; target=&#34;_blank&#34;&gt;release team&lt;/a&gt; led by Claire Laurence, Senior Technical Program Manager at Pivotal Software. The 38 individuals on the release team coordinated many aspects of the release, from documentation to testing, validation, and feature completeness.&lt;/p&gt;

&lt;p&gt;As the Kubernetes community has grown, our release process represents an amazing demonstration of collaboration in open source software development. Kubernetes continues to gain new users at a rapid clip. This growth creates a positive feedback cycle where more contributors commit code creating a more vibrant ecosystem. Kubernetes has had over &lt;a href=&#34;https://k8s.devstats.cncf.io/d/24/overall-project-statistics?orgId=1&#34; target=&#34;_blank&#34;&gt;32,000 individual contributors&lt;/a&gt; to date and an active community of more than 66,000 people.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Project Velocity&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The CNCF has continued refining DevStats, an ambitious project to visualize the myriad contributions that go into the project. &lt;a href=&#34;https://devstats.k8s.io&#34; target=&#34;_blank&#34;&gt;K8s DevStats&lt;/a&gt; illustrates the breakdown of contributions from major company contributors, as well as an impressive set of preconfigured reports on everything from individual contributors to pull request lifecycle times. On average over the past year, &lt;a href=&#34;https://k8s.devstats.cncf.io/d/11/companies-contributing-in-repository-groups?orgId=1&amp;amp;from=now-1y&amp;amp;to=now&#34; target=&#34;_blank&#34;&gt;379 different companies and over 2,715 individuals&lt;/a&gt; contribute to Kubernetes each month. &lt;a href=&#34;https://k8s.devstats.cncf.io/d/11/companies-contributing-in-repository-groups?orgId=1&amp;amp;var-period=m&amp;amp;var-repogroup_name=All&#34; target=&#34;_blank&#34;&gt;Check out DevStats&lt;/a&gt; to learn more about the overall velocity of the Kubernetes project and community.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;User Highlights&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Established, global organizations are using &lt;a href=&#34;https://kubernetes.io/case-studies/&#34; target=&#34;_blank&#34;&gt;Kubernetes in production&lt;/a&gt; at massive scale. Recently published user stories from the community include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;China Unicom&lt;/strong&gt; is using Kubernetes to &lt;a href=&#34;https://kubernetes.io/case-studies/chinaunicom/&#34; target=&#34;_blank&#34;&gt;increase their resource utilization 20-50%&lt;/a&gt;, lowering IT infrastructure costs, and cutting deployment time from hours to 10-15 minutes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The City of Montreal&lt;/strong&gt; is using Kubernetes to &lt;a href=&#34;https://kubernetes.io/case-studies/city-of-montreal/&#34; target=&#34;_blank&#34;&gt;decrease deployments from months to hours&lt;/a&gt; and run 200 application components on 8 machines with 5 people operating Kubernetes clusters.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SLAMTEC&lt;/strong&gt; is using Kubernetes along with other CNCF projects to achiever &lt;a href=&#34;https://kubernetes.io/case-studies/slamtec/&#34; target=&#34;_blank&#34;&gt;18+ months of 100% uptime&lt;/a&gt; saving 50% time spent on troubleshooting and debugging and 30% time savings on CI/CD efforts.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ThredUP&lt;/strong&gt; has decreased deployment time by &lt;a href=&#34;https://kubernetes.io/case-studies/thredup/&#34; target=&#34;_blank&#34;&gt;about 50% on average for key services&lt;/a&gt; and has shrunk lead time for deployment to under 20 minutes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Is Kubernetes helping your team? &lt;a href=&#34;https://docs.google.com/a/google.com/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform&#34; target=&#34;_blank&#34;&gt;Share your story&lt;/a&gt; with the community.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Ecosystem Updates&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Kubernetes recently celebrated its &lt;a href=&#34;https://www.cncf.io/blog/2019/06/06/reflections-on-the-fifth-anniversary-of-kubernetes/&#34; target=&#34;_blank&#34;&gt;five-year anniversary&lt;/a&gt; at KubeCon + CloudNativeCon Barcelona&lt;/li&gt;
&lt;li&gt;The &lt;a href=&#34;https://www.cncf.io/certification/expert/cka/&#34; target=&#34;_blank&#34;&gt;Certified Kubernetes Administrator (CKA) exam&lt;/a&gt; has become one of the most popular Linux Foundation certifications to date with over 9,000 registrations and over 1,700 individuals that passed and received the certification.&lt;/li&gt;
&lt;li&gt;Coming off the heels of a successful &lt;a href=&#34;https://events.linuxfoundation.org/events/kubecon-cloudnativecon-europe-2019/&#34; target=&#34;_blank&#34;&gt;KubeCon + CloudNativeCon Europe 2019&lt;/a&gt;, the CNCF announced it has over 400 members with a 130 percent year-over-year growth rate.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;KubeCon&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The world’s largest Kubernetes gathering, KubeCon + CloudNativeCon is coming to &lt;a href=&#34;https://www.lfasiallc.com/events/kubecon-cloudnativecon-china-2019/&#34; target=&#34;_blank&#34;&gt;Shanghai&lt;/a&gt; (co-located with Open Source Summit) from June 24-26, 2019 and &lt;a href=&#34;https://events.linuxfoundation.org/events/kubecon-cloudnativecon-north-america-2019/&#34; target=&#34;_blank&#34;&gt;San Diego&lt;/a&gt; from November 18-21. These conferences will feature technical sessions, case studies, developer deep dives, salons, and more! &lt;a href=&#34;https://www.cncf.io/community/kubecon-cloudnativecon-events/&#34; target=&#34;_blank&#34;&gt;Register today&lt;/a&gt;!&lt;/p&gt;

&lt;h2 id=&#34;webinar&#34;&gt;&lt;strong&gt;Webinar&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Join members of the Kubernetes 1.15 release team on July 23 at 10am PDT to learn about the major features in this release. Register &lt;a href=&#34;https://zoom.us/webinar/register/8415609575308/WN_AtjsGjz5TRqOsLrEFTWlJQ&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Get Involved&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The simplest way to get involved with Kubernetes is by joining one of the many &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/sig-list.md&#34; target=&#34;_blank&#34;&gt;Special Interest Groups&lt;/a&gt; (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/communication&#34; target=&#34;_blank&#34;&gt;community meeting&lt;/a&gt;, and through the channels below. Thank you for your continued feedback and support.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Follow us on Twitter &lt;a href=&#34;https://twitter.com/kubernetesio&#34; target=&#34;_blank&#34;&gt;@Kubernetesio&lt;/a&gt; for latest updates&lt;/li&gt;
&lt;li&gt;Join the community discussion on &lt;a href=&#34;https://discuss.kubernetes.io/&#34; target=&#34;_blank&#34;&gt;Discuss&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Join the community on &lt;a href=&#34;http://slack.k8s.io/&#34; target=&#34;_blank&#34;&gt;Slack&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Post questions (or answer questions) on &lt;a href=&#34;http://stackoverflow.com/questions/tagged/kubernetes&#34; target=&#34;_blank&#34;&gt;Stack Overflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Share your Kubernetes &lt;a href=&#34;https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform&#34; target=&#34;_blank&#34;&gt;story&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Join us at the Contributor Summit in Shanghai</title>
      <link>https://kubernetes.io/blog/2019/06/12/join-us-at-the-contributor-summit-in-shanghai/</link>
      <pubDate>Wed, 12 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/06/12/join-us-at-the-contributor-summit-in-shanghai/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Josh Berkus (Red Hat)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2019-
06-11-contributor-summit-shanghai/panel.png&#34; alt=&#34;Picture of contributor panel at 2018 Shanghai contributor summit.  Photo by Josh Berkus, licensed CC-BY 4.0&#34; /&gt;&lt;/p&gt;

&lt;p&gt;For the second year, we will have &lt;a href=&#34;https://www.lfasiallc.com/events/contributors-summit-china-2019/&#34; target=&#34;_blank&#34;&gt;a Contributor Summit event&lt;/a&gt; the day before &lt;a href=&#34;https://events.linuxfoundation.cn/events/kubecon-cloudnativecon-china-2019/&#34; target=&#34;_blank&#34;&gt;KubeCon China&lt;/a&gt; in Shanghai. If you already contribute to Kubernetes or would like to contribute, please consider attending and &lt;a href=&#34;https://www.lfasiallc.com/events/contributors-summit-china-2019/register/&#34; target=&#34;_blank&#34;&gt;register&lt;/a&gt;. The Summit will be held June 24th, at the Shanghai Expo Center (the same location where KubeCon will take place), and will include a Current Contributor Day as well as the New Contributor Workshop and the Documentation Sprints.&lt;/p&gt;

&lt;h3 id=&#34;current-contributor-day&#34;&gt;Current Contributor Day&lt;/h3&gt;

&lt;p&gt;After last year&amp;rsquo;s Contributor Day, our team received feedback that many of our contributors in Asia and Oceania would like content for current contributors as well. As such, we have added a Current Contributor track to the schedule.&lt;/p&gt;

&lt;p&gt;While we do not yet have a full schedule up, the topics covered in the current contributor track will include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;How to write a KEP (Kubernetes Enhancement Proposal)&lt;/li&gt;
&lt;li&gt;Codebase and repository review&lt;/li&gt;
&lt;li&gt;Local Build &amp;amp; Test troubleshooting session&lt;/li&gt;
&lt;li&gt;Guide to Non-Code Contribution opportunities&lt;/li&gt;
&lt;li&gt;SIG-Azure face-to-face meeting&lt;/li&gt;
&lt;li&gt;SIG-Scheduling face-to-face meeting&lt;/li&gt;
&lt;li&gt;Other SIG face-to-face meetings as we confirm them&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The schedule will be on &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/events/2019/06-contributor-summit&#34; target=&#34;_blank&#34;&gt;the Community page&lt;/a&gt; once it is complete.&lt;/p&gt;

&lt;p&gt;If your SIG wants to have a face-to-face meeting at Kubecon Shanghai, please contact &lt;a href=&#34;mailto:jberkus@redhat.com&#34; target=&#34;_blank&#34;&gt;Josh Berkus&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;new-contributor-workshop&#34;&gt;New Contributor Workshop&lt;/h3&gt;

&lt;p&gt;Students at &lt;a href=&#34;https://kubernetes.io/blog/2018/12/05/new-contributor-workshop-shanghai/&#34;&gt;last year&amp;rsquo;s New Contributor Workshop&lt;/a&gt; (NCW) found it to be extremely valuable, and the event helped to orient a few of the many Asian and Pacific developers looking to participate in the Kubernetes community.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;It&amp;rsquo;s a one-stop-shop for becoming familiar with the community.&amp;rdquo; said one participant.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;If you have not contributed to Kubernetes before, or have only done one or two things, please consider &lt;a href=&#34;https://www.lfasiallc.com/events/contributors-summit-china-2019/register/&#34; target=&#34;_blank&#34;&gt;enrolling&lt;/a&gt; in the NCW.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Got to know the process from signing CLA to PR and made friends with other contributors.&amp;rdquo; said another.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;documentation-sprints&#34;&gt;Documentation Sprints&lt;/h3&gt;

&lt;p&gt;Both old and new contributors on our Docs Team will spend a day both improving our documentation and translating it into other languages. If you are interested in having better documentation, fully localized into Chinese and other languages, please &lt;a href=&#34;https://www.lfasiallc.com/events/contributors-summit-china-2019/register/&#34; target=&#34;_blank&#34;&gt;sign up&lt;/a&gt; to help with the Doc Sprints.&lt;/p&gt;

&lt;h3 id=&#34;before-you-attend&#34;&gt;Before you attend&lt;/h3&gt;

&lt;p&gt;Regardless of where you participate, everyone at the Contributor Summit should &lt;a href=&#34;https://git.k8s.io/community/CLA.md#the-contributor-license-agreement&#34; target=&#34;_blank&#34;&gt;sign the Kubernetes Contributor License Agreement&lt;/a&gt; (CLA) before coming to the conference. You should also bring a laptop suitable for working on documentation or code development.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kyma - extend and build on Kubernetes with ease</title>
      <link>https://kubernetes.io/blog/2019/05/23/kyma-extend-and-build-on-kubernetes-with-ease/</link>
      <pubDate>Thu, 23 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/05/23/kyma-extend-and-build-on-kubernetes-with-ease/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lukasz Gornicki (SAP)&lt;/p&gt;

&lt;p&gt;According to this recently completed &lt;a href=&#34;https://www.cncf.io/blog/2018/08/29/cncf-survey-use-of-cloud-native-technologies-in-production-has-grown-over-200-percent/&#34; target=&#34;_blank&#34;&gt;CNCF Survey&lt;/a&gt;, the adoption rate of Cloud Native technologies in production is growing rapidly. Kubernetes is at the heart of this technological revolution. Naturally, the growth of cloud native technologies has been accompanied by the growth of the ecosystem that surrounds it. Of course, the complexity of cloud native technologies have increased as well. Just google for the phrase “Kubernetes is hard”, and you’ll get plenty of articles that explain this complexity problem. The best thing about the CNCF community is that problems like this can be solved by smart people building new tools to enable Kubernetes users: Projects like Knative and its &lt;a href=&#34;https://github.com/knative/build&#34; target=&#34;_blank&#34;&gt;Build resource&lt;/a&gt; extension, for example, serve to reduce complexity across a range of scenarios. Even though increasing complexity might seem like the most important issue to tackle, it is not the only challenge you face when transitioning to Cloud Native.&lt;/p&gt;

&lt;h2 id=&#34;problems-to-solve&#34;&gt;Problems to solve&lt;/h2&gt;

&lt;h3 id=&#34;picking-the-right-technology-is-hard&#34;&gt;Picking the right technology is hard&lt;/h3&gt;

&lt;p&gt;Now that you understand Kubernetes, your teams are trained and you’ve started building applications on top, it’s time to face a new layer of challenges. Cloud native doesn’t just mean deploying a platform for developers to build on top of. Developers also need storage, backup, monitoring, logging and a service mesh to enforce policies upon data in transit. Each of these individual systems must be properly configured and deployed, as well as logged, monitored and backed up on its own. The CNCF is here to help. We provide a &lt;a href=&#34;https://landscape.cncf.io/&#34; target=&#34;_blank&#34;&gt;landscape&lt;/a&gt; overview of all cloud-native technologies, but the list is huge and can be overwhelming.&lt;/p&gt;

&lt;p&gt;This is where &lt;a href=&#34;http://kyma-project.io&#34; target=&#34;_blank&#34;&gt;Kyma&lt;/a&gt; will make your life easier. Its mission statement is to enable a flexible and easy way of extending applications.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2019-05-23-Kyma-extend-and-build-on-kubernetes-with-ease/kyma-center.png&#34; width=&#34;40%&#34; alt=&#34;Kyma in center&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This project is designed to give you the tools you need to be able to write an end-to-end, production-grade cloud native application. &lt;a href=&#34;https://github.com/kyma-project/kyma/&#34; target=&#34;_blank&#34;&gt;Kyma&lt;/a&gt; was donated to the open-source community by &lt;a href=&#34;https://www.sap.com&#34; target=&#34;_blank&#34;&gt;SAP&lt;/a&gt;; a company with great experience in writing production-grade cloud native applications. That’s why we’re so excited to &amp;ndash; &lt;a href=&#34;https://twitter.com/kymaproject/status/1121426458243678209&#34; target=&#34;_blank&#34;&gt;announce&lt;/a&gt; the first major release of &lt;a href=&#34;https://github.com/kyma-project/kyma/releases/tag/1.0.0&#34; target=&#34;_blank&#34;&gt;Kyma 1.0&lt;/a&gt;!&lt;/p&gt;

&lt;h3 id=&#34;deciding-on-the-path-from-monolith-to-cloud-native-is-hard&#34;&gt;Deciding on the path from monolith to cloud-native is hard&lt;/h3&gt;

&lt;p&gt;Try Googling &lt;code&gt;monolith to cloud native&lt;/code&gt; or &lt;code&gt;monolith to microservices&lt;/code&gt; and you’ll get a list of plenty of talks and papers that tackle this challenge. There are many different paths available for migrating a monolith to the cloud, and our experience has taught us to be quite opinionated in this area. First, let&amp;rsquo;s answer the question of why you’d want to move from monolith to cloud native. The goals driving this move are typically:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Increased scalability.&lt;/li&gt;
&lt;li&gt;Faster implementation of new features.&lt;/li&gt;
&lt;li&gt;More flexible approach to extensibility.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You do not have to rewrite your monolith to achieve these goals. Why spend all that time rewriting functionality that you already have? Just focus on enabling your monolith to support &lt;a href=&#34;https://en.wikipedia.org/wiki/Event-driven_architecture&#34; target=&#34;_blank&#34;&gt;event-driven architecture&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;how-does-kyma-solve-your-challenges&#34;&gt;How does Kyma solve your challenges?&lt;/h2&gt;

&lt;h3 id=&#34;what-is-kyma&#34;&gt;What is Kyma?&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://kyma-project.io/docs/root/kyma/#overview-overview&#34; target=&#34;_blank&#34;&gt;Kyma&lt;/a&gt; runs on Kubernetes and consists of a number of different components, three of which are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kyma-project.io/docs/components/application-connector/&#34; target=&#34;_blank&#34;&gt;Application connector&lt;/a&gt; that you can use to connect any application with a Kubernetes cluster and expose its APIs and Events through the &lt;a href=&#34;https://github.com/kubernetes-incubator/service-catalog&#34; target=&#34;_blank&#34;&gt;Kubernetes Service Catalog&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kyma-project.io/docs/components/serverless/&#34; target=&#34;_blank&#34;&gt;Serverless&lt;/a&gt; which enables you to easily write extensions for your application. Your function code can be triggered by API calls and also by events coming from external system. You can also securely call back the integrated system from your function.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kyma-project.io/docs/components/service-catalog/&#34; target=&#34;_blank&#34;&gt;Service Catalog&lt;/a&gt; is here to expose integrated systems. This integration also enables you to use services from hyperscalers like Azure, AWS or Google Cloud. &lt;a href=&#34;https://kyma-project.io/docs/components/service-catalog/#service-brokers-service-brokers&#34; target=&#34;_blank&#34;&gt;Kyma&lt;/a&gt; allows for easy integration of official service brokers maintained by Microsoft and Google.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2019-05-23-Kyma-extend-and-build-on-kubernetes-with-ease/ac-s-sc.svg&#34; alt=&#34;core components&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You can watch &lt;a href=&#34;https://www.youtube.com/watch?v=wJzVWFGkiKk&#34; target=&#34;_blank&#34;&gt;this video&lt;/a&gt; for a short overview of Kyma key features that is based on a real demo scenario.&lt;/p&gt;

&lt;h3 id=&#34;we-picked-the-right-technologies-for-you&#34;&gt;We picked the right technologies for you&lt;/h3&gt;

&lt;p&gt;You can provide reliable extensibility in a project like Kyma only if it is properly monitored and configured. We decided not to reinvent the wheel. There are many great projects in the CNCF landscape, most with huge communities behind them. We decided to pick the best ones and glue them all together in Kyma. You can see the same architecture diagram that is above but with a focus on the projects we put together to create Kyma:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2019-05-23-Kyma-extend-and-build-on-kubernetes-with-ease/arch.png&#34; width=&#34;70%&#34; alt=&#34;Kyma architecture&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Monitoring and alerting is based on &lt;a href=&#34;https://prometheus.io/&#34; target=&#34;_blank&#34;&gt;Prometheus&lt;/a&gt; and &lt;a href=&#34;https://grafana.com/&#34; target=&#34;_blank&#34;&gt;Grafana&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Logging is based on &lt;a href=&#34;https://grafana.com/loki&#34; target=&#34;_blank&#34;&gt;Loki&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Eventing uses &lt;a href=&#34;https://github.com/knative/eventing/&#34; target=&#34;_blank&#34;&gt;Knative&lt;/a&gt; and &lt;a href=&#34;https://nats.io/&#34; target=&#34;_blank&#34;&gt;NATS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Asset management uses &lt;a href=&#34;https://min.io/&#34; target=&#34;_blank&#34;&gt;Minio&lt;/a&gt; as a storage&lt;/li&gt;
&lt;li&gt;Service Mesh is based on &lt;a href=&#34;https://istio.io/&#34; target=&#34;_blank&#34;&gt;Istio&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Tracing is done with &lt;a href=&#34;https://www.jaegertracing.io/&#34; target=&#34;_blank&#34;&gt;Jaeger&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Authentication is supported by &lt;a href=&#34;https://github.com/dexidp/dex&#34; target=&#34;_blank&#34;&gt;dex&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You don&amp;rsquo;t have to integrate these tools: We made sure they all play together well, and are always up to date ( Kyma is already using Istio 1.1). With our custom &lt;a href=&#34;https://github.com/kyma-project/kyma/blob/master/docs/kyma/04-02-local-installation.md&#34; target=&#34;_blank&#34;&gt;Installer&lt;/a&gt; and &lt;a href=&#34;https://helm.sh/&#34; target=&#34;_blank&#34;&gt;Helm&lt;/a&gt; charts, we enabled easy installation and easy upgrades to new versions of Kyma.&lt;/p&gt;

&lt;h3 id=&#34;do-not-rewrite-your-monoliths&#34;&gt;Do not rewrite your monoliths&lt;/h3&gt;

&lt;p&gt;Rewriting is hard, costs a fortune, and in most cases is not needed. At the end of the day, what you need is to be able to write and put new features into production quicker. You can do it by connecting your monolith to Kyma using the &lt;a href=&#34;https://kyma-project.io/docs/components/application-connector&#34; target=&#34;_blank&#34;&gt;Application Connector&lt;/a&gt;. In short, this component makes sure that:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;You can securely call back the registered monolith without the need to take care of authorization, as the Application Connector handles this.&lt;/li&gt;
&lt;li&gt;Events sent from your monolith get securely to the Kyma Event Bus.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;At the moment, your monolith can consume three different types of services: REST (with &lt;a href=&#34;https://www.openapis.org/&#34; target=&#34;_blank&#34;&gt;OpenAPI&lt;/a&gt; specification)  and OData (with Entity Data Model specification) for synchronous communication, and for asynchronous communication you can register a catalog of events based on &lt;a href=&#34;https://www.asyncapi.com/&#34; target=&#34;_blank&#34;&gt;AsyncAPI&lt;/a&gt; specification. Your events are later delivered internally using &lt;a href=&#34;https://nats.io/&#34; target=&#34;_blank&#34;&gt;NATS Streaming&lt;/a&gt; channel with &lt;a href=&#34;https://github.com/knative/eventing/&#34; target=&#34;_blank&#34;&gt;Knative eventing&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Once your monolith&amp;rsquo;s services are connected, you can provision them in selected Namespaces thanks to the previously mentioned &lt;a href=&#34;https://kyma-project.io/docs/components/service-catalog/&#34; target=&#34;_blank&#34;&gt;Service Catalog&lt;/a&gt; integration. You, as a developer, can go to the catalog and see a list of all the services you can consume. There are services from your monolith, and services from other 3rd party providers thanks to registered Service Brokers, like &lt;a href=&#34;https://github.com/Azure/open-service-broker-azure&#34; target=&#34;_blank&#34;&gt;Azure&amp;rsquo;s OSBA&lt;/a&gt;. It is the one single place with everything you need. If you want to stand up a new application, everything you need is already available in Kyma.&lt;/p&gt;

&lt;h3 id=&#34;finally-some-code&#34;&gt;Finally some code&lt;/h3&gt;

&lt;p&gt;Check out some code I had to write to integrate a monolith with Azure services. I wanted to understand the sentiments shared by customers under the product&amp;rsquo;s review section. On every event with a review comment, I wanted to use machine learning to call a sentiments analysis service, and in the case of a negative comment, I wanted to store it in a database for later review. This is the code of a function created thanks to our &lt;a href=&#34;https://kyma-project.io/docs/components/serverless&#34; target=&#34;_blank&#34;&gt;Serverless&lt;/a&gt; component. Pay attention to my code comments:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;You can watch &lt;a href=&#34;https://www.youtube.com/watch?v=wJzVWFGkiKk&#34; target=&#34;_blank&#34;&gt;this&lt;/a&gt; short video for a full demo of sentiment analysis function.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-js&#34; data-lang=&#34;js&#34;&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;/* It is a function powered by NodeJS runtime so I have to import some necessary dependencies. I choosed Azure&amp;#39;s CosmoDB that is a Mongo-like database, so I could use a MongoClient */&lt;/span&gt;
&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;const&lt;/span&gt; axios &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; require(&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;axios&amp;#34;&lt;/span&gt;);
&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;const&lt;/span&gt; MongoClient &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; require(&lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;mongodb&amp;#39;&lt;/span&gt;).MongoClient;

module.exports &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; { main&lt;span style=&#34;color:#666&#34;&gt;:&lt;/span&gt; async &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;function&lt;/span&gt; (event, context) {
    &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;/* My function was triggered because it was subscribed to customer review event. I have access to the payload of the event. */&lt;/span&gt;
    &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;let&lt;/span&gt; negative &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; await isNegative(event.data.comment)
    
    &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;if&lt;/span&gt; (negative) {
      console.log(&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;Customer sentiment is negative:&amp;#34;&lt;/span&gt;, event.data)
      await mongoInsert(event.data)
    } &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;else&lt;/span&gt; {
      console.log(&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;This positive comment was not saved:&amp;#34;&lt;/span&gt;, event.data) 
    }
}}

&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;/* Like in case of isNegative function, I focus of usage of the MongoClient API. The necessary information about the database location and an authorization needed to call it is injected into my function and I just need to pick a proper environment variable. */&lt;/span&gt;
async &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;function&lt;/span&gt; mongoInsert(data) {

    &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;try&lt;/span&gt; {
          client &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; await MongoClient.connect(process.env.connectionString, { useNewUrlParser&lt;span style=&#34;color:#666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;true&lt;/span&gt; });
          db &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; client.db(&lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;mycommerce&amp;#39;&lt;/span&gt;);
          &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;const&lt;/span&gt; collection &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; db.collection(&lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;comments&amp;#39;&lt;/span&gt;);
          &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;return&lt;/span&gt; await collection.insertOne(data);
    } &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;finally&lt;/span&gt; {
      client.close();
    }
}
&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;/* This function calls Azure&amp;#39;s Text Analytics service to get information about the sentiment. Notice process.env.textAnalyticsEndpoint and process.env.textAnalyticsKey part. When I wrote this function I didn&amp;#39;t have to go to Azure&amp;#39;s console to get these details. I had these variables automatically injected into my function thanks to our integration with Service Catalog and our Service Binding Usage controller that pairs the binding with a function. */&lt;/span&gt;
async &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;function&lt;/span&gt; isNegative(comment) {
    &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;let&lt;/span&gt; response &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; await axios.post(&lt;span style=&#34;color:#b44&#34;&gt;`&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;${&lt;/span&gt;process.env.textAnalyticsEndpoint&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;/sentiment`&lt;/span&gt;,
      { documents&lt;span style=&#34;color:#666&#34;&gt;:&lt;/span&gt; [{ id&lt;span style=&#34;color:#666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;1&amp;#39;&lt;/span&gt;, text&lt;span style=&#34;color:#666&#34;&gt;:&lt;/span&gt; comment }] }, {headers&lt;span style=&#34;color:#666&#34;&gt;:&lt;/span&gt;{ &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;Ocp-Apim-Subscription-Key&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;:&lt;/span&gt; process.env.textAnalyticsKey }})
    &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;return&lt;/span&gt; response.data.documents[&lt;span style=&#34;color:#666&#34;&gt;0&lt;/span&gt;].score &lt;span style=&#34;color:#666&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;0.5&lt;/span&gt;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Thanks to Kyma, I don&amp;rsquo;t have to worry about the infrastructure around my function. As I mentioned, I have all the tools needed in Kyma, and they are integrated together. I can quickly get access to my logs through &lt;a href=&#34;https://grafana.com/loki&#34; target=&#34;_blank&#34;&gt;Loki&lt;/a&gt;, and I can quickly get access to a preconfigured Grafana dashboard to see the metrics of my Lambda delivered thanks to &lt;a href=&#34;https://prometheus.io/&#34; target=&#34;_blank&#34;&gt;Prometheus&lt;/a&gt; and &lt;a href=&#34;https://istio.io/&#34; target=&#34;_blank&#34;&gt;Istio&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2019-05-23-Kyma-extend-and-build-on-kubernetes-with-ease/grafana-lambda.png&#34; width=&#34;70%&#34; alt=&#34;Grafana with preconfigured lambda dashboard&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Such an approach gives you a lot of flexibility in adding new functionality. It also gives you time to rethink the need to rewrite old functions.&lt;/p&gt;

&lt;h2 id=&#34;contribute-and-give-feedback&#34;&gt;Contribute and give feedback&lt;/h2&gt;

&lt;p&gt;Kyma is an open source project, and we would love help it grow. The way that happens is with your help. After reading this post, you already know that we don&amp;rsquo;t want to reinvent the wheel. We stay true to this approach in our work model, which enables community contributors. We work in &lt;a href=&#34;https://github.com/kyma-project/community/tree/master/contributing&#34; target=&#34;_blank&#34;&gt;Special Interest Groups&lt;/a&gt; and have publicly recorded meeting that you can join any time, so we have a setup similar to what you know from Kubernetes itself.
Feel free to share also your feedback with us, through &lt;a href=&#34;https://twitter.com/kymaproject&#34; target=&#34;_blank&#34;&gt;Twitter&lt;/a&gt; or &lt;a href=&#34;http://slack.kyma-project.io&#34; target=&#34;_blank&#34;&gt;Slack&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes, Cloud Native, and the Future of Software</title>
      <link>https://kubernetes.io/blog/2019/05/17/kubernetes-cloud-native-and-the-future-of-software/</link>
      <pubDate>Fri, 17 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/05/17/kubernetes-cloud-native-and-the-future-of-software/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Brian Grant (Google), Jaice Singer DuMars (Google)&lt;/p&gt;

&lt;h1 id=&#34;kubernetes-cloud-native-and-the-future-of-software&#34;&gt;Kubernetes, Cloud Native, and the Future of Software&lt;/h1&gt;

&lt;p&gt;Five years ago this June, Google Cloud announced a new application management technology called Kubernetes. It began with a &lt;a href=&#34;https://github.com/kubernetes/kubernetes/commit/2c4b3a562ce34cddc3f8218a2c4d11c7310e6d56&#34; target=&#34;_blank&#34;&gt;simple open source commit&lt;/a&gt;, followed the next day by a &lt;a href=&#34;https://cloudplatform.googleblog.com/2014/06/an-update-on-container-support-on-google-cloud-platform.html&#34; target=&#34;_blank&#34;&gt;one-paragraph blog mention&lt;/a&gt; around container support. Later in the week, Eric Brewer &lt;a href=&#34;https://www.youtube.com/watch?v=YrxnVKZeqK8&#34; target=&#34;_blank&#34;&gt;talked about Kubernetes for the first time&lt;/a&gt; at DockerCon. And soon the world was watching.&lt;/p&gt;

&lt;p&gt;We’re delighted to see Kubernetes become core to the creation and operation of modern software, and thereby a key part of the global economy. To us, the success of Kubernetes represents even more: A business transition with truly worldwide implications, thanks to the unprecedented cooperation afforded by the open source software movement.&lt;/p&gt;

&lt;p&gt;Like any important technology, Kubernetes has become about more than just itself; it has positively affected the environment in which it arose, changing how software is deployed at scale, how work is done, and how corporations engage with big open-source projects.&lt;/p&gt;

&lt;p&gt;Let’s take a look at how this happened, since it tells us a lot about where we are today, and what might be happening next.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Beginnings&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The most important precursor to Kubernetes was the rise of application containers. Docker, the first tool to really make containers usable by a broad audience, began as an open source project in 2013. By containerizing an application, developers could achieve easier language runtime management, deployment, and scalability. This triggered a sea change in the application ecosystem. Containers made stateless applications easily scalable and provided an immutable deployment artifact that drastically reduced the number of variables previously encountered between test and production systems.&lt;/p&gt;

&lt;p&gt;While containers presented strong stand-alone value for developers, the next challenge was how to deliver and manage services, applications, and architectures that spanned multiple containers and multiple hosts.&lt;/p&gt;

&lt;p&gt;Google had already encountered similar issues within its own IT infrastructure. Running the world’s most popular search engine (and several other products with millions of users) lead to early innovation around, and adoption of, containers. Kubernetes was inspired by Borg, Google’s internal platform for scheduling and managing the hundreds of millions, and eventually billions, of containers that implement all of our services.&lt;/p&gt;

&lt;p&gt;Kubernetes is more than just “Borg, for everyone” It distills the most successful architectural and API patterns of prior systems and couples them with load balancing, authorization policies, and other features needed to run and manage applications at scale. This in turn provides the groundwork for cluster-wide abstractions that allow true portability across clouds.&lt;/p&gt;

&lt;p&gt;The November 2014 &lt;a href=&#34;https://cloudplatform.googleblog.com/2014/11/google-cloud-platform-live-introducing-container-engine-cloud-networking-and-much-more.html&#34; target=&#34;_blank&#34;&gt;alpha launch&lt;/a&gt; of Google Cloud’s &lt;a href=&#34;https://cloud.google.com/kubernetes-engine/&#34; target=&#34;_blank&#34;&gt;Google Kubernetes Engine (GKE)&lt;/a&gt; introduced managed Kubernetes. There was an explosion of innovation around Kubernetes, and companies from the enterprise down to the startup saw barriers to adoption fall away. Google, Red Hat, and others in the community increased their investment of people, experience, and architectural know-how to ensure it was ready for increasingly mission-critical workloads. The response was a wave of adoption that swept it to the forefront of the crowded container management space.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Rise of Cloud Native&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Every enterprise, regardless of its core business, is embracing more digital technology. The ability to rapidly adapt is fundamental to continued growth and competitiveness. Cloud-native technologies, and especially Kubernetes, arose to meet this need, providing the automation and observability necessary to manage applications at scale and with high velocity. Organizations previously constrained to quarterly deployments of critical applications can now deploy safely multiple times a day.&lt;/p&gt;

&lt;p&gt;Kubernetes’s declarative, API-driven infrastructure empowers teams to operate independently, and enables them to focus on their business objectives. An inevitable cultural shift in the workplace has come from enabling greater autonomy and productivity and reducing the toil of development teams.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Increased engagement with open source&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The ability for teams to rapidly develop and deploy new software creates a virtuous cycle of success for companies and technical practitioners alike. Companies have started to recognize that contributing back to the software projects they use not only improves the performance of the software for their use cases, but also builds critical skills and creates challenging opportunities that help them attract and retain new developers.&lt;/p&gt;

&lt;p&gt;The Kubernetes project in particular curates a collaborative culture that encourages contribution and sharing of learning and development with the community. This fosters a positive-sum ecosystem that benefits both contributors and end-users equally.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What’s Next?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Where Kubernetes is concerned, five years seems like an eternity. That says much about the collective innovation we’ve seen in the community, and the rapid adoption of the technology.&lt;/p&gt;

&lt;p&gt;In other ways, it is just the start. New applications such as machine learning, edge computing, and the Internet of Things are finding their way into the cloud native ecosystem via projects like Kubeflow. Kubernetes is almost certain to be at the heart of their success.&lt;/p&gt;

&lt;p&gt;Kubernetes may be most successful if it becomes an invisible essential of daily life, like urban plumbing or electrical grids. True standards are dramatic, but they are also taken for granted. As Googler and KubeCon co-chair Janet Kuo said in a &lt;a href=&#34;https://www.youtube.com/watch?v=LAO7RuWwfzA&#34; target=&#34;_blank&#34;&gt;recent keynote&lt;/a&gt;, Kubernetes is going to become boring, and that’s a good thing, at least for the majority of people who don’t have to care about container management.&lt;/p&gt;

&lt;p&gt;At Google Cloud, we’re still excited about the project, and we go to work on it every day. Yet it’s all of the solutions and extensions that expand from Kubernetes that will dramatically change the world as we know it.&lt;/p&gt;

&lt;p&gt;So, as we all celebrate the continued success of Kubernetes, remember to take the time and thank someone you see helping make the community better. It’s up to all of us to foster a cloud-native ecosystem that prizes the efforts of everyone who helps maintain and nurture the work we do together.&lt;/p&gt;

&lt;p&gt;And, to everyone who has been a part of the global success of Kubernetes, thank you. You have changed the world.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Expanding our Contributor Workshops</title>
      <link>https://kubernetes.io/blog/2019/05/14/expanding-our-contributor-workshops/</link>
      <pubDate>Tue, 14 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/05/14/expanding-our-contributor-workshops/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Guinevere Saenger (GitHub) and Paris Pittman (Google)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt; - learn about the contributor community with us and land your first
PR! We have spots available in &lt;a href=&#34;https://events.linuxfoundation.org/events/contributor-summit-europe-2019/&#34; target=&#34;_blank&#34;&gt;Barcelona&lt;/a&gt; (registration &lt;strong&gt;closes&lt;/strong&gt; on
Wednesday May 15, so grab your spot!) and the upcoming &lt;a href=&#34;https://www.lfasiallc.com/events/contributors-summit-china-2019/&#34; target=&#34;_blank&#34;&gt;Shanghai&lt;/a&gt; Summit.
The Barcelona event is poised to be our biggest one yet, with more registered
attendees than ever before!&lt;/p&gt;

&lt;p&gt;Have you always wanted to contribute to Kubernetes, but not sure where to begin?
Have you seen our community’s many code bases and seen places to improve? We
have a workshop for you!&lt;/p&gt;

&lt;p&gt;KubeCon + CloudNativeCon Barcelona’s &lt;a href=&#34;https://events.linuxfoundation.org/events/contributor-summit-europe-2019/&#34; target=&#34;_blank&#34;&gt;new contributor workshop&lt;/a&gt; will be the
fourth one of its kind, and we’re really looking forward to it! The workshop was
kickstarted last year at KubeConEU in Copenhagen, and so far we have taken it to
Shanghai and Seattle, and now Barcelona, as well as some non-KubeCon locations.
We are constantly updating and improving the workshop content based on feedback
from past sessions. This time, we’re breaking up the participants by their
experience and comfort level with open source and Kubernetes. We’ll have
developer setup and project workflow support for folks entirely new to open
source and Kubernetes as part of the 101 track, and hope to set up each
participant with their very own first issue to work on. In the 201 track, we
will have a codebase walkthrough and local development and test demonstration
for folks who have a bit more experience in open source but may be unfamiliar
with our community’s development tools. For both tracks, you will have a chance
to get your hands dirty and have some fun. Because not every contributor works
with code, and not every contribution is technical, we will spend the beginning
of the workshop learning how our project is structured and organized, where to
find the right people, and where to get help when stuck.&lt;/p&gt;

&lt;h2 id=&#34;mentoring-opportunities&#34;&gt;Mentoring Opportunities&lt;/h2&gt;

&lt;p&gt;We will also bring back the SIG Meet-and-Greet where new contributors will have
a chance to mingle with current contributors, perhaps find their dream SIG,
learn what exciting areas they can help with, gain mentors, and make friends.&lt;/p&gt;

&lt;p&gt;PS - there are also two mentoring sessions DURING KubeCon + CloudNativeCon on
Thursday, May 23. &lt;a href=&#34;http://bit.ly/mentor-bcn&#34; target=&#34;_blank&#34;&gt;Sign up here&lt;/a&gt;. 60% of the attendees during the
Seattle event asked contributor questions.&lt;/p&gt;

&lt;h2 id=&#34;past-attendee-story-vallery-lancy-engineer-at-lyft&#34;&gt;Past Attendee Story - Vallery Lancy, Engineer at Lyft&lt;/h2&gt;

&lt;p&gt;We talked to a few of our past participants in a series of interviews that we
will publish throughout the course of the year. In our first two clips, we meet
Vallery Lancy, an Engineer at Lyft and one of 75 attendees at our recent Seattle
edition of the workshop. She was poking around in the community for a while to
see where she could jump in.&lt;/p&gt;

&lt;p&gt;Watch Vallery talk about her experience here:
&lt;center&gt;&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/uKg5WUcl6WU&#34; frameborder=&#34;0&#34; allow=&#34;autoplay; encrypted-media&#34; allowfullscreen&gt;&lt;/iframe&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;What does Vallery say to folks curious about the workshops, or those attending
the Barcelona edition?&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/niHiem7JmPA&#34; frameborder=&#34;0&#34; allow=&#34;autoplay; encrypted-media&#34; allowfullscreen&gt;&lt;/iframe&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Be like Vallery and hundreds of previous New Contributor Workshop attendees:
join us in Barcelona (or Shanghai - or San Diego!) for a unique experience
without digging into our documentation! Have the opportunity to meet with the
experts and go step by step into your journey with your peers around you. We’re
looking forward to seeing you there! &lt;a href=&#34;https://events.linuxfoundation.org/events/contributor-summit-europe-2019/&#34; target=&#34;_blank&#34;&gt;Register here&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Cat shirts and Groundhog Day: the Kubernetes 1.14 release interview</title>
      <link>https://kubernetes.io/blog/2019/05/13/cat-shirts-and-groundhog-day-the-kubernetes-1.14-release-interview/</link>
      <pubDate>Mon, 13 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/05/13/cat-shirts-and-groundhog-day-the-kubernetes-1.14-release-interview/</guid>
      <description>
        
        
        &lt;p&gt;&lt;b&gt;Author&lt;/b&gt;: Craig Box (Google)&lt;/p&gt;

&lt;p&gt;Last week we celebrated one year of the &lt;a href=&#34;https://kubernetespodcast.com/&#34; target=&#34;_blank&#34;&gt;Kubernetes Podcast from Google&lt;/a&gt;. In this weekly show, my co-host Adam Glick and I focus on all the great things that are happening in the world of Kubernetes and Cloud Native. From the news of the week, to interviews with people in the community, we help you stay up to date on everything Kubernetes.&lt;/p&gt;

&lt;p&gt;Every few cycles we check in on the release process for Kubernetes itself.  Last year we &lt;a href=&#34;https://kubernetespodcast.com/episode/010-kubernetes-1.11/&#34; target=&#34;_blank&#34;&gt;interviewed the release managers for Kubernetes 1.11&lt;/a&gt;, and shared that transcript on the Kubernetes blog.  We got such great feedback that we wanted to share the transcript of our recent conversation with Aaron Crickenberger, the release manager for Kubernetes 1.14.&lt;/p&gt;

&lt;p&gt;As always, the canonical version can be enjoyed by listening to &lt;a href=&#34;https://kubernetespodcast.com/episode/046-kubernetes-1.14/&#34; target=&#34;_blank&#34;&gt;the podcast version&lt;/a&gt;. If you like what you hear, &lt;a href=&#34;https://kubernetespodcast.com/subscribe/&#34; target=&#34;_blank&#34;&gt;we encourage you to subscribe&lt;/a&gt;!&lt;/p&gt;

&lt;hr/&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: We like to start with our guests into digging into their backgrounds a little bit. Kubernetes is built from contributors from many different companies. You worked on Kubernetes at Samsung SDS before joining Google. Does anything change in your position in the community and the work you do, when you change companies?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;AARON CRICKENBERGER: Largely, no. I think the food&amp;rsquo;s a little bit better at the current company! But by and large, I have gotten to work with basically the same people doing basically the same thing. I cared about the community first and Google second before I joined Google, and I kind of still operate that way mostly because I believe that Google&amp;rsquo;s success depends upon the community&amp;rsquo;s success, as does everybody else who depends upon Kubernetes. A good and healthy upstream makes a good and healthy downstream.&lt;/p&gt;

&lt;p&gt;So that was largely why Samsung had me working on Kubernetes in the first place was because we thought the technology was legit. But we needed to make sure that the community and project as a whole was also legit. And so that&amp;rsquo;s why you&amp;rsquo;ve seen me continue to advocate for transparency and community empowerment throughout my tenure in Kubernetes.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: You co-founded the &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-testing&#34; target=&#34;_blank&#34;&gt;Testing SIG&lt;/a&gt;. How did you decide that that was needed, and at what stage in the process did you come to that?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;AARON CRICKENBERGER: This was very early on in the Kubernetes project. I&amp;rsquo;m actually a little hazy on specifically when it happened. But at the time, my boss, Bob Wise, worked with some folks within Google to co-found the Scalability SIG.&lt;/p&gt;

&lt;p&gt;If you remember way, way back when Kubernetes first started, there was concern over whether or not Kubernetes was performance enough. Like, I believe it officially supported something on the order of 100 nodes. And there were some who thought, that&amp;rsquo;s silly. I mean, come on, Google can do way more than that. And who in their right mind is going to use a container orchestrator that only supports 100 nodes?&lt;/p&gt;

&lt;p&gt;And of course the thing is we&amp;rsquo;re being super-conservative. We&amp;rsquo;re trying to iterate, ship early and often. And so we helped push the boundaries to make sure that Kubernetes could prove that it worked up to a thousand nodes before it was even officially supported to say, look, it already does this, we&amp;rsquo;re just trying to make sure we have all of the nuts and bolts tightened.&lt;/p&gt;

&lt;p&gt;OK, so great. We decided we needed to create a thing called a SIG in the very first place to talk about these things and make sure that we were moving in the right direction. I then turned my personal attention to testing as the next thing that I believe needed a SIG. So I believe that testing was the second SIG ever to be created for Kubernetes. It was co-founded initially with &lt;a href=&#34;https://github.com/ihmccreery&#34; target=&#34;_blank&#34;&gt;Ike McCreary&lt;/a&gt; who, at the time I believe, was an SRE for Google, and then eventually it was handed over to some folks who work in the engineering productivity part of Google where I think it aligned really well with testing&amp;rsquo;s interests.&lt;/p&gt;

&lt;p&gt;It is like &amp;ldquo;I don&amp;rsquo;t know what you people are trying to write here with Kubernetes, but I want to help you write it better, faster, and stronger&amp;rdquo;. And so I want to make sure we, as a community and as a project, are making it easier for you to write tests, easier for you to run tests, and most importantly, easier for you to act based on those test results.&lt;/p&gt;

&lt;p&gt;That came down to, let&amp;rsquo;s make sure that Kubernetes gets tested on more than just Google Cloud. That was super important to me, as somebody who operated not in Google Cloud but in other clouds. I think it really helped sell the story and build confidence in Kubernetes as something that worked effectively on multiple clouds. And I also thought it was really helpful to see SIG Testing in the community&amp;rsquo;s advocacy move us to a world today we can use test grids so that everybody see the same set of test results to understand what is allowed to prevent Kubernetes from going out the door.&lt;/p&gt;

&lt;p&gt;The process was basically just saying, let&amp;rsquo;s do it. The process was finding people who were motivated and suggesting that we meet on a recurring basis and we try to rally around a common set of work. This was sort of well before SIG governance was an official thing. And we gradually, after about a year, I think, settled on the pattern that most SIGs follow where you try to make sure you have a meeting agenda, you have a Slack channel, you have a mailing list, you discuss everything out in the open, you try to use sort a consistent set of milestones and move forward.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: A couple of things I wanted to ask about your life before Kubernetes. Why is there a &lt;a href=&#34;https://www.rockwellcollins.com/Products-and-Services/Defense/Simulation-and-Training/Training-Systems/Transportable-Black-Hawk-Operations-Simulator.aspx&#34; target=&#34;_blank&#34;&gt;Black Hawk flight simulator in a shipping container?&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;AARON CRICKENBERGER: As you may imagine, Black Hawk helicopters are flown in a variety of places around the world, not just next to a building that happens to have a parking lot next to it. And so in order to keep your pilots fresh, you may want to make sure they have good training hours and flight time, without spending fuel to fly an actual helicopter.&lt;/p&gt;

&lt;p&gt;I was involved in helping make what&amp;rsquo;s called a operation simulator, to train pilots on a bunch of the procedures using the same exact hardware that was deployed in Black Hawk helicopters, complete with motion seats that would shake to simulate movement and a full-fidelity visual system. This was all packed up in two shipping containers so that the simulator could be deployed wherever needed.&lt;/p&gt;

&lt;p&gt;I definitely had a really fun experience working on this simulator in the field at an Air Force base prior to a conference where I got to experience F-16s doing takeoff drills, which was amazing. They would get off the runway, and then just slam the afterburners to max and go straight up into the air. And I got to work on graphic simulation bugs. It was really cool.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: And for a lot of people, when you click on the web page they have listed in the GitHub link, you get their resume, or you get the list of open source projects they work on. In your case, there is &lt;a href=&#34;https://soundcloud.com/spiffxp&#34; target=&#34;_blank&#34;&gt;a SoundCloud page&lt;/a&gt;. What do people find on that page?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;AARON CRICKENBERGER: They get to see me living my whole life. I find that music is a very important part of my life. It&amp;rsquo;s a non-verbal voice that I have developed over time. I needed some place to host that. And then it came down between SoundCloud and Bandcamp, and SoundCloud was a much easier place to host my recordings.&lt;/p&gt;

&lt;p&gt;So you get to hear the results of me having picked up a guitar and noodling with that about five years ago. You get to hear what I&amp;rsquo;ve learned messing around with Ableton Live. You get to hear some mixes that I&amp;rsquo;ve done of ambient music. And I haven&amp;rsquo;t posted anything in a while there because I&amp;rsquo;m trying to get my recording of drums just right.&lt;/p&gt;

&lt;p&gt;So if you go to &lt;a href=&#34;https://www.youtube.com/channel/UCfnUO-9Q_gMraUXjbk4p50g&#34; target=&#34;_blank&#34;&gt;my YouTube channel&lt;/a&gt;, mostly what you&amp;rsquo;ll see are recordings of the various SIG meetings that I&amp;rsquo;ve participated in. But if you go back a little bit earlier than that, you&amp;rsquo;ll see that I do, in fact, play the drums. I&amp;rsquo;m trying to get those folded into my next songs.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: Do you know who &lt;a href=&#34;https://en.wikipedia.org/wiki/Hugh_Padgham#The_%22gated_drum%22_sound&#34; target=&#34;_blank&#34;&gt;Hugh Padgham&lt;/a&gt; is?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;AARON CRICKENBERGER: I do not.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: Hugh Padgham was the recording engineer who did the gated reverb drum sound that basically defined Phil Collins in the 1980s. I think you should call him up if you&amp;rsquo;re having problems with your drum sound.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;AARON CRICKENBERGER: That is awesome.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: You mentioned you can also find videos of the work that you&amp;rsquo;re doing with the SIG. How did you become the release manager for 1.14?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;AARON CRICKENBERGER: I&amp;rsquo;ve been involved in the Kubernetes release process way back in the 1.4 days. I started out as somebody who tried to help figure out, how do you write release notes for this thing? How do you take this whole mess and try to describe it in a sane way that makes sense to end users and developers? And I gradually became involved in other aspects of the release over time.&lt;/p&gt;

&lt;p&gt;I helped out with CI Signal. I helped out with issue triage. When I helped out with CI Signal, I wrote the &lt;a href=&#34;https://github.com/kubernetes/sig-release/blob/master/release-team/role-handbooks/ci-signal/README.md&#34; target=&#34;_blank&#34;&gt;very first playbook&lt;/a&gt; to describe what it is I do around here. That&amp;rsquo;s the model that has since been used for the rest of the release team, where every role describes what they do in a playbook that is used not just for their own benefit, but to help them train other people.&lt;/p&gt;

&lt;p&gt;Formally how I became release lead was I served as release shadow in 1.13. And when release leads are looking to figure out who&amp;rsquo;s going to lead the next release, they turn around and they look at their shadows, because those are who they have been helping out and training.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: If they don&amp;rsquo;t have a shadow, do they have to wait another three months and do a release again?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;AARON CRICKENBERGER: They do not. The way it works is the release lead can look at their shadows, then they take a look at the rest of their release team leads to see if there is sufficient experience there. And then if not, they consult with the chairs of SIG release.&lt;/p&gt;

&lt;p&gt;So for example, for Kubernetes v1.15, I ended up in an unfortunate situation where neither of my shadows were available to step up and become the leads for 1.15. I consulted with &lt;a href=&#34;https://github.com/claurence&#34; target=&#34;_blank&#34;&gt;Claire Lawrence&lt;/a&gt;, who was my enhancements lead for 1.14 and who was on the release team for two quarters, and so met the requirements to become a release lead that way. So she will be the release lead for v1.15.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: That was a fantastic answer to a throwaway &lt;a href=&#34;https://en.wikipedia.org/wiki/Groundhog_Day&#34; target=&#34;_blank&#34;&gt;Groundhog Day&lt;/a&gt; joke. I appreciate that.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;AARON CRICKENBERGER: [LAUGHS]&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: You can ask it again and see what the answer is, and then another time, and see how it evolves over time.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;AARON CRICKENBERGER: I&amp;rsquo;m short on my Groundhog Day riffs. I&amp;rsquo;ll come back to you.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: What are your responsibilities as the release lead?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;AARON CRICKENBERGER: Don&amp;rsquo;t Panic. I mean, essentially, a release lead&amp;rsquo;s job is to make the final call, and then hold the line by making the final call. So what you shouldn&amp;rsquo;t be doing as a release lead is attempting to dive in and fix all of the things, or do all of the things, or second-guess anybody else&amp;rsquo;s work. You are there principally and primarily to listen to everybody else&amp;rsquo;s advice and help them make the best decision. And only in the situations where there&amp;rsquo;s not a clear consensus do you wade in and make the call yourself.&lt;/p&gt;

&lt;p&gt;I feel like I was helped out by a very capable team in this regard, this release cycle. So it was super helpful. But as somebody who has what I like to call an &amp;ldquo;accomplishment monkey&amp;rdquo; on my back, it can be very difficult to resist the urge to dive right in and help out, because I have been there before. I have the boots-on-the-ground experience.&lt;/p&gt;

&lt;p&gt;The release lead&amp;rsquo;s job is not to be the boots on the ground, but to help make sure that everybody who is boots on the ground is actually doing what they need to do and unblocked in doing what they need to do. It also involves doing songs and dances and making funny pictures. So I view it more as like it&amp;rsquo;s about effective communication. And doing a lot of songs and dances, and funny pictures, and memes is one way that I do that.&lt;/p&gt;

&lt;p&gt;So one way that I thought it would help people pay attention to the release updates that I gave every week at the Kubernetes community meeting was to make sure that I wore a different cat T-shirt each week. After people riffed and joked out my first cat T-shirt where I said, I really need coffee right &amp;ldquo;meow&amp;rdquo;, and somebody asked if I got that coffee from a &amp;ldquo;purr-colator&amp;rdquo;, I decided to up the ante.&lt;/p&gt;

&lt;p&gt;And I&amp;rsquo;ve heard that people will await those cat T-shirts. They want to know what the latest one is. I even got a special cat T-shirt just to signify that code freeze was coming.&lt;/p&gt;

&lt;p&gt;We also decided that instead of imposing this crazy process that involved a lot of milestones, and labels, and whatnot that would cause the machinery to impose a bunch of additional friction, I would just post a lot of memes to Twitter about code freeze coming. And that seems to have worked out really well. So by and large, the release lead&amp;rsquo;s job is communication, unblocking, and then doing nothing for as much as possible.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s really kind of difficult and terrifying because you always have this feeling that you may have missed something, or that you&amp;rsquo;re just not seeing something that&amp;rsquo;s out there. So I&amp;rsquo;m sitting in this position with a release that has been extremely stable, and I spent a lot of time thinking, OK, what am I missing? Like, this looks too good. This is too quiet. There&amp;rsquo;s usually something that blows up. Come on, what is it, what is it, what is it? And it&amp;rsquo;s an exercise in keeping that all in and not sharing it with everybody until the release is over.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: &lt;a href=&#34;https://twitter.com/KubernetesPod/status/1110611630180597760/photo/1&#34; target=&#34;_blank&#34;&gt;He is here in a cat T-shirt&lt;/a&gt;, as well.&lt;/p&gt;

&lt;p&gt;When a new US President takes over the office, it&amp;rsquo;s customary that the outgoing president leaves them a note with advice in it. Aside from the shadow team, is there something similar that exists with Kubernetes release management?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;AARON CRICKENBERGER: Yeah, I would say there&amp;rsquo;s a very special&amp;ndash; I don&amp;rsquo;t know what the word is I&amp;rsquo;m looking for here&amp;ndash; bond, relationship, or something where people who have been release leads in the past are very empathetic and very supportive of those who step into the role as release lead.&lt;/p&gt;

&lt;p&gt;You know, I talked about release lead being a lot of uncertainty and second-guessing yourself, while on the outside you have to pretend like everything is OK. And having the support of people who have been there and who have gone through that experience is tremendously helpful.&lt;/p&gt;

&lt;p&gt;So I was able to reach out to a previous release lead. Not to pull the game with&amp;ndash; what is it, like two envelopes? The first envelope, you blame the outgoing president. The second envelope, you write two letters. It&amp;rsquo;s not quite like that.&lt;/p&gt;

&lt;p&gt;I am totally happy to be blamed for all of the changes we made to the release process that didn&amp;rsquo;t go well, but I&amp;rsquo;m also happy to help support my successor. I feel like my job as a release lead is, number one, make sure the release gets out the door, number two, make sure I set up my successor for success.&lt;/p&gt;

&lt;p&gt;So I&amp;rsquo;ve already been meeting with Claire to describe what I would do as the introductory steps. And I plan on continuing to consult with Claire throughout the release process to make sure that things are going well.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: If you want to hear the perspective from some previous release leads, check out &lt;a href=&#34;http://kubernetespodcast.com/episode/010-kubernetes-1.11/&#34; target=&#34;_blank&#34;&gt;episode 10&lt;/a&gt;, where we interview Josh Berkus and Tim Pepper.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: What do you plan to put into that set of notes for Claire?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;AARON CRICKENBERGER: That&amp;rsquo;s a really good question. I would tell Claire to trust her team first and trust her gut second. Like I said, I think it is super important to establish trust with your team, because the release is this superhuman effort that involves consuming, or otherwise fielding, or shepherding the work of hundreds of contributors.&lt;/p&gt;

&lt;p&gt;And your team is made up of at least 13 people. You could go all the way up to 40 or 50, if you include all of the people that are being trained by those people. There&amp;rsquo;s so much work out there. It&amp;rsquo;s just more work than any one person can possibly handle.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s honestly the same thing I will tell new contributors to Kubernetes is that there&amp;rsquo;s no way you can possibly understand all of it. You will not understand the shape of Kubernetes. You will never be the expert who knows literally all of the things, and that&amp;rsquo;s OK. The important part is to make sure that you have people who, when you don&amp;rsquo;t know the answer, you know who to ask for the answer. And it is really helpful if your team are those people.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: The specific version that you&amp;rsquo;ve been working on and the release that&amp;rsquo;s just come out is Kubernetes 1.14. What are some of the new things in this release?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;AARON CRICKENBERGER: This release of Kubernetes contains more stable enhancements than any other release of Kubernetes ever. And I&amp;rsquo;m pretty proud of that fact. I know in the past you may have heard other release leads talk about, like, this is the stability release, or this time we&amp;rsquo;re really making things a little more mature. But I feel a lot of confidence in saying that this time around.&lt;/p&gt;

&lt;p&gt;Like, I stood in a room, and it was a leadership summit, I think, back in 2017 where we said, look, we&amp;rsquo;re really going to try and make Kubernetes more stable. And we&amp;rsquo;re going to focus on sort of hardening the core of Kubernetes and defining what the core of Kubernetes is. And we&amp;rsquo;re not going to accept a bunch of new features. And then we kind of went and accepted a bunch of new features. And that was a while ago. And here we are today.&lt;/p&gt;

&lt;p&gt;But I think we are finally starting to see the results of work that was started back then. &lt;a href=&#34;https://github.com/kubernetes/enhancements/blob/master/keps/sig-windows/20190103-windows-node-support.md&#34; target=&#34;_blank&#34;&gt;Windows Server Container Support&lt;/a&gt; is probably the biggest one. You can hear Michael Michael tell stories about how SIG Windows was started about three years ago. And today, they can finally announce that Windows Server containers have gone GA. That&amp;rsquo;s a huge accomplishment.&lt;/p&gt;

&lt;p&gt;A lot of the heavy lifting for this, I believe, came at the end. It started with a conversation in Kubernetes 1.13, and was really wrapped up this release where we define, what are Windows Server containers, exactly? How do they differ from Docker containers or other container runtimes that run on Linux?&lt;/p&gt;

&lt;p&gt;Because today so much of the assumptions people make about the functionality that Kubernetes offers are also baked in with the functionality that Linux-based containers offer. And so we wanted to enable people to use the awesome Kubernetes orchestration capabilities that they have come to love, but to also use that to orchestrate some applications or capabilities that are only available on Windows.&lt;/p&gt;

&lt;p&gt;So we put together what&amp;rsquo;s called a &lt;a href=&#34;https://github.com/kubernetes/enhancements/tree/master/keps&#34; target=&#34;_blank&#34;&gt;Kubernetes Enhancement Proposal&lt;/a&gt; process, or a KEP, for short. And we said that we&amp;rsquo;re going to use these KEPs to describe exactly what the criteria are to call something alpha, or beta, or stable. And so the Windows feature allowed us to use a KEP&amp;ndash; or in getting Windows in here, we used the KEP to describe everything that would and would not work for Windows Server containers. That was super huge. And that really, I think, helped us better understand or define what Kubernetes is in that context.&lt;/p&gt;

&lt;p&gt;But OK, I&amp;rsquo;ve spent most of the time answering your question with just one single stable feature.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: Well, let&amp;rsquo;s dig a little bit in to the KEP process then, because this is the first release where there&amp;rsquo;s a new rule. It says, all proposed enhancements for this release must have an associated KEP. So that&amp;rsquo;s a Kubernetes Enhancement Proposal, a one-page document that describes it. What has the process been like of A, getting engineers on-board with using that, and then B, building something based on these documents?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;AARON CRICKENBERGER: It is a process of continued improvement. So it is by no means done, but it honestly required a lot of talking, and saying the same thing over and over to the same people or to different people, as is often the case when it comes to things that involve communication and process changes. But by and large, everybody was pretty much on-board with this.&lt;/p&gt;

&lt;p&gt;There was a little bit of confusion, though, over how high the bar would be set and how rigorously or rigidly we would be enforcing these criteria. And that&amp;rsquo;s where I feel like we have room to iterate and improve on. But we have collectively agreed that, yeah, we do like having all of the information about a particular enhancement in one place. Right?&lt;/p&gt;

&lt;p&gt;The way the world used to operate before is we would throw around Google Docs, that were these design proposals, and then we&amp;rsquo;d comment on those a bunch. And then eventually, those were turned into markdown files. And those would end up in the community repo,&lt;/p&gt;

&lt;p&gt;And then we&amp;rsquo;d have a bunch of associated issues that talked about that. And then maybe somebody would open up another issue that they&amp;rsquo;d call an umbrella issue. And then a bunch of comments would be put there. And then there&amp;rsquo;s lots of discussion that goes on in the PRs. There&amp;rsquo;s like seven different things that I just rattled off there.&lt;/p&gt;

&lt;p&gt;So KEPs are about focusing all of the discussion about the design and implementation and reasoning behind enhancements in one single place. And I think there, we are fully on board. Do we have room to improve? Absolutely. Humans are involved, and it&amp;rsquo;s a messy process. We could definitely find places to automate this better, structure it better. And I look forward to seeing those improvements happen.&lt;/p&gt;

&lt;p&gt;You know, I think another one of the big things was a lot of these KEPs were mired across three different SIGs. There was sort of SIG architecture who had the technical vision for these. There was SIG PM, who&amp;ndash; you know, pick your P of choice&amp;ndash; product, project, process, program, people who are better about how to shepherd things forward, and then SIG release, who just wanted to figure out, what&amp;rsquo;s landing in the release, and why, and how, and why is it important? And so taking the responsibilities across all of those three SIGs and putting it in the right place, which is SIG PM, I think really will help us iterate properly, moving forward.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: The other change in this release is that there is no code slush. &lt;a href=&#34;https://github.com/kubernetes/sig-release/issues/269&#34; target=&#34;_blank&#34;&gt;What is a code slush, and why don&amp;rsquo;t we have one anymore?&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;AARON CRICKENBERGER: That&amp;rsquo;s a really good question. I had 10 different people ask me that question over the past couple of months, quarters, years. Take your pick. And so I finally decided, if nobody knows what a code slush is, why do we even have it?&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: It&amp;rsquo;s like a thawed freeze, but possibly with sugar?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;AARON CRICKENBERGER: [LAUGHING] So code slush is about&amp;ndash; we want to slow the rate of change prior to code freeze. Like, let&amp;rsquo;s accept code freeze as this big deadline where nothing&amp;rsquo;s going to happen after a code freeze.&lt;/p&gt;

&lt;p&gt;So while I really want to assume and aspire to live in a world where developers are super productive, and start their changes early, and get them done when they&amp;rsquo;re done, today, I happen to live in a world where developers are driven by deadlines. And they get distracted. And there&amp;rsquo;s other stuff going on. And then suddenly, they realize there&amp;rsquo;s a code freeze ahead of them.&lt;/p&gt;

&lt;p&gt;And this wonderful feature that they&amp;rsquo;ve been thinking about implementing over the past two months, they now have to get done in two weeks. And so suddenly, all sorts of code starts to fly in super fast and super quickly. And OK, that&amp;rsquo;s great. I love empowering people to be productive.&lt;/p&gt;

&lt;p&gt;But what we don&amp;rsquo;t want to have happen is somebody decide to land some massive feature or enhancement that changes absolutely everything. Or maybe they decided they want to refactor the world. And if they do that, then they make everybody else&amp;rsquo;s life super difficult because of merge conflicts and rebases. Or maybe all of the test signal that we had kind-of grown accustomed to and gotten used to, completely changes.&lt;/p&gt;

&lt;p&gt;So code slush was about reminding people, hey, don&amp;rsquo;t be jerks. Be kind of responsible. Please try not to land anything super huge at the last minute. But the way that we enforced this was with, like, make sure your PR has a milestone. And make sure that it has priority critical/urgent. In times past, we were like, make sure there is a label called status approved for milestone.&lt;/p&gt;

&lt;p&gt;We were like, what do all these things even mean? People became obsessed with all the labels, and the milestones, and the process. And they never really paid attention to why we&amp;rsquo;re asking people to pay attention to the fact that code freeze was coming soon.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: Process for process sake, they could start to build on top of each other. You mentioned that there is a number of other things in the release. Do you want to talk about some of the other pieces that are in there?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;AARON CRICKENBERGER: Sure. I think two of the other stable features that I believe other people will find to be exciting are &lt;a href=&#34;https://github.com/kubernetes/enhancements/blob/master/keps/sig-network/0007-pod-ready%2B%2B.md&#34; target=&#34;_blank&#34;&gt;readiness gates&lt;/a&gt; and &lt;a href=&#34;https://github.com/kubernetes/enhancements/blob/master/keps/sig-scheduling/20190131-pod-priority-preemption.md&#34; target=&#34;_blank&#34;&gt;Pod priority and preemption&lt;/a&gt;. Today, Pods have the concept of liveliness and readiness. A live Pod has an application running in it, but it might not be ready to do anything. And so when a Pod is ready, that means it&amp;rsquo;s ready to receive traffic.&lt;/p&gt;

&lt;p&gt;So if you&amp;rsquo;re thinking of some big application that&amp;rsquo;s scaled out everywhere, you want to make sure your Pods are only handling traffic when they&amp;rsquo;re good and ready to do so. But prior to 1.14, the only ways you could verify that were by using either TCP probes, HTTP probes, or exec probes. Either make sure that ports are open inside of the container, or run a command inside of the container and see what that command says.&lt;/p&gt;

&lt;p&gt;And then you can definitely customize a fair amount there, but that requires that you put all of that information inside of the Pod. And it might be really useful for some cluster operators to signify some more overarching concerns that they have before a Pod could be ready. So just&amp;ndash; I don&amp;rsquo;t know&amp;ndash; make sure a Pod has registered with some other system to make sure that it is authorized to serve traffic, or something of that nature. Pod readiness gates allow that sort of capability to happen&amp;ndash; to transparently extend the conditions that you use to figure out whether a Pod is ready for traffic. We believe this will enable more sophisticated orchestration and deployment mechanisms for people who are trying to manage their applications and services.&lt;/p&gt;

&lt;p&gt;I feel like Pod priority and preemption will be interesting to consumers who like to oversubscribe their Kubernetes clusters. Instead of assuming everything is the same size and is the same priority, and first Pods win, you can now say that certain Pods are more important than other Pods. They get scheduled before other Pods, and maybe even so that they kick out other Pods to make room for the really important Pods.&lt;/p&gt;

&lt;p&gt;You could think of it as if you have any super important agents or daemons that have to run on your cluster. Those should always be there. Now, you can describe them as high-priority to make sure that they are definitely always there and always scheduled before anything else is.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: Are there any other new features that are in alpha or beta that you&amp;rsquo;re keeping your eye on?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;AARON CRICKENBERGER: Yeah. So I feel like, on the beta side of things, a lot of what I am interested in&amp;ndash; if I go back to my theme of maturity, and stability, and defining the core of Kubernetes, I think that the storage SIG has been doing amazing work. They continue to ship out, quarter, after quarter, after quarter, after quarter, new and progressive enhancements to storage&amp;ndash; mostly these days through the CSI, Container Storage Interface project, which is fantastic. It allows you to plug in arbitrary pieces of storage functionality.&lt;/p&gt;

&lt;p&gt;They have a number of things related to that that are in beta this time around, such as topology support. So you&amp;rsquo;re going to be able to more accurately express how and where your CSI volumes need to live relative to your application. Block storage support is something I&amp;rsquo;ve heard a number of people asking for, as well as the ability to define &lt;a href=&#34;https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/20190124-local-persistent-volumes.md&#34; target=&#34;_blank&#34;&gt;durable local volumes&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s say you&amp;rsquo;re running a Pod on a node, and you want to make sure it&amp;rsquo;s writing directly to the node&amp;rsquo;s local volumes. And that way, it could be super performant. Cool. Give it an emptydir. It&amp;rsquo;ll be fine.&lt;/p&gt;

&lt;p&gt;But if you destroy the Pod, then you lose all the data that the Pod wrote. And so again, I go back to the example of maybe it&amp;rsquo;s an agent, and it&amp;rsquo;s writing a bunch of useful, stateful information to disk. And you&amp;rsquo;d love for the agent to be able to go away and something to replace it, and be able to get all of that information off of disk. Local durable volumes allow you to do that. And you get to do that in the same way that you&amp;rsquo;re used to specifying durable or persistent volumes that are given to you by a cloud provider, for example.&lt;/p&gt;

&lt;p&gt;Since I did co-found SIG testing, I think I have to call out a testing feature that I like. It&amp;rsquo;s really tiny and silly, but it has always bugged me that when you try to download the tests, you download something that&amp;rsquo;s over a gigabyte in size. That&amp;rsquo;s the way things used work for Kubernetes back in the old days for Kubernetes client and server stuff as well. And we have since broken that up into&amp;ndash; you only need to download the binaries that makes sense for your platform.&lt;/p&gt;

&lt;p&gt;So say I&amp;rsquo;m developing Kubernetes on my MacBook. I probably don&amp;rsquo;t need to download the Linux test binaries, or the Windows test binaries, or the ARM64 test binaries, or the s390x test binaries. Did I mention Kubernetes supports a lot of different architectures?&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: I hadn&amp;rsquo;t noticed s390 was a supported platform until now.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;AARON CRICKENBERGER: It is definitely something that we build binaries for. I&amp;rsquo;m not sure if we&amp;rsquo;ve actually seen a certified conformant Kubernetes that runs on s390, but it is definitely one of the things we build Kubernetes against.&lt;/p&gt;

&lt;p&gt;Not having to download an entire gigabyte plus of binaries just to run some tests is super great. I like to live in a world where I don&amp;rsquo;t have to build the tests from scratch. Can I please just run a program that has all the tests? Maybe I can use that to soak test or sanity test my cluster to make sure that everything is OK. And downloading just the thing that I need is super great.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: You&amp;rsquo;re talking about the idea of Kubernetes having a core and the idea of releases and stability. If you think back to Linux distributions maybe even 10 years ago, we didn&amp;rsquo;t care so much about the version number releases of the kernel anymore, but we cared when there was a new feature in a Red Hat release. Do you think we&amp;rsquo;re getting to that point with Kubernetes at the moment?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;AARON CRICKENBERGER: I think that is one model that people really hope to see Kubernetes move toward. I&amp;rsquo;m not sure if it is the model that we will move toward, but I think it is an ongoing discussion. So you know, we&amp;rsquo;ve created a working group called &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/wg-lts&#34; target=&#34;_blank&#34;&gt;WG LTS&lt;/a&gt;. I like to call it by its longer name&amp;ndash; WG &amp;ldquo;to LTS, or not to LTS&amp;rdquo;. What does LTS even mean? What are we trying to release and support?&lt;/p&gt;

&lt;p&gt;Because I think that when people think about distributions, they do naturally gravitate towards some distributions have higher velocity release cadences, and others have slower release cadences. And that&amp;rsquo;s cool and great for people who want to live on a piece of software that never ever changes. But those of us who run software at scale find that you can&amp;rsquo;t actually prevent change from happening. There will always be pieces of your infrastructure, or your environment, or your software, that are not under your control.&lt;/p&gt;

&lt;p&gt;And so anything we can do to achieve what I like to call a dynamic stability is probably better for everybody involved. Make the cost of change as low as you possibly can. Make the pain of changing and upgrade as low as you possibly can, and accept that everything will always be changing all the time.&lt;/p&gt;

&lt;p&gt;So yeah. Maybe that&amp;rsquo;s where Linux lives, where the Kernel is always changing. And you can either care about that, or not. And you can go with a distribution that is super up-to-date with the Linux Kernel, or maybe has a slightly longer upgrade cadence. But I think it&amp;rsquo;s about enabling both of those options. Because I think if we try to live in a world where there are only distributions and nothing else, that&amp;rsquo;s going to actually harm everybody in the long term and maybe bring us away from all of these cloud-native ideals that we have, trying to accept change as a constant.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: We can&amp;rsquo;t let you go without talking about the Beard. What is SIG Beard, and how critical was it in you becoming the 1.14 release manager?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;AARON CRICKENBERGER: I feel like it&amp;rsquo;s a new requirement for all release leads to be a member of SIG Beard. SIG Beard happened because, one day, I realized I had gotten lazy, and I had this just ginormous and magnificent beard. It was really flattering to have Brendan Burns up on stage at KubeCon Seattle compliment my beard in front of an audience of thousands of people. I cannot tell you what that feels like.&lt;/p&gt;

&lt;p&gt;But to be serious for a moment, like OK, I&amp;rsquo;m a dude. I have a beard. There are a lot of dudes who work in tech, and many dudes are bearded. And this is by no means a way of being exclusionary, or calling that out, or anything like that. It was just noticing that while I was on camera, there seemed to be more beard than face at times. And what is that about?&lt;/p&gt;

&lt;p&gt;And I had somebody start referring to me as &amp;ldquo;The Beard&amp;rdquo; in my company. It turns out they read Neil Stevenson&amp;rsquo;s &amp;ldquo;&lt;a href=&#34;https://en.wikipedia.org/wiki/Cryptonomicon&#34; target=&#34;_blank&#34;&gt;Cryptonomicon&lt;/a&gt;,&amp;rdquo; if you&amp;rsquo;re familiar with that book at all.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: It&amp;rsquo;s a great book.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;AARON CRICKENBERGER: Yeah. It talks about how you have the beard, and you have the suit. The suit is the person who&amp;rsquo;s responsible for doing all the talking, and the beard is responsible for doing all the walking. And I guess I have gained a reputation for doing an awful lot of walking and showing up in an awful lot of places. And so I thought I would embrace that.&lt;/p&gt;

&lt;p&gt;When I showed up to Google my first day at work where I was looking for the name tag that shows what desk is mine, and my name tag was SIG Beard. And I don&amp;rsquo;t know who did it, but I was like, all right, I&amp;rsquo;m running with it. And so I referred to myself as &amp;ldquo;Aaron of SIG Beard&amp;rdquo; from then on.&lt;/p&gt;

&lt;p&gt;And so to me, the beard is not so much about being bearded on my face, but being bearded at heart&amp;ndash; being welcoming, being fun, embracing this community for all of the awesomeness that it has, and encouraging other people to do the same. So in that regard, I would like to see more people be members of SIG Beard. I&amp;rsquo;m trying to figure out ways to make that happen. And yeah, it&amp;rsquo;s great.&lt;/p&gt;

&lt;hr/&gt;

&lt;p&gt;&lt;i&gt;&lt;a href=&#34;http://twitter.com/spiffxp&#34; target=&#34;_blank&#34;&gt;Aaron Crickenberger&lt;/a&gt; is a senior test engineer with &lt;a href=&#34;https://cloud.google.com/&#34; target=&#34;_blank&#34;&gt;Google Cloud&lt;/a&gt;. He co-founded the Kubernetes Testing SIG, has participated in every Kubernetes release since version 1.4, has served on the &lt;a href=&#34;https://github.com/kubernetes/steering&#34; target=&#34;_blank&#34;&gt;Kubernetes steering committee&lt;/a&gt; since its inception in 2017, and most recently served as the Kubernetes 1.14 release lead.&lt;/p&gt;

&lt;p&gt;You can find the &lt;a href=&#34;http://www.kubernetespodcast.com/&#34; target=&#34;_blank&#34;&gt;Kubernetes Podcast from Google&lt;/a&gt; at &lt;a href=&#34;https://twitter.com/KubernetesPod&#34; target=&#34;_blank&#34;&gt;@kubernetespod&lt;/a&gt; on Twitter, and you can &lt;a href=&#34;https://kubernetespodcast.com/subscribe/&#34; target=&#34;_blank&#34;&gt;subscribe&lt;/a&gt; so you never miss an episode.  Please come and say Hello to us at KubeCon EU!&lt;/i&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Join us for the 2019 KubeCon Diversity Lunch &amp; Hack</title>
      <link>https://kubernetes.io/blog/2019/05/02/kubecon-diversity-lunch-and-hack/</link>
      <pubDate>Thu, 02 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/05/02/kubecon-diversity-lunch-and-hack/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kiran Oliver, Podcast Producer, The New Stack&lt;/p&gt;

&lt;p&gt;Join us for the 2019 KubeCon Diversity Lunch &amp;amp; Hack: Building Tech Skills &amp;amp; An Inclusive Community - Sponsored by Google Cloud and VMware&lt;/p&gt;

&lt;p&gt;Registration for the Diversity Lunch opens today, May 2nd, 2019. To register, go to the main &lt;a href=&#34;https://events.linuxfoundation.org/events/kubecon-cloudnativecon-europe-2019/schedule/&#34;&gt;KubeCon + CloudNativeCon EU schedule&lt;/a&gt;, then log in to your Sched account, and confirm your attendance to the Diversity Lunch. Please sign up ASAP once the link is live, as spaces will fill quickly. We filled the event in just a few days last year, and anticipate doing so again this year.&lt;/p&gt;

&lt;p&gt;The 2019 KubeCon Diversity Lunch &amp;amp; Hack will be held at the Fira Gran Via Barcelona Hall 8.0 Room F1 on May 22nd, 2019 from 12:30-14:00.&lt;/p&gt;

&lt;p&gt;If you’ve never attended a Diversity Lunch before, not to worry. All are welcome, and there’s a variety of things to experience and discuss.&lt;/p&gt;

&lt;p&gt;First things first, let’s establish some ground rules:&lt;/p&gt;

&lt;p&gt;This is a safe space. What does that mean? Simple:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Asking for and using people’s pronouns&lt;/li&gt;
&lt;li&gt;Absolutely no photography&lt;/li&gt;
&lt;li&gt;Awareness of your actions towards others. Do your best to ensure that you contribute towards making this environment welcoming, safe, and inclusive for all.&lt;/li&gt;
&lt;li&gt;Please avoid tech-heavy arbitrary community slang/jargon [keep in mind that not all of us are developers, many are tech-adjacent and/or new to the community]&lt;/li&gt;
&lt;li&gt;Act with care and empathy towards your fellow community members at all times.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This event also follows the &lt;a href=&#34;https://events.linuxfoundation.org/events/kubecon-cloudnativecon-europe-2019/attend/code-of-conduct/&#34;&gt;Code of Conduct&lt;/a&gt; for all CNCF events.&lt;/p&gt;

&lt;p&gt;We have run a very successful diversity lunch event before. This isn’t a trial run, nor is it a proof of concept. We had a fun, productive, and educational conversation last year in Seattle, and hope to do so again this year. As 2018’s KubeCon + CloudNativeCon in Seattle marked our first Diversity Lunch with pair programming, we hammered out a lot of kinks post-mortem, using that feedback to inform and improve upon our decision making, planning, and organizational process moving forward, to bring you an improved experience at the 2019 KubeCon + CloudNativeCon Diversity Lunch.&lt;/p&gt;

&lt;p&gt;Tables not related to pair-programming or hands-on Kubernetes will be led by a moderator, where notes and feedback will then be taken and shared at the end of the lunch and in a post-mortem discussion after KubeCon+CloudNativeCon Barcelona ends, as part of our continuous improvement process. Some of last year’s tables were dedicated to topics that were submitted at registration, such as: security, D&amp;amp;I, service meshes, and more. You can suggest your own table topic on the registration form this year as well, and we highly encourage you to do so, particularly if you do not see your preferred topic or activity of choice listed. Your suggestions will then be used to determine the discussion table tracks that will be available at this year’s Diversity Lunch &amp;amp; Hack.&lt;/p&gt;

&lt;p&gt;We hope you are also excited to participate in the ‘Hack’ portion of this ‘Lunch and Hack.’ This breakout track will include a variety of peer-programming exercises led by your fellow Kubernetes community members, with discussion leads working together with attendees hands-on to solve their Kubernetes-related problems in a welcoming, safe environment.&lt;/p&gt;

&lt;p&gt;To make this all possible, we need you. Yes, you, to register. As much as we love having groups of diverse people all gather in the same room, we also need allies. If you’re a member of a privileged group or majority, you are welcome and encouraged to join us. Most importantly, we want you to take what you learn and experience at the Diversity Lunch back to both your companies and your open source communities, so that you can help us make positive changes not only within our industry, but beyond. No-one lives [or works] in a bubble. We hope that the things you learn here will carry over and bring about positive change in the world as a whole.&lt;/p&gt;

&lt;p&gt;We look forward to seeing you!&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Special thanks to &lt;a href=&#34;https://www.linkedin.com/in/leahstunts/&#34; target=&#34;_blank&#34;&gt;Leah Petersen&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/sarah-conway-6166151/&#34; target=&#34;_blank&#34;&gt;Sarah Conway&lt;/a&gt; and &lt;a href=&#34;https://www.linkedin.com/in/parispittman/&#34; target=&#34;_blank&#34;&gt;Paris Pittman&lt;/a&gt; for their help in editing this post.&lt;/em&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: How You Can Help Localize Kubernetes Docs</title>
      <link>https://kubernetes.io/blog/2019/04/26/how-you-can-help-localize-kubernetes-docs/</link>
      <pubDate>Fri, 26 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/04/26/how-you-can-help-localize-kubernetes-docs/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author: Zach Corleissen (Linux Foundation)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Last year we optimized the Kubernetes website for &lt;a href=&#34;https://kubernetes.io/blog/2018/11/08/kubernetes-docs-updates-international-edition/&#34;&gt;hosting multilingual content&lt;/a&gt;. Contributors responded by adding multiple new localizations: as of April 2019, Kubernetes docs are partially available in nine different languages, with six added in 2019 alone. You can see a list of available languages in the language selector at the top of each page.&lt;/p&gt;

&lt;p&gt;By &lt;em&gt;partially available&lt;/em&gt;, I mean that localizations are ongoing projects. They range from mostly complete (&lt;a href=&#34;https://v1-12.docs.kubernetes.io/zh/&#34; target=&#34;_blank&#34;&gt;Chinese docs for 1.12&lt;/a&gt;) to brand new (1.14 docs in &lt;a href=&#34;https://kubernetes.io/pt/&#34; target=&#34;_blank&#34;&gt;Portuguese&lt;/a&gt;). If you&amp;rsquo;re interested in helping an existing localization, read on!&lt;/p&gt;

&lt;h2 id=&#34;what-is-a-localization&#34;&gt;What is a localization?&lt;/h2&gt;

&lt;p&gt;Translation is about words and meaning. Localization is about words, meaning, process, and design.&lt;/p&gt;

&lt;p&gt;A localization is like a translation, but more thorough. Instead of just translating words, a localization optimizes the framework for writing and publishing words. For example, most site navigation features (button text) on kubernetes.io are strings contained in a &lt;a href=&#34;https://github.com/kubernetes/website/tree/master/i18n&#34; target=&#34;_blank&#34;&gt;single file&lt;/a&gt;. Part of creating a new localization involves adding a language-specific version of that file and translating the strings it contains.&lt;/p&gt;

&lt;p&gt;Localization matters because it reduces barriers to adoption and support. When we can read Kubernetes docs in our own language, it&amp;rsquo;s easier to get started using Kubernetes and contributing to its development.&lt;/p&gt;

&lt;h2 id=&#34;how-do-localizations-happen&#34;&gt;How do localizations happen?&lt;/h2&gt;

&lt;p&gt;The availability of docs in different languages is a feature&amp;mdash;and like all Kubernetes features, contributors develop localized docs in a SIG, share them for review, and add them to the project.&lt;/p&gt;

&lt;p&gt;Contributors work in teams to localize content. Because folks can&amp;rsquo;t approve their own PRs, localization teams have a minimum size of two&amp;mdash;for example, the Italian localization has two contributors. Teams can also be quite large: the Chinese team has several dozen contributors.&lt;/p&gt;

&lt;p&gt;Each team has its own workflow. Some teams localize all content manually; others use editors with translation plugins and review machine output for accuracy. SIG Docs focuses on standards of output; this leaves teams free to adopt the workflow that works best for them. That said, teams frequently collaborate with each other on best practices, and sharing abounds in the best spirit of the Kubernetes community.&lt;/p&gt;

&lt;h2 id=&#34;helping-with-localizations&#34;&gt;Helping with localizations&lt;/h2&gt;

&lt;p&gt;If you&amp;rsquo;re interested in starting a new localization for Kubernetes docs, the &lt;a href=&#34;https://kubernetes.io/docs/contribute/localization/&#34; target=&#34;_blank&#34;&gt;Kubernetes contribution guide&lt;/a&gt; shows you how.&lt;/p&gt;

&lt;p&gt;Existing localizations also need help. If you&amp;rsquo;d like to contribute to an existing project, join the localization team&amp;rsquo;s Slack channel and introduce yourself. Folks on that team can help you get started.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Localization&lt;/th&gt;
&lt;th&gt;Slack channel&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Chinese (中文)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://kubernetes.slack.com/messages/CE3LNFYJ1/&#34; target=&#34;_blank&#34;&gt;#kubernetes-docs-zh&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://kubernetes.slack.com/messages/C1J0BPD2M/&#34; target=&#34;_blank&#34;&gt;#sig-docs&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;French (Français)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://kubernetes.slack.com/messages/CG838BFT9/&#34; target=&#34;_blank&#34;&gt;#kubernetes-docs-fr&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;German (Deutsch)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://kubernetes.slack.com/messages/CH4UJ2BAL/&#34; target=&#34;_blank&#34;&gt;#kubernetes-docs-de&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Hindi&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://kubernetes.slack.com/messages/CJ14B9BDJ/&#34; target=&#34;_blank&#34;&gt;#kubernetes-docs-hi&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Indonesian&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://kubernetes.slack.com/messages/CJ1LUCUHM/&#34; target=&#34;_blank&#34;&gt;#kubernetes-docs-id&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Italian&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://kubernetes.slack.com/messages/CGB1MCK7X/&#34; target=&#34;_blank&#34;&gt;#kubernetes-docs-it&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Japanese (日本語)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://kubernetes.slack.com/messages/CAG2M83S8/&#34; target=&#34;_blank&#34;&gt;#kubernetes-docs-ja&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Korean (한국어)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://kubernetes.slack.com/messages/CA1MMR86S/&#34; target=&#34;_blank&#34;&gt;#kubernetes-docs-ko&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Portuguese (Português)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://kubernetes.slack.com/messages/CJ21AS0NA/&#34; target=&#34;_blank&#34;&gt;#kubernetes-docs-pt&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Spanish (Español)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://kubernetes.slack.com/messages/CH7GB2E3B/&#34; target=&#34;_blank&#34;&gt;#kubernetes-docs-es&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;what-s-next&#34;&gt;What&amp;rsquo;s next?&lt;/h2&gt;

&lt;p&gt;There&amp;rsquo;s a new &lt;a href=&#34;https://kubernetes.slack.com/messages/CJ14B9BDJ/&#34; target=&#34;_blank&#34;&gt;Hindi localization&lt;/a&gt; beginning. Why not add your language, too?&lt;/p&gt;

&lt;p&gt;As a chair of SIG Docs, I&amp;rsquo;d love to see localization spread beyond the docs and into Kubernetes components. Is there a Kubernetes component you&amp;rsquo;d like to see supported in a different language? Consider making a &lt;a href=&#34;https://github.com/kubernetes/enhancements/tree/master/keps&#34; target=&#34;_blank&#34;&gt;Kubernetes Enhancement Proposal&lt;/a&gt; to support the change.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Hardware Accelerated SSL/TLS Termination in Ingress Controllers using Kubernetes Device Plugins and RuntimeClass</title>
      <link>https://kubernetes.io/blog/2019/04/24/hardware-accelerated-ssl/tls-termination-in-ingress-controllers-using-kubernetes-device-plugins-and-runtimeclass/</link>
      <pubDate>Wed, 24 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/04/24/hardware-accelerated-ssl/tls-termination-in-ingress-controllers-using-kubernetes-device-plugins-and-runtimeclass/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mikko Ylinen (Intel)&lt;/p&gt;

&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;

&lt;p&gt;A Kubernetes Ingress is a way to connect cluster services to the world outside the cluster. In order
to correctly route the traffic to service backends, the cluster needs an Ingress controller. The
Ingress controller is responsible for setting the right destinations to backends based on the
Ingress API objects’ information. The actual traffic is routed through a proxy server that
is responsible for tasks such as load balancing and SSL/TLS (later “SSL” refers to both SSL
or TLS ) termination. The SSL termination is a CPU heavy operation due to the crypto operations
involved. To offload some of the CPU intensive work away from the CPU, OpenSSL based proxy
servers can take the benefit of OpenSSL Engine API and dedicated crypto hardware. This frees
CPU cycles for other things and improves the overall throughput of the proxy server.&lt;/p&gt;

&lt;p&gt;In this blog post, we will show how easy it is to make hardware accelerated crypto available
for containers running the Ingress controller proxy using some of the recently created Kubernetes
building blocks: Device plugin framework and RuntimeClass. At the end, a reference setup is given
using an HAproxy based Ingress controller accelerated using Intel&amp;reg; QuickAssist Technology cards.&lt;/p&gt;

&lt;h2 id=&#34;about-proxies-openssl-engine-and-crypto-hardware&#34;&gt;About Proxies, OpenSSL Engine and Crypto Hardware&lt;/h2&gt;

&lt;p&gt;The proxy server plays a vital role in a Kubernetes Ingress Controller function. It proxies
the traffic to the backends per Ingress objects routes. Under heavy traffic load, the performance
becomes critical especially if the proxying involves CPU intensive operations like SSL crypto.&lt;/p&gt;

&lt;p&gt;The OpenSSL project provides the widely adopted library for implementing the SSL protocol. Of
the commonly known proxy servers used by Kubernetes Ingress controllers, Nginx and HAproxy use
OpenSSL. The CNCF graduated Envoy proxy uses BoringSSL but there seems to be &lt;a href=&#34;https://github.com/envoyproxy/envoy/pull/5161#issuecomment-446374130&#34; target=&#34;_blank&#34;&gt;community interest
in having OpenSSL as the alternative&lt;/a&gt; for it too.&lt;/p&gt;

&lt;p&gt;The OpenSSL SSL protocol library relies on libcrypto that implements the cryptographic functions.
For quite some time now (first introduced in 0.9.6 release), OpenSSL has provided an &lt;a href=&#34;https://github.com/openssl/openssl/blob/master/README.ENGINE&#34; target=&#34;_blank&#34;&gt;ENGINE
concept&lt;/a&gt; that allows these cryptographic operations to be offloaded to a dedicated crypto
acceleration hardware. Later, a special &lt;em&gt;dynamic&lt;/em&gt; ENGINE enabled the crypto hardware specific
pieces to be implemented in an independent loadable module that can be developed outside the
OpenSSL code base and distributed separately. From the application’s perspective, this is also
ideal because they don’t need to know the details of how to use the hardware, and the hardware
specific module can be loaded/used when the hardware is available.&lt;/p&gt;

&lt;p&gt;Hardware based crypto can greatly improve Cloud applications’ performance due to hardware
accelerated processing in SSL operations as discussed, and can provide other crypto
services like key/random number generation. Clouds can make the hardware easily available
using the dynamic ENGINE and several loadable module implementations exist, for
example, &lt;a href=&#34;https://docs.aws.amazon.com/cloudhsm/latest/userguide/openssl-library.html&#34; target=&#34;_blank&#34;&gt;CloudHSM&lt;/a&gt;, &lt;a href=&#34;https://github.com/opencryptoki/openssl-ibmca&#34; target=&#34;_blank&#34;&gt;IBMCA&lt;/a&gt;, or &lt;a href=&#34;https://github.com/intel/QAT_Engine/&#34; target=&#34;_blank&#34;&gt;QAT Engine&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For Cloud deployments, the ideal scenario is for these modules to be shipped as part of
the container workload. The workload would get scheduled on a node that provides the
underlying hardware that the module needs to access. On the other hand, the workloads
should run the same way and without code modifications regardless of the crypto acceleration
hardware being available or not. The OpenSSL dynamic engine enables this. Figure 1 below
illustrates these two scenarios using a typical Ingress Controller container as an example.
The red colored boxes indicate the differences between a container with a crypto hardware
engine enabled container vs. a “standard” one. It’s worth pointing out that the configuration
changes shown do not necessarily require another version of the container since the configurations
could be managed, e.g., using ConfigMaps.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-04-23-hardware-accelerated-tls-termination/k8s-blog-fig1.png&#34;
         alt=&#34;Figure 1. Examples of Ingress controller containers&#34; width=&#34;600&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;Figure 1. Examples of Ingress controller containers&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;


&lt;h2 id=&#34;hardware-resources-and-isolation&#34;&gt;Hardware Resources and Isolation&lt;/h2&gt;

&lt;p&gt;To be able to deploy workloads with hardware dependencies, Kubernetes provides excellent extension
and configurability mechanisms. Let’s take a closer look into Kubernetes the &lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/&#34; target=&#34;_blank&#34;&gt;device plugin framework&lt;/a&gt;
(beta in 1.14) and &lt;a href=&#34;https://kubernetes.io/docs/concepts/containers/runtime-class/&#34; target=&#34;_blank&#34;&gt;RuntimeClass&lt;/a&gt; (beta in 1.14) and learn how they can be leveraged to expose crypto
hardware to workloads.&lt;/p&gt;

&lt;p&gt;The device plugin framework, first introduced in Kubernetes 1.8, provides a way for hardware vendors
to register and allocate node hardware resources to Kubelets. The plugins implement the hardware
specific initialization logic and resource management. The pods can request hardware resources in
their PodSpec, which also guarantees the pod is scheduled on a node that can provide those resources.&lt;/p&gt;

&lt;p&gt;The device resource allocation for containers is non-trivial. For applications dealing with security,
the hardware level isolation is critical. The PCIe based crypto acceleration device functions can
benefit from IO hardware virtualization, through an I/O Memory Management Unit (IOMMU), to provide
the isolation: an &lt;em&gt;IOMMU group&lt;/em&gt; the device belongs to provides the isolated resource for a workload
(assuming the crypto cards do not share the IOMMU group with other devices). The number of isolated
resources can be further increased if the PCIe device supports the Single-Root I/O Virtualization
(SR-IOV) specification. SR-IOV allows the PCIe device to be split further to &lt;em&gt;virtual functions&lt;/em&gt; (VF),
derived from &lt;em&gt;physical function&lt;/em&gt; (PF) devices, and each belonging to their own IOMMU group. To expose
these IOMMU isolated device functions to user space and containers, the host kernel should bind
them to a specific device driver. In Linux, this  driver is vfio-pci and it makes each device
available through a character device in user space. The kernel vfio-pci driver provides user space
applications with a direct, IOMMU backed access to PCIe devices and functions, using a mechanism
called &lt;em&gt;PCI passthrough&lt;/em&gt;. The interface can be leveraged by user space frameworks, such as the
Data Plane Development Kit (DPDK). Additionally, virtual machine (VM) hypervisors can provide
these user space device nodes to VMs and expose them as PCI devices to the guest kernel.
Assuming support from the guest kernel, the VM gets close to native performant direct access to the
underlying host devices.&lt;/p&gt;

&lt;p&gt;To advertise these device resources to Kubernetes, we can have a simple Kubernetes device plugin
that runs the initialization (i.e., binding), calls kubelet’s &lt;code&gt;Registration&lt;/code&gt; gRPC service, and
implements the DevicePlugin gRPC service that kubelet calls to, e.g., to &lt;code&gt;Allocate&lt;/code&gt; the resources
upon Pod creation.&lt;/p&gt;

&lt;h2 id=&#34;device-assignment-and-pod-deployment&#34;&gt;Device Assignment and Pod Deployment&lt;/h2&gt;

&lt;p&gt;At this point, you may ask what the container could do with a VFIO device node? The answer comes
after we first take a quick look into the Kubernetes RuntimeClass.&lt;/p&gt;

&lt;p&gt;The Kubernetes RuntimeClass was created to provide better control and configurability
over a variety of &lt;em&gt;runtimes&lt;/em&gt; (an earlier &lt;a href=&#34;https://kubernetes.io/blog/2018/10/10/kubernetes-v1.12-introducing-runtimeclass/&#34; target=&#34;_blank&#34;&gt;blog post&lt;/a&gt; goes into the details of the needs,
status and roadmap for it) that are available in the cluster. In essence, the RuntimeClass
provides cluster users better tools to pick and use the runtime that best suits for the pod use case.&lt;/p&gt;

&lt;p&gt;The OCI compatible &lt;a href=&#34;https://katacontainers.io/&#34; target=&#34;_blank&#34;&gt;Kata Containers runtime&lt;/a&gt; provides workloads with a hardware virtualized
isolation layer. In addition to workload isolation, the Kata Containers VM has the added
side benefit that the VFIO devices, as &lt;code&gt;Allocate&lt;/code&gt;’d by the device plugin, can be passed
through to the container as hardware isolated devices. The only requirement is that the
Kata Containers kernel has driver for the exposed device enabled.&lt;/p&gt;

&lt;p&gt;That’s all it really takes to enable hardware accelerated crypto for container workloads. To summarize:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Cluster needs a device plugin running on the node that provides the hardware&lt;/li&gt;
&lt;li&gt;Device plugin exposes the hardware to user space using  the VFIO driver&lt;/li&gt;
&lt;li&gt;Pod requests the device resources and Kata Containers as the RuntimeClass in the PodSpec&lt;/li&gt;
&lt;li&gt;The container has the hardware adaptation library and the OpenSSL engine module&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Figure 2 shows the overall setup using the Container A illustrated earlier.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-04-23-hardware-accelerated-tls-termination/k8s-blog-fig2.png&#34;
         alt=&#34;Figure 2. Deployment overview&#34; width=&#34;600&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;Figure 2. Deployment overview&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;


&lt;h2 id=&#34;reference-setup&#34;&gt;Reference Setup&lt;/h2&gt;

&lt;p&gt;Finally, we describe the necessary building blocks and steps to build a functional
setup described in Figure 2 that enables hardware accelerated SSL termination in
an Ingress Controller using an Intel&amp;reg; QuickAssist Technology (QAT) PCIe device.
It should be noted that the use cases are not limited to Ingress controllers, but
any OpenSSL based workload can be accelerated.&lt;/p&gt;

&lt;h3 id=&#34;cluster-configuration&#34;&gt;Cluster configuration:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Kubernetes 1.14 (&lt;code&gt;RuntimeClass&lt;/code&gt; and &lt;code&gt;DevicePlugin&lt;/code&gt; feature gates enabled (both are &lt;code&gt;true&lt;/code&gt; in 1.14)&lt;/li&gt;
&lt;li&gt;RuntimeClass ready runtime and Kata Containers configured&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;host-configuration&#34;&gt;Host configuration:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Intel&amp;reg; QAT driver release with the kernel drivers installed for both host kernel and Kata Containers kernel (or on a rootfs as loadable modules)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/intel/intel-device-plugins-for-kubernetes/tree/master/cmd/qat_plugin&#34; target=&#34;_blank&#34;&gt;QAT device plugin&lt;/a&gt; DaemonSet deployed&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;ingress-controller-configuration-and-deployment&#34;&gt;Ingress controller configuration and deployment:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/jcmoraisjr/haproxy-ingress&#34; target=&#34;_blank&#34;&gt;HAproxy-ingress&lt;/a&gt; ingress controller in a modified container that has

&lt;ul&gt;
&lt;li&gt;the QAT HW HAL user space library (part of Intel&amp;reg; QAT SW release) and&lt;/li&gt;
&lt;li&gt;the &lt;a href=&#34;https://github.com/intel/QAT_Engine/&#34; target=&#34;_blank&#34;&gt;OpenSSL QAT Engine&lt;/a&gt; built in&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Haproxy-ingress ConfigMap to enable QAT engine usage

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ssl-engine=”qat”&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ssl-mode-async=true&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Haproxy-ingress deployment &lt;code&gt;.yaml&lt;/code&gt; to

&lt;ul&gt;
&lt;li&gt;Request &lt;code&gt;qat.intel.com: n&lt;/code&gt; resources&lt;/li&gt;
&lt;li&gt;Request &lt;code&gt;runtimeClassName: kata-containers&lt;/code&gt; (name value depends on cluster config)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;(QAT device config file for each requested device resource with OpenSSL engine configured available in the container)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Once the building blocks are available, the hardware accelerated SSL/TLS can be tested by following the &lt;a href=&#34;https://github.com/jcmoraisjr/haproxy-ingress/tree/master/examples/tls-termination&#34; target=&#34;_blank&#34;&gt;TLS termination
example&lt;/a&gt; steps. In order to verify the hardware is used, you can check &lt;code&gt;/sys/kernel/debug/*/fw_counters&lt;/code&gt; files on host as they
get updated by the Intel&amp;reg; QAT firmware.&lt;/p&gt;

&lt;p&gt;Haproxy-ingress and HAproxy are used because HAproxy can be directly configured to use the OpenSSL engine using
&lt;code&gt;ssl-engine &amp;lt;name&amp;gt; [algo ALGOs]&lt;/code&gt; configuration flag without modifications to the global openssl configuration file.
Moreover, HAproxy can offload configured algorithms using asynchronous calls (with &lt;code&gt;ssl-mode-async&lt;/code&gt;) to further improve performance.&lt;/p&gt;

&lt;h2 id=&#34;call-to-action&#34;&gt;Call to Action&lt;/h2&gt;

&lt;p&gt;In this blog post we have shown how Kubernetes Device Plugins and RuntimeClass can be used to provide isolated hardware
access for applications in pods to offload crypto operations to hardware accelerators. Hardware accelerators can be used
to speed up crypto operations and also save CPU cycles to other tasks. We demonstrated the setup using HAproxy that already
supports asynchronous crypto offload with OpenSSL.&lt;/p&gt;

&lt;p&gt;The next steps for our team is to repeat the same for Envoy (with an OpenSSL based TLS transport socket built
as an extension). Furthermore, we are working to enhance Envoy to be able to &lt;a href=&#34;https://github.com/envoyproxy/envoy/issues/6248&#34; target=&#34;_blank&#34;&gt;offload BoringSSL asynchronous
private key operations&lt;/a&gt; to a crypto acceleration hardware. Any review feedback or help is appreciated!&lt;/p&gt;

&lt;p&gt;How many CPU cycles can your crypto application save for other tasks when offloading crypto processing to a dedicated accelerator?&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Introducing kube-iptables-tailer: Better Networking Visibility in Kubernetes Clusters</title>
      <link>https://kubernetes.io/blog/2019/04/19/introducing-kube-iptables-tailer/</link>
      <pubDate>Fri, 19 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/04/19/introducing-kube-iptables-tailer/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Saifuding Diliyaer, Software Engineer, Box&lt;/p&gt;

&lt;p&gt;At Box, we use Kubernetes to empower our engineers to own the whole lifecycle of their microservices. When it comes to networking, our engineers use Tigera’s &lt;a href=&#34;https://www.tigera.io/tigera-calico/&#34; target=&#34;_blank&#34;&gt;Project Calico&lt;/a&gt; to declaratively manage network policies for their apps running in our Kubernetes clusters. App owners define a Calico policy in order to enable their Pods to send/receive network traffic, which is instantiated as iptables rules.&lt;/p&gt;

&lt;p&gt;There may be times, however, when such network policy is missing or declared incorrectly by app owners. In this situation, the iptables rules will cause network packet drops between the affected Pods, which get logged in a file that is inaccessible to app owners. We needed a mechanism to seamlessly deliver alerts about those iptables packet drops based on their network policies to help app owners quickly diagnose the corresponding issues. To solve this, we developed a service called &lt;a href=&#34;https://github.com/box/kube-iptables-tailer&#34; target=&#34;_blank&#34;&gt;kube-iptables-tailer&lt;/a&gt; to detect packet drops from iptables logs and report them as Kubernetes events. We are proud to open-source kube-iptables-tailer for you to utilize in your own cluster, regardless of whether you use Calico or other network policy tools.&lt;/p&gt;

&lt;h2 id=&#34;improved-experience-for-app-owners&#34;&gt;Improved Experience for App Owners&lt;/h2&gt;

&lt;p&gt;App owners do not have to apply any additional changes to utilize kube-iptables-tailer. They can simply run &lt;code&gt;kubectl describe pods&lt;/code&gt; to check if any of their Pods&amp;rsquo; traffic has been dropped due to iptables rules. All the results sent from kube-iptables-tailer will be shown under the &lt;em&gt;Events&lt;/em&gt; section, which is a much better experience for developers when compared to reading through raw iptables logs.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;$ kubectl describe pods --namespace&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;YOUR_NAMESPACE

...
Events:
 Type     Reason      Age    From                    Message
 ----     ------      ----   ----                    -------    
 Warning  PacketDrop  5s     kube-iptables-tailer    Packet dropped when receiving traffic from example-service-2 &lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;IP: &lt;span style=&#34;color:#666&#34;&gt;22&lt;/span&gt;.222.22.222&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;.

 Warning  PacketDrop  10m    kube-iptables-tailer    Packet dropped when sending traffic to example-service-1 &lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;IP: &lt;span style=&#34;color:#666&#34;&gt;11&lt;/span&gt;.111.11.111&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;.&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;em&gt;* output of events sent from kube-iptables-tailer to Kubernetes Pods having networking issues&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;process-behind-kube-iptables-tailer&#34;&gt;Process behind kube-iptables-tailer&lt;/h2&gt;

&lt;p&gt;Before we had kube-iptables-tailer, the only way for Box’s engineers to get information about packet drops related to their network policies was parsing through the raw iptables logs and matching their service IPs. This was a suboptimal experience because iptables logs only contain basic IP address information. Mapping these IPs to specific Pods could be painful, especially in the Kubernetes world where Pods and containers are ephemeral and IPs are frequently changing. This process involved a bunch of manual commands for our engineers. Additionally, iptables logs could be noisy due to a number of drops, and if IP addresses were being reused, the app owners might even have some stale data. With the help of kube-iptables-tailer, life now becomes much easier for our developers. As shown in the following diagram, the principle of this service can be divided into three steps:
&lt;img src=&#34;https://i.imgur.com/fGAIVuS.png&#34; alt=&#34;sequence diagram for kube-iptables-tailer&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;* sequence diagram for kube-iptables-tailer&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;1-watch-changes-on-iptables-log-file&#34;&gt;1. Watch changes on iptables log file&lt;/h3&gt;

&lt;p&gt;Instead of requiring human engineers to manually decipher the raw iptables logs, we now use kube-iptables-tailer to help identify changes in that file. We run the service as a &lt;strong&gt;DaemonSet&lt;/strong&gt; on every host node in our cluster, and it tails the iptables log file periodically. The service itself is written in Go, and it has multiple goroutines for the different service components running concurrently. We use channels to share information among those various components. In this step, for instance, the service will send out any changes it detected in iptables log file to a Go channel to be parsed later.&lt;/p&gt;

&lt;h3 id=&#34;2-parse-iptables-logs-based-on-log-prefix&#34;&gt;2. Parse iptables logs based on log prefix&lt;/h3&gt;

&lt;p&gt;Once the parser receives a new log message through a particular Go channel, it will first check whether the log message includes any network policy related packet drop information by parsing the log prefix. Packet drops based on our Calico policies will be logged containing “calico-drop:” as the log prefix in iptables log file. In this case, an object will be created by the parser with the data from the log message being stored as the object’s fields. These handy objects will be later used to locate the relevant Pods running in Kubernetes and post notifications directly to them. The parser is also able to identify duplicate logs and filter them to avoid causing confusion and consuming extra resources. After the parsing process, it will come to the final step for kube-iptables-tailer to send out the results.&lt;/p&gt;

&lt;h3 id=&#34;3-locate-pods-and-send-out-events&#34;&gt;3. Locate pods and send out events&lt;/h3&gt;

&lt;p&gt;Using the Kubernetes API, kube-iptables-tailer will try locating both senders and receivers in our cluster by matching the IPs stored in objects parsed from the previous step. As a result, an event will be posted to these affected Pods if they are located successfully. Kubernetes events are objects designed to provide information about what is happening inside a Kubernetes component. At Box, one of the use cases for Kubernetes events is to report errors directly to the corresponding applications (for more details, please refer to this &lt;a href=&#34;https://kubernetes.io/blog/2018/01/reporting-errors-using-kubernetes-events/&#34; target=&#34;_blank&#34;&gt;blog post&lt;/a&gt;). The event generated by kube-iptables-tailer includes useful information such as traffic direction, IPs and the namespace of Pods from the other side. We have added DNS lookup as well because our Pods also send and receive traffic from services running on bare-metal hosts and VMs. Besides, exponential backoff is implemented to avoid overwhelming the Kubernetes API server.&lt;/p&gt;

&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;

&lt;p&gt;At Box, kube-iptables-tailer has saved time as well as made life happier for many developers across various teams. Instead of flying blind with regards to packet drops based on network policies, the service is able to help detect changes in iptables log file and get the corresponding information delivered right to the Pods inside Kubernetes clusters. If you’re not using Calico, you can still apply any other log prefix (configured as an environment variable in the service) to match whatever is defined in your iptables rules and get notified about the network policy related packet drops. You may also find other cases where it is useful to make information from host systems available to Pods via the Kubernetes API. As an open-sourced project, every contribution is more than welcome to help improve the project together. You can find this project hosted on Github at &lt;a href=&#34;https://github.com/box/kube-iptables-tailer&#34; target=&#34;_blank&#34;&gt;https://github.com/box/kube-iptables-tailer&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Special thanks to &lt;a href=&#34;https://www.linkedin.com/in/kunalparmar/&#34; target=&#34;_blank&#34;&gt;Kunal Parmar&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/greg-lyons-8277a188/&#34; target=&#34;_blank&#34;&gt;Greg Lyons&lt;/a&gt; and &lt;a href=&#34;https://www.linkedin.com/in/shrenikd/&#34; target=&#34;_blank&#34;&gt;Shrenik Dedhia&lt;/a&gt; for contributing to this project.&lt;/em&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: The Future of Cloud Providers in Kubernetes</title>
      <link>https://kubernetes.io/blog/2019/04/17/the-future-of-cloud-providers-in-kubernetes/</link>
      <pubDate>Wed, 17 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/04/17/the-future-of-cloud-providers-in-kubernetes/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Andrew Sy Kim (VMware), Mike Crute (AWS), Walter Fender (Google)&lt;/p&gt;

&lt;p&gt;Approximately 9 months ago, the Kubernetes community agreed to form the Cloud Provider Special Interest Group (SIG). The justification was to have a single governing SIG to own and shape the integration points between Kubernetes and the many cloud providers it supported. A lot has been in motion since then and we’re here to share with you what has been accomplished so far and what we hope to see in the future.&lt;/p&gt;

&lt;h2 id=&#34;the-mission&#34;&gt;The Mission&lt;/h2&gt;

&lt;p&gt;First and foremost, I want to share what the mission of the SIG is, because we use it to guide our present &amp;amp; future work. Taken straight from our &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/sig-cloud-provider/CHARTER.md&#34; target=&#34;_blank&#34;&gt;charter&lt;/a&gt;, the mission of the SIG is to simplify, develop and maintain cloud provider integrations as extensions, or add-ons, to Kubernetes clusters. The motivation behind this is two-fold: to ensure Kubernetes remains extensible and cloud agnostic.&lt;/p&gt;

&lt;h2 id=&#34;the-current-state-of-cloud-providers&#34;&gt;The Current State of Cloud Providers&lt;/h2&gt;

&lt;p&gt;In order to gain a forward looking perspective to our work, I think it’s important to take a step back to look at the current state of cloud providers. Today, each core Kubernetes component (except the scheduler and kube-proxy) has a &amp;ndash;cloud-provider flag you can configure to enable a set of functionalities that integrate with the underlying infrastructure provider, a.k.a the cloud provider. Enabling this integration unlocks a wide set of features for your clusters such as: node address &amp;amp; zone discovery, cloud load balancers for Services with Type=LoadBalancer, IP address management, and cluster networking via VPC routing tables. Today, the cloud provider integrations can be done either in-tree or out-of-tree.&lt;/p&gt;

&lt;h2 id=&#34;in-tree-out-of-tree-providers&#34;&gt;In-Tree &amp;amp; Out-of-Tree Providers&lt;/h2&gt;

&lt;p&gt;In-tree cloud providers are the providers we develop &amp;amp; release in the &lt;a href=&#34;https://github.com/kubernetes/kubernetes/tree/master/pkg/cloudprovider/providers&#34; target=&#34;_blank&#34;&gt;main Kubernetes repository&lt;/a&gt;. This results in embedding the knowledge and context of each cloud provider into most of the Kubernetes components. This enables more native integrations such as the kubelet requesting information about itself via a metadata service from the cloud provider.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/docs/pre-ccm-arch.png&#34;
         alt=&#34;In-Tree Cloud Provider Architecture (source: kubernetes.io)&#34; width=&#34;600&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;In-Tree Cloud Provider Architecture (source: kubernetes.io)&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Out-of-tree cloud providers are providers that can be developed, built, and released independent of Kubernetes core. This requires deploying a new component called the cloud-controller-manager which is responsible for running all the cloud specific controllers that were previously run in the kube-controller-manager.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/docs/post-ccm-arch.png&#34;
         alt=&#34;Out-of-Tree Cloud Provider Architecture (source: kubernetes.io)&#34; width=&#34;600&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;Out-of-Tree Cloud Provider Architecture (source: kubernetes.io)&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;When cloud provider integrations were initially developed, they were developed natively (in-tree). We integrated each provider close to the core of Kubernetes and within the monolithic repository that is k8s.io/kubernetes today. As Kubernetes became more ubiquitous and more infrastructure providers wanted to support Kubernetes natively, we realized that this model was not going to scale. Each provider brings along a large set of dependencies which increases potential vulnerabilities in our code base and significantly increases the binary size of each component. In addition to this, more of the Kubernetes release notes started to focus on provider specific changes rather than core changes that impacted all Kubernetes users.&lt;/p&gt;

&lt;p&gt;In late 2017, we developed a way for cloud providers to build integrations without adding them to the main Kubernetes tree (out-of-tree). This became the de-facto way for new infrastructure providers in the ecosystem to integrate with Kubernetes.  Since then, we’ve been actively working towards migrating all cloud providers to use the out-of-tree architecture as most clusters today are still using the in-tree cloud providers.&lt;/p&gt;

&lt;h2 id=&#34;looking-ahead&#34;&gt;Looking Ahead&lt;/h2&gt;

&lt;p&gt;Looking ahead, the goal of the SIG is to remove all existing in-tree cloud providers in favor of their out-of-tree equivalents with minimal impact to users. In addition to the core cloud provider integration mentioned above, there are more extension points for cloud integrations like CSI and the image credential provider that are actively being worked on for v1.15. Getting to this point would mean that Kubernetes is truly cloud-agnostic with no native integrations for any cloud provider. By doing this work we empower each cloud provider to develop and release new versions at their own cadence independent of Kubernetes.  We’ve learned by now that this is a large feat with a unique set of challenges. Migrating workloads is never easy, especially when it’s an essential part of the control plane. Providing a safe and easy migration path between in-tree and out-of-tree cloud providers is of the highest priority for our SIG in the upcoming releases. If any of this sounds interesting to you, I encourage you to check out of some of our &lt;a href=&#34;https://github.com/kubernetes/enhancements/tree/master/keps/sig-cloud-provider&#34; target=&#34;_blank&#34;&gt;KEPs&lt;/a&gt; and get in touch with our SIG by joining the &lt;a href=&#34;https://groups.google.com/forum/#!forum/kubernetes-sig-cloud-provider&#34; target=&#34;_blank&#34;&gt;mailing list&lt;/a&gt; or our slack channel (#sig-cloud-provider in Kubernetes slack).&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Pod Priority and Preemption in Kubernetes</title>
      <link>https://kubernetes.io/blog/2019/04/16/pod-priority-and-preemption-in-kubernetes/</link>
      <pubDate>Tue, 16 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/04/16/pod-priority-and-preemption-in-kubernetes/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Bobby Salamat&lt;/p&gt;

&lt;p&gt;Kubernetes is well-known for running scalable workloads. It scales your workloads based on their resource usage. When a workload is scaled up, more instances of the application get created. When the application is critical for your product, you want to make sure that these new instances are scheduled even when your cluster is under resource pressure. One obvious solution to this problem is to over-provision your cluster resources to have some amount of slack resources available for scale-up situations. This approach often works, but costs more as you would have to pay for the resources that are idle most of the time.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/&#34; target=&#34;_blank&#34;&gt;Pod priority and preemption&lt;/a&gt; is a scheduler feature made generally available in Kubernetes 1.14 that allows you to achieve high levels of scheduling confidence for your critical workloads without overprovisioning your clusters. It also provides a way to improve resource utilization in your clusters without sacrificing the reliability of your essential workloads.&lt;/p&gt;

&lt;h2 id=&#34;guaranteed-scheduling-with-controlled-cost&#34;&gt;Guaranteed scheduling with controlled cost&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/cluster-management/#cluster-autoscaling&#34; target=&#34;_blank&#34;&gt;Kubernetes Cluster Autoscaler&lt;/a&gt; is an excellent tool in the ecosystem which adds more nodes to your cluster when your applications need them. However, cluster autoscaler has some limitations and may not work for all users:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;It does not work in physical clusters.&lt;/li&gt;
&lt;li&gt;Adding more nodes to the cluster costs more.&lt;/li&gt;
&lt;li&gt;Adding nodes is not instantaneous and could take minutes before those nodes become available for scheduling.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;An alternative is Pod Priority and Preemption. In this approach, you combine multiple workloads in a single cluster. For example, you may run your CI/CD pipeline, ML workloads, and your critical service in the same cluster. When multiple workloads run in the same cluster, the size of your cluster is larger than a cluster that you would use to run only your critical service. If you give your critical service the highest priority and your CI/CD and ML workloads lower priority, when your service needs more computing resources, the scheduler preempts (evicts) enough pods of your lower priority workloads, e.g., ML workload, to allow all your higher priority pods to schedule.&lt;/p&gt;

&lt;p&gt;With pod priority and preemption you can set a maximum size for your cluster in the Autoscaler configuration to ensure your costs get controlled without sacrificing availability of your service. Moreover, preemption is much faster than adding new nodes to the cluster. Within seconds your high priority pods are scheduled, which is critical for latency sensitive services.&lt;/p&gt;

&lt;h2 id=&#34;improve-cluster-resource-utilization&#34;&gt;Improve cluster resource utilization&lt;/h2&gt;

&lt;p&gt;Cluster operators who run critical services learn over time a rough estimate of the number of nodes that they need in their clusters to achieve high service availability. The estimate is usually conservative. Such estimates take bursts of traffic into account to find the number of required nodes. Cluster autoscaler can be configured never to reduce the size of the cluster below this level. The only problem is that such estimates are often conservative and cluster resources may remain underutilized most of the time. Pod priority and preemption allows you to improve resource utilization significantly by running a non-critical workload in the cluster.&lt;/p&gt;

&lt;p&gt;The non-critical workload may have many more pods that can fit in the cluster. If you give a negative priority to your non-critical workload, Cluster Autoscaler does not add more nodes to your cluster when the non-critical pods are pending. Therefore, you won’t incur higher expenses. When your critical workload requires more computing resources, the scheduler preempts non-critical pods and schedules critical ones.&lt;/p&gt;

&lt;p&gt;The non-critical pods fill the “holes” in your cluster resources which improves resource utilization without raising your costs.&lt;/p&gt;

&lt;h2 id=&#34;get-involved&#34;&gt;Get Involved&lt;/h2&gt;

&lt;p&gt;If you have feedback for this feature or are interested in getting involved with the design and development, join the &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-scheduling&#34; target=&#34;_blank&#34;&gt;Scheduling Special Interest Group&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Process ID Limiting for Stability Improvements in Kubernetes 1.14</title>
      <link>https://kubernetes.io/blog/2019/04/15/process-id-limiting-for-stability-improvements-in-kubernetes-1.14/</link>
      <pubDate>Mon, 15 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/04/15/process-id-limiting-for-stability-improvements-in-kubernetes-1.14/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author: Derek Carr&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Have you ever seen someone take more than their fair share of the cookies? The one person who reaches in and grabs a half dozen fresh baked chocolate chip chunk morsels and skitters off like Cookie Monster exclaiming “Om nom nom nom.”&lt;/p&gt;

&lt;p&gt;In some rare workloads, a similar occurrence was taking place inside Kubernetes clusters. With each Pod and Node, there comes a finite number of possible process IDs (PIDs) for all applications to share. While it is rare for any one process or pod to reach in and grab all the PIDs, some users were experiencing resource starvation due to this type of behavior. So in Kubernetes 1.14, we introduced an enhancement to mitigate the risk of a single pod monopolizing all of the PIDs available.&lt;/p&gt;

&lt;h2 id=&#34;can-you-spare-some-pids&#34;&gt;Can You Spare Some PIDs?&lt;/h2&gt;

&lt;p&gt;Here, we’re talking about the greed of certain containers. Outside the ideal, runaway processes occur from time to time, particularly in clusters where testing is taking place. Thus, some wildly non-production-ready activity is happening.&lt;/p&gt;

&lt;p&gt;In such a scenario, it’s possible for something akin to a fork bomb taking place inside a node. As resources slowly erode, being taken over by some zombie-like process that continually spawns children, other legitimate workloads begin to get bumped in favor of this inflating balloon of wasted processing power. This could result in other processes on the same pod being starved of their needed PIDs. It could also lead to interesting side effects as a node could fail and a replica of that pod is scheduled to a new machine where the process repeats across your entire cluster.&lt;/p&gt;

&lt;h2 id=&#34;fixing-the-problem&#34;&gt;Fixing the Problem&lt;/h2&gt;

&lt;p&gt;Thus, in Kubernetes 1.14, we have added a feature that allows for the configuration of a kubelet to limit the number of PIDs a given pod can consume. If that machine supports 32,768 PIDs and 100 pods, one can give each pod a budget of 300 PIDs to prevent total exhaustion of PIDs. If the admin wants to overcommit PIDs similar to cpu or memory, they may do so as well with some additional risks. Either way, no one pod can bring the whole machine down. This will generally prevent against simple fork bombs from taking over your cluster.&lt;/p&gt;

&lt;p&gt;This change allows administrators to protect one pod from another, but does not ensure if all pods on the machine can protect the node, and the node agents themselves from falling over. Thus, we’ve introduced a feature in this release in alpha form that provides isolation of PIDs from end user workloads on a pod from the node agents (kubelet, runtime, etc.). The admin is able to reserve a specific number of PIDs&amp;ndash;similar to how one reserves CPU or memory today&amp;ndash;and ensure they are never consumed by pods on that machine. Once that graduates from alpha, to beta, then stable in future releases of Kubernetes, we’ll have protection against an easily starved Linux resource.&lt;/p&gt;

&lt;p&gt;Get started with &lt;a href=&#34;https://github.com/kubernetes/kubernetes/releases/tag/v1.14.0&#34; target=&#34;_blank&#34;&gt;Kubernetes 1.14&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;get-involved&#34;&gt;Get Involved&lt;/h2&gt;

&lt;p&gt;If you have feedback for this feature or are interested in getting involved with the design and development, join the &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-node&#34; target=&#34;_blank&#34;&gt;Node Special Interest Group&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;about-the-author&#34;&gt;About the author:&lt;/h3&gt;

&lt;p&gt;Derek Carr is Senior Principal Software Engineer at Red Hat. He is a Kubernetes contributor and member of the Kubernetes Community Steering Committee.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes 1.14: Local Persistent Volumes GA</title>
      <link>https://kubernetes.io/blog/2019/04/04/kubernetes-1.14-local-persistent-volumes-ga/</link>
      <pubDate>Thu, 04 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/04/04/kubernetes-1.14-local-persistent-volumes-ga/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt;: Michelle Au (Google), Matt Schallert (Uber), Celina Ward (Uber)&lt;/p&gt;

&lt;p&gt;The &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/volumes/#local&#34; target=&#34;_blank&#34;&gt;Local Persistent Volumes&lt;/a&gt;
feature has been promoted to GA in Kubernetes 1.14.
It was first introduced as alpha in Kubernetes 1.7, and then
&lt;a href=&#34;https://kubernetes.io/blog/2018/04/13/local-persistent-volumes-beta/&#34; target=&#34;_blank&#34;&gt;beta&lt;/a&gt; in Kubernetes
1.10. The GA milestone indicates that Kubernetes users may depend on the feature
and its API for production use. GA features are protected by the Kubernetes
&lt;a href=&#34;https://kubernetes.io/docs/reference/using-api/deprecation-policy/&#34; target=&#34;_blank&#34;&gt;deprecation
policy&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;what-is-a-local-persistent-volume&#34;&gt;What is a Local Persistent Volume?&lt;/h2&gt;

&lt;p&gt;A local persistent volume represents a local disk directly-attached to a single
Kubernetes Node.&lt;/p&gt;

&lt;p&gt;Kubernetes provides a powerful volume plugin system that enables Kubernetes
workloads to use a &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/volumes/#types-of-volumes&#34; target=&#34;_blank&#34;&gt;wide
variety&lt;/a&gt;
of block and file storage to persist data. Most
of these plugins enable remote storage &amp;ndash; these remote storage systems persist
data independent of the Kubernetes node where the data originated. Remote
storage usually can not offer the consistent high performance guarantees of
local directly-attached storage. With the Local Persistent Volume plugin,
Kubernetes workloads can now consume high performance local storage using the
same volume APIs that app developers have become accustomed to.&lt;/p&gt;

&lt;h2 id=&#34;how-is-it-different-from-a-hostpath-volume&#34;&gt;How is it different from a HostPath Volume?&lt;/h2&gt;

&lt;p&gt;To better understand the benefits of a Local Persistent Volume, it is useful to
compare it to a &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/volumes/#hostpath&#34; target=&#34;_blank&#34;&gt;HostPath volume&lt;/a&gt;.
HostPath volumes mount a file or directory from
the host node’s filesystem into a Pod. Similarly a Local Persistent Volume
mounts a local disk or partition into a Pod.&lt;/p&gt;

&lt;p&gt;The biggest difference is that the Kubernetes scheduler understands which node a
Local Persistent Volume belongs to. With HostPath volumes, a pod referencing a
HostPath volume may be moved by the scheduler to a different node resulting in
data loss. But with Local Persistent Volumes, the Kubernetes scheduler ensures
that a pod using a Local Persistent Volume is always scheduled to the same node.&lt;/p&gt;

&lt;p&gt;While HostPath volumes may be referenced via a Persistent Volume Claim (PVC) or
directly inline in a pod definition, Local Persistent Volumes can only be
referenced via a PVC. This provides additional security benefits since
Persistent Volume objects are managed by the administrator, preventing Pods from
being able to access any path on the host.&lt;/p&gt;

&lt;p&gt;Additional benefits include support for formatting of block devices during
mount, and volume ownership using fsGroup.&lt;/p&gt;

&lt;h2 id=&#34;what-s-new-with-ga&#34;&gt;What&amp;rsquo;s New With GA?&lt;/h2&gt;

&lt;p&gt;Since 1.10, we have mainly focused on improving stability and scalability of the
feature so that it is production ready.&lt;/p&gt;

&lt;p&gt;The only major feature addition is the ability to specify a raw block device and
have Kubernetes automatically format and mount the filesystem. This reduces the
previous burden of having to format and mount devices before giving it to
Kubernetes.&lt;/p&gt;

&lt;h2 id=&#34;limitations-of-ga&#34;&gt;Limitations of GA&lt;/h2&gt;

&lt;p&gt;At GA, Local Persistent Volumes do not support &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/dynamic-provisioning/&#34; target=&#34;_blank&#34;&gt;dynamic volume
provisioning&lt;/a&gt;.
However there is an &lt;a href=&#34;https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner&#34; target=&#34;_blank&#34;&gt;external
controller&lt;/a&gt;
available to help manage the local
PersistentVolume lifecycle for individual disks on your nodes. This includes
creating the PersistentVolume objects, cleaning up and reusing disks once they
have been released by the application.&lt;/p&gt;

&lt;h2 id=&#34;how-to-use-a-local-persistent-volume&#34;&gt;How to Use a Local Persistent Volume?&lt;/h2&gt;

&lt;p&gt;Workloads can request a local persistent volume using the same
PersistentVolumeClaim interface as remote storage backends. This makes it easy
to swap out the storage backend across clusters, clouds, and on-prem
environments.&lt;/p&gt;

&lt;p&gt;First, a StorageClass should be created that sets &lt;code&gt;volumeBindingMode:
WaitForFirstConsumer&lt;/code&gt; to enable &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode&#34; target=&#34;_blank&#34;&gt;volume topology-aware
scheduling&lt;/a&gt;.
This mode instructs Kubernetes to wait to bind a PVC until a Pod using it is scheduled.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then, the external static provisioner can be &lt;a href=&#34;https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner#user-guide&#34; target=&#34;_blank&#34;&gt;configured and
run&lt;/a&gt; to create PVs
for all the local disks on your nodes.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pv
NAME                CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM  STORAGECLASS   REASON      AGE
local-pv-27c0f084   368Gi      RWO            Delete           Available          local-storage              8s
local-pv-3796b049   368Gi      RWO            Delete           Available          local-storage              7s
local-pv-3ddecaea   368Gi      RWO            Delete           Available          local-storage              7s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Afterwards, workloads can start using the PVs by creating a PVC and Pod or a
StatefulSet with volumeClaimTemplates.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: local-test
spec:
  serviceName: &amp;quot;local-service&amp;quot;
  replicas: 3
  selector:
    matchLabels:
      app: local-test
  template:
    metadata:
      labels:
        app: local-test
    spec:
      containers:
      - name: test-container
        image: k8s.gcr.io/busybox
        command:
        - &amp;quot;/bin/sh&amp;quot;
        args:
        - &amp;quot;-c&amp;quot;
        - &amp;quot;sleep 100000&amp;quot;
        volumeMounts:
        - name: local-vol
          mountPath: /usr/test-pod
  volumeClaimTemplates:
  - metadata:
      name: local-vol
    spec:
      accessModes: [ &amp;quot;ReadWriteOnce&amp;quot; ]
      storageClassName: &amp;quot;local-storage&amp;quot;
      resources:
        requests:
          storage: 368Gi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once the StatefulSet is up and running, the PVCs are all bound:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pvc
NAME                     STATUS   VOLUME              CAPACITY   ACCESS MODES   STORAGECLASS      AGE
local-vol-local-test-0   Bound    local-pv-27c0f084   368Gi      RWO            local-storage     3m45s
local-vol-local-test-1   Bound    local-pv-3ddecaea   368Gi      RWO            local-storage     3m40s
local-vol-local-test-2   Bound    local-pv-3796b049   368Gi      RWO            local-storage     3m36s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When the disk is no longer needed, the PVC can be deleted. The external static provisioner
will clean up the disk and make the PV available for use again.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl patch sts local-test -p &#39;{&amp;quot;spec&amp;quot;:{&amp;quot;replicas&amp;quot;:2}}&#39;
statefulset.apps/local-test patched

$ kubectl delete pvc local-vol-local-test-2
persistentvolumeclaim &amp;quot;local-vol-local-test-2&amp;quot; deleted

$ kubectl get pv
NAME                CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                            STORAGECLASS   REASON      AGE
local-pv-27c0f084   368Gi      RWO            Delete           Bound       default/local-vol-local-test-0   local-storage              11m
local-pv-3796b049   368Gi      RWO            Delete           Available                                    local-storage              7s
local-pv-3ddecaea   368Gi      RWO            Delete           Bound       default/local-vol-local-test-1   local-storage              19m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can find full &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/volumes/#local&#34; target=&#34;_blank&#34;&gt;documentation&lt;/a&gt;
for the feature on the Kubernetes website.&lt;/p&gt;

&lt;h2 id=&#34;what-are-suitable-use-cases&#34;&gt;What Are Suitable Use Cases?&lt;/h2&gt;

&lt;p&gt;The primary benefit of Local Persistent Volumes over remote persistent storage
is performance: local disks usually offer higher IOPS and throughput and lower
latency compared to remote storage systems.&lt;/p&gt;

&lt;p&gt;However, there are important limitations and caveats to consider when using
Local Persistent Volumes:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Using local storage ties your application to a specific node, making your
application harder to schedule. Applications which use local storage should
specify a high priority so that lower priority pods, that don’t require local
storage, can be preempted if necessary.&lt;/li&gt;
&lt;li&gt;If that node or local volume encounters a failure and becomes inaccessible, then
that pod also becomes inaccessible. Manual intervention, external controllers,
or operators may be needed to recover from these situations.&lt;/li&gt;
&lt;li&gt;While most remote storage systems implement synchronous replication, most local
disk offerings do not provide data durability guarantees. Meaning loss of the
disk or node may result in loss of all the data on that disk&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For these reasons, local persistent storage should only be considered for
workloads that handle data replication and backup at the application layer, thus
making the applications resilient to node or data failures and unavailability
despite the lack of such guarantees at the individual disk level.&lt;/p&gt;

&lt;p&gt;Examples of good workloads include software defined storage systems and
replicated databases. Other types of applications should continue to use highly
available, remotely accessible, durable storage.&lt;/p&gt;

&lt;h2 id=&#34;how-uber-uses-local-storage&#34;&gt;How Uber Uses Local Storage&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://eng.uber.com/m3/&#34; target=&#34;_blank&#34;&gt;M3&lt;/a&gt;, Uber’s in-house metrics platform,
piloted Local Persistent Volumes at scale
in an effort to evaluate &lt;a href=&#34;https://m3db.io/&#34; target=&#34;_blank&#34;&gt;M3DB&lt;/a&gt; —
an open-source, distributed timeseries database
created by Uber. One of M3DB’s notable features is its ability to shard its
metrics into partitions, replicate them by a factor of three, and then evenly
disperse the replicas across separate failure domains.&lt;/p&gt;

&lt;p&gt;Prior to the pilot with local persistent volumes, M3DB ran exclusively in
Uber-managed environments. Over time, internal use cases arose that required the
ability to run M3DB in environments with fewer dependencies. So the team began
to explore options. As an open-source project, we wanted to provide the
community with a way to run M3DB as easily as possible, with an open-source
stack, while meeting M3DB’s requirements for high throughput, low-latency
storage, and the ability to scale itself out.&lt;/p&gt;

&lt;p&gt;The Kubernetes Local Persistent Volume interface, with its high-performance,
low-latency guarantees, quickly emerged as the perfect abstraction to build on
top of. With Local Persistent Volumes, individual M3DB instances can comfortably
handle up to 600k writes per-second. This leaves plenty of headroom for spikes
on clusters that typically process a few million metrics per-second.&lt;/p&gt;

&lt;p&gt;Because M3DB also gracefully handles losing a single node or volume, the limited
data durability guarantees of Local Persistent Volumes are not an issue. If a
node fails, M3DB finds a suitable replacement and the new node begins streaming
data from its two peers.&lt;/p&gt;

&lt;p&gt;Thanks to the Kubernetes scheduler’s intelligent handling of volume topology,
M3DB is able to programmatically evenly disperse its replicas across multiple
local persistent volumes in all available cloud zones, or, in the case of
on-prem clusters, across all available server racks.&lt;/p&gt;

&lt;h2 id=&#34;uber-s-operational-experience&#34;&gt;Uber&amp;rsquo;s Operational Experience&lt;/h2&gt;

&lt;p&gt;As mentioned above, while Local Persistent Volumes provide many benefits, they
also require careful planning and careful consideration of constraints before
committing to them in production. When thinking about our local volume strategy
for M3DB, there were a few things Uber had to consider.&lt;/p&gt;

&lt;p&gt;For one, we had to take into account the hardware profiles of the nodes in our
Kubernetes cluster. For example, how many local disks would each node cluster
have? How would they be partitioned?&lt;/p&gt;

&lt;p&gt;The local static provisioner provides
&lt;a href=&#34;https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner/blob/master/docs/best-practices.md&#34; target=&#34;_blank&#34;&gt;guidance&lt;/a&gt;
to help answer these questions. It’s best to be able to dedicate a full disk to each local volume
(for IO isolation) and a full partition per-volume (for capacity isolation).
This was easier in our cloud environments where we could mix and match local
disks. However, if using local volumes on-prem, hardware constraints may be a
limiting factor depending on the number of disks available and their
characteristics.&lt;/p&gt;

&lt;p&gt;When first testing local volumes, we wanted to have a thorough understanding of
the effect
&lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/pods/disruptions/&#34; target=&#34;_blank&#34;&gt;disruptions&lt;/a&gt;
(voluntary and involuntary) would have on pods using
local storage, and so we began testing some failure scenarios. We found that
when a local volume becomes unavailable while the node remains available (such
as when performing maintenance on the disk), a pod using the local volume will
be stuck in a ContainerCreating state until it can mount the volume. If a node
becomes unavailable, for example if it is removed from the cluster or is
&lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/&#34; target=&#34;_blank&#34;&gt;drained&lt;/a&gt;,
then pods using local volumes on that node are stuck in an Unknown or
Pending state depending on whether or not the node was removed gracefully.&lt;/p&gt;

&lt;p&gt;Recovering pods from these interim states means having to delete the PVC binding
the pod to its local volume and then delete the pod in order for it to be
rescheduled (or wait until the node and disk are available again). We took this
into account when building our &lt;a href=&#34;https://github.com/m3db/m3db-operator&#34; target=&#34;_blank&#34;&gt;operator&lt;/a&gt;
for M3DB, which makes changes to the
cluster topology when a pod is rescheduled such that the new one gracefully
streams data from the remaining two peers. Eventually we plan to automate the
deletion and rescheduling process entirely.&lt;/p&gt;

&lt;p&gt;Alerts on pod states can help call attention to stuck local volumes, and
workload-specific controllers or operators can remediate them automatically.
Because of these constraints, it’s best to exclude nodes with local volumes from
automatic upgrades or repairs, and in fact some cloud providers explicitly
mention this as a best practice.&lt;/p&gt;

&lt;h2 id=&#34;portability-between-on-prem-and-cloud&#34;&gt;Portability Between On-Prem and Cloud&lt;/h2&gt;

&lt;p&gt;Local Volumes played a big role in Uber’s decision to build orchestration for
M3DB using Kubernetes, in part because it is a storage abstraction that works
the same across on-prem and cloud environments. Remote storage solutions have
different characteristics across cloud providers, and some users may prefer not
to use networked storage at all in their own data centers. On the other hand,
local disks are relatively ubiquitous and provide more predictable performance
characteristics.&lt;/p&gt;

&lt;p&gt;By orchestrating M3DB using local disks in the cloud, where it was easier to get
up and running with Kubernetes, we gained confidence that we could still use our
operator to run M3DB in our on-prem environment without any modifications. As we
continue to work on how we’d run Kubernetes on-prem, having solved such an
important pending question is a big relief.&lt;/p&gt;

&lt;h2 id=&#34;what-s-next-for-local-persistent-volumes&#34;&gt;What&amp;rsquo;s Next for Local Persistent Volumes?&lt;/h2&gt;

&lt;p&gt;As we’ve seen with Uber’s M3DB, local persistent volumes have successfully been
used in production environments. As adoption of local persistent volumes
continues to increase, SIG Storage continues to seek feedback for ways to
improve the feature.&lt;/p&gt;

&lt;p&gt;One of the most frequent asks has been for a controller that can help with
recovery from failed nodes or disks, which is currently a manual process (or
something that has to be built into an operator). SIG Storage is investigating
creating a common controller that can be used by workloads with simple and
similar recovery processes.&lt;/p&gt;

&lt;p&gt;Another popular ask has been to support dynamic provisioning using lvm. This can
simplify disk management, and improve disk utilization. SIG Storage is
evaluating the performance tradeoffs for the viability of this feature.&lt;/p&gt;

&lt;h2 id=&#34;getting-involved&#34;&gt;Getting Involved&lt;/h2&gt;

&lt;p&gt;If you have feedback for this feature or are interested in getting involved with
the design and development, join the &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/sig-storage/README.md&#34; target=&#34;_blank&#34;&gt;Kubernetes Storage
Special-Interest-Group&lt;/a&gt;
(SIG). We’re rapidly growing and always welcome new contributors.&lt;/p&gt;

&lt;p&gt;Special thanks to all the contributors that helped bring this feature to GA,
including Chuqiang Li (lichuqiang), Dhiraj Hedge (dhirajh), Ian Chakeres
(ianchakeres), Jan Šafránek (jsafrane), Michelle Au (msau42), Saad Ali
(saad-ali), Yecheng Fu (cofyc) and Yuquan Ren (nickrenren).&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes v1.14 delivers production-level support for Windows nodes and Windows containers</title>
      <link>https://kubernetes.io/blog/2019/04/01/kubernetes-v1.14-delivers-production-level-support-for-windows-nodes-and-windows-containers/</link>
      <pubDate>Mon, 01 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/04/01/kubernetes-v1.14-delivers-production-level-support-for-windows-nodes-and-windows-containers/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Michael Michael (VMware), Patrick Lang (Microsoft)&lt;/p&gt;

&lt;p&gt;The first release of Kubernetes in 2019 brings a highly anticipated feature -  production-level support for Windows workloads. Up until now Windows node support in Kubernetes has been in beta, allowing many users to experiment and see the value of Kubernetes for Windows containers. While in beta, developers in the Kubernetes community and Windows Server team worked together to improve the container runtime, build a continuous testing process, and complete features needed for a good user experience. Kubernetes now officially supports adding Windows nodes as worker nodes and scheduling Windows containers, enabling a vast ecosystem of Windows applications to leverage the power of our platform.&lt;/p&gt;

&lt;p&gt;As Windows developers and devops engineers have been adopting containers over the last few years, they&amp;rsquo;ve been looking for a way to manage all their workloads with a common interface. Kubernetes has taken the lead for container orchestration, and this gives users a consistent way to manage their container workloads whether they need to run on Linux or Windows.&lt;/p&gt;

&lt;p&gt;The journey to a stable release of Windows in Kubernetes was not a walk in the park. The community has been working on Windows support for 3 years, delivering an alpha release with v1.5, a beta with v1.9, and now a stable release with v1.14. We would not be here today without rallying broad support and getting significant contributions from companies including Microsoft, Docker, VMware, Pivotal, Cloudbase Solutions, Google and Apprenda. During this journey, there were 3 critical points in time that significantly advanced our progress.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Advancements in Windows Server container networking that provided the infrastructure to create CNI (Container Network Interface) plugins&lt;/li&gt;
&lt;li&gt;Enhancements shipped in Windows Server semi-annual channel releases enabled Kubernetes development to move forward - culminating with Windows Server 2019 on the Long-Term Servicing Channel. This is the best release of Windows Server for running containers.&lt;/li&gt;
&lt;li&gt;The adoption of the KEP (Kubernetes Enhancement Proposals) &lt;a href=&#34;https://github.com/kubernetes/enhancements/blob/master/keps/README.md&#34; target=&#34;_blank&#34;&gt;process&lt;/a&gt;. The Windows KEP outlined a clear and agreed upon set of goals, expectations, and deliverables based on review and feedback from stakeholders across multiple SIGs. This created a clear plan that SIG-Windows could follow, paving the path towards this stable release.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;With v1.14, we&amp;rsquo;re declaring that Windows node support is stable, well-tested, and ready for adoption in production scenarios. This is a huge milestone for many reasons. For Kubernetes, it strengthens its position in the industry, enabling a vast ecosystem of Windows-based applications to be deployed on the platform. For Windows operators and developers, this means they can use the same tools and processes to manage their Windows and Linux workloads, taking full advantage of the efficiencies of the cloud-native ecosystem powered by Kubernetes. Let’s dig in a little bit into these.&lt;/p&gt;

&lt;h2 id=&#34;operator-advantages&#34;&gt;Operator Advantages&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Gain operational efficiencies by leveraging existing investments in solutions, tools, and technologies to manage Windows containers the same way as Linux containers&lt;/li&gt;
&lt;li&gt;Knowledge, training and expertise on container orchestration transfers to Windows container support&lt;/li&gt;
&lt;li&gt;IT can deliver a scalable self-service container platform to Linux and Windows developers&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;developer-advantages&#34;&gt;Developer Advantages&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Containers simplify packaging and deploying applications during development and test. Now you also get to take advantage of Kubernetes’ benefits in creating reliable, secure, and scalable distributed applications.&lt;/li&gt;
&lt;li&gt;Windows developers can now take advantage of the growing ecosystem of cloud and container-native tools to build and deploy faster, resulting in a faster time to market for their applications&lt;/li&gt;
&lt;li&gt;Taking advantage of Kubernetes as the leader in container orchestration, developers only need to learn how to use Kubernetes and that skillset will transfer across development environments and across clouds&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;cio-advantages&#34;&gt;CIO Advantages&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Leverage the operational and cost efficiencies that are introduced with Kubernetes&lt;/li&gt;
&lt;li&gt;Containerize existing.NET applications or Windows-based workloads to eliminate old hardware or underutilized virtual machines, and streamline migration from end-of-support OS versions. You  retain the benefit your application brings to the business, but decrease  the cost of keeping it running&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;“Using Kubernetes on Windows allows us to run our internal web applications as microservices. This provides quick scaling in response to load, smoother upgrades, and allows for different development groups to build without worry of other group&amp;rsquo;s version dependencies. We save money because development times are shorter and operation&amp;rsquo;s time is not spent maintaining multiple virtual machine environments,” said Jeremy, a lead devops engineer working for a top multinational legal firm, one of the early adopters of Windows on Kubernetes.&lt;/p&gt;

&lt;p&gt;There are many features that are surfaced with this release. We want to turn your attention to a few key features and enablers of Windows support in Kubernetes. For a detailed list of supported functionality, you can read our &lt;a href=&#34;https://kubernetes.io/docs/setup/windows/intro-windows-in-kubernetes/#supported-functionality&#34; target=&#34;_blank&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;You can now add Windows Server 2019 worker nodes&lt;/li&gt;
&lt;li&gt;You can now schedule Windows containers utilizing deployments, pods, services, and workload controllers&lt;/li&gt;
&lt;li&gt;Out of tree CNI plugins are provided for Azure, OVN-Kubernetes, and Flannel&lt;/li&gt;
&lt;li&gt;Containers can utilize a variety of in and out-of-tree storage plugins&lt;/li&gt;
&lt;li&gt;Improved support for metrics/quotas closely matches the capabilities offered for Linux containers&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When looking at Windows support in Kubernetes, many start drawing comparisons to Linux containers. Although some of the comparisons that highlight limitations are fair, it is important to distinguish between &lt;strong&gt;operational limitations and differences between the Windows and Linux operating systems&lt;/strong&gt;. From a container management standpoint, we must  strike a balance between preserving OS-specific behaviors required for application compatibility, and reaching operational consistency in Kubernetes across multiple operating systems. For example, some Linux-specific file system features, user IDs and permissions exposed through Kubernetes will not work on Windows today, and users are familiar with these fundamental differences. We will also be adding support for Windows-specific configurations to meet the needs of Windows customers that may not exist on Linux. The alpha support for Windows Group Managed Service Accounts is one example. Other areas such as memory reservations for Windows pods and the Windows kubelet are a work in progress and highlight an operational limitation. We will continue working on operational limitations based on what’s important to our community in future releases.&lt;/p&gt;

&lt;p&gt;Today, Kubernetes master components will continue to run on Linux. That way users can add Windows nodes without having to create a separate Kubernetes cluster. As always, our future direction is set by the community, so more components, features and deployment methods will come over time. Users should understand the differences between Windows and Linux and utilize the advantages of each platform. Our goal with this release is not to make Windows interchangeable with Linux or to answer the question of Windows vs Linux. We offer consistency in management. Managing workloads without automation is tedious and expensive. Rewriting or re-architecting workloads is even more expensive. Containers provide a clear path forward whether your app runs on Linux or Windows, and Kubernetes brings an IT organization operational consistency.&lt;/p&gt;

&lt;p&gt;As a community, our work is not complete. As already mentioned , we still have a fair bit of &lt;a href=&#34;https://kubernetes.io/docs/setup/windows/intro-windows-in-kubernetes/#limitations&#34; target=&#34;_blank&#34;&gt;limitations&lt;/a&gt; and a healthy &lt;a href=&#34;https://kubernetes.io/docs/setup/windows/intro-windows-in-kubernetes/#what-s-next&#34; target=&#34;_blank&#34;&gt;roadmap&lt;/a&gt;. We will continue making progress and enhancing Windows container support in Kubernetes, with some notable upcoming features including:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Support for CRI-ContainerD and Hyper-V isolation, bringing hypervisor-level isolation between pods for additional security and extending our container-to-node compatibility matrix&lt;/li&gt;
&lt;li&gt;Additional network plugins, including the stable release of Flannel overlay support&lt;/li&gt;
&lt;li&gt;Simple heterogeneous cluster creation using kubeadm on Windows&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We welcome you to get involved and join our community to share feedback and deployment stories, and contribute to code, docs, and improvements of any kind.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Read our getting started and contributor guides, which include links to the community meetings and past recordings, at &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-windows&#34; target=&#34;_blank&#34;&gt;https://github.com/kubernetes/community/tree/master/sig-windows&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Explore our documentation at &lt;a href=&#34;https://kubernetes.io/docs/setup/production-environment/windows/&#34; target=&#34;_blank&#34;&gt;https://kubernetes.io/docs/setup/production-environment/windows/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Join us on &lt;a href=&#34;https://kubernetes.slack.com/messages/sig-windows&#34; target=&#34;_blank&#34;&gt;Slack&lt;/a&gt; or the &lt;a href=&#34;https://discuss.kubernetes.io/c/general-discussions/windows&#34; target=&#34;_blank&#34;&gt;Kubernetes Community Forums&lt;/a&gt; to chat about Windows containers on Kubernetes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thank you and feel free to reach us individually if you have any questions.&lt;/p&gt;

&lt;p&gt;Michael Michael
&lt;br&gt;
SIG-Windows Chair
&lt;br&gt;
Director of Product Management, VMware
&lt;br&gt;
@michmike77 on Twitter
&lt;br&gt;
@m2 on Slack&lt;/p&gt;

&lt;p&gt;Patrick Lang
&lt;br&gt;
SIG-Windows Chair
&lt;br&gt;
Senior Software Engineer, Microsoft
&lt;br&gt;
@PatrickLang on Slack&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: kube-proxy Subtleties: Debugging an Intermittent Connection Reset</title>
      <link>https://kubernetes.io/blog/2019/03/29/kube-proxy-subtleties-debugging-an-intermittent-connection-reset/</link>
      <pubDate>Fri, 29 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2019/03/29/kube-proxy-subtleties-debugging-an-intermittent-connection-reset/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author:&lt;/strong&gt; &lt;a href=&#34;mailto:ygui@google.com&#34; target=&#34;_blank&#34;&gt;Yongkun Gui&lt;/a&gt;, Google&lt;/p&gt;

&lt;p&gt;I recently came across a bug that causes intermittent connection resets.  After
some digging, I found it was caused by a subtle combination of several different
network subsystems. It helped me understand Kubernetes networking better, and I
think it’s worthwhile to share with a wider audience who are interested in the same
topic.&lt;/p&gt;

&lt;h2 id=&#34;the-symptom&#34;&gt;The symptom&lt;/h2&gt;

&lt;p&gt;We received a user report claiming they were getting connection resets while using a
Kubernetes service of type ClusterIP to serve large files to pods running in the
same cluster. Initial debugging of the cluster did not yield anything
interesting: network connectivity was fine and downloading the files did not hit
any issues. However, when we ran the workload in parallel across many clients,
we were able to reproduce the problem. Adding to the mystery was the fact that
the problem could not be reproduced when the workload was run using VMs without
Kubernetes. The problem, which could be easily reproduced by &lt;a href=&#34;https://github.com/tcarmet/k8s-connection-reset&#34; target=&#34;_blank&#34;&gt;a simple
app&lt;/a&gt;, clearly has something to
do with Kubernetes networking, but what?&lt;/p&gt;

&lt;h2 id=&#34;kubernetes-networking-basics&#34;&gt;Kubernetes networking basics&lt;/h2&gt;

&lt;p&gt;Before digging into this problem, let’s talk a little bit about some basics of
Kubernetes networking, as Kubernetes handles network traffic from a pod
very differently depending on different destinations.&lt;/p&gt;

&lt;h3 id=&#34;pod-to-pod&#34;&gt;Pod-to-Pod&lt;/h3&gt;

&lt;p&gt;In Kubernetes, every pod has its own IP address. The benefit is that the
applications running inside pods could use their canonical port, instead of
remapping to a different random port. Pods have L3 connectivity between each
other. They can ping each other, and send TCP or UDP packets to each other.
&lt;a href=&#34;https://github.com/containernetworking/cni&#34; target=&#34;_blank&#34;&gt;CNI&lt;/a&gt; is the standard that solves
this problem for containers running on different hosts. There are tons of
different plugins that support CNI.&lt;/p&gt;

&lt;h3 id=&#34;pod-to-external&#34;&gt;Pod-to-external&lt;/h3&gt;

&lt;p&gt;For the traffic that goes from pod to external addresses, Kubernetes simply uses
&lt;a href=&#34;https://en.wikipedia.org/wiki/Network_address_translation&#34; target=&#34;_blank&#34;&gt;SNAT&lt;/a&gt;. What it does
is replace the pod’s internal source IP:port with the host’s IP:port. When
the return packet comes back to the host, it rewrites the pod’s IP:port as the
destination and sends it back to the original pod. The whole process is transparent
to the original pod, who doesn’t know the address translation at all.&lt;/p&gt;

&lt;h3 id=&#34;pod-to-service&#34;&gt;Pod-to-Service&lt;/h3&gt;

&lt;p&gt;Pods are mortal. Most likely, people want reliable service. Otherwise, it’s
pretty much useless. So Kubernetes has this concept called &amp;ldquo;service&amp;rdquo; which is
simply a L4 load balancer in front of pods. There are several different types of
services. The most basic type is called ClusterIP. For this type of service, it
has a unique VIP address that is only routable inside the cluster.&lt;/p&gt;

&lt;p&gt;The component in Kubernetes that implements this feature is called kube-proxy.
It sits on every node, and programs complicated iptables rules to do all kinds
of filtering and NAT between pods and services. If you go to a Kubernetes node
and type &lt;code&gt;iptables-save&lt;/code&gt;, you’ll see the rules that are inserted by Kubernetes
or other programs. The most important chains are &lt;code&gt;KUBE-SERVICES&lt;/code&gt;, &lt;code&gt;KUBE-SVC-*&lt;/code&gt;
and &lt;code&gt;KUBE-SEP-*&lt;/code&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;KUBE-SERVICES&lt;/code&gt; is the entry point for service packets. What it does is to
match the destination IP:port and dispatch the packet to the corresponding
&lt;code&gt;KUBE-SVC-*&lt;/code&gt; chain.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;KUBE-SVC-*&lt;/code&gt; chain acts as a load balancer, and distributes the packet to
&lt;code&gt;KUBE-SEP-*&lt;/code&gt; chain equally. Every &lt;code&gt;KUBE-SVC-*&lt;/code&gt; has the same number of
&lt;code&gt;KUBE-SEP-*&lt;/code&gt; chains as the number of endpoints behind it.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;KUBE-SEP-*&lt;/code&gt; chain represents a Service EndPoint. It simply does DNAT,
replacing service IP:port with pod&amp;rsquo;s endpoint IP:Port.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For DNAT, conntrack kicks in and tracks the connection state using a state
machine. The state is needed because it needs to remember the destination
address it changed to, and changed it back when the returning packet came back.
Iptables could also rely on the conntrack state (ctstate) to decide the destiny
of a packet. Those 4 conntrack states are especially important:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;NEW&lt;/em&gt;: conntrack knows nothing about this packet, which happens when the SYN
packet is received.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;ESTABLISHED&lt;/em&gt;: conntrack knows the packet belongs to an established connection,
which happens after handshake is complete.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;RELATED&lt;/em&gt;: The packet doesn’t belong to any connection, but it is affiliated
to another connection, which is especially useful for protocols like FTP.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;INVALID&lt;/em&gt;: Something is wrong with the packet, and conntrack doesn’t know how
to deal with it. This state plays a centric role in this Kubernetes issue.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here is a diagram of how a TCP connection works between pod and service. The
sequence of events are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Client pod from left hand side sends a packet to a
service: 192.168.0.2:80&lt;/li&gt;
&lt;li&gt;The packet is going through iptables rules in client
node and the destination is changed to pod IP, 10.0.1.2:80&lt;/li&gt;
&lt;li&gt;Server pod handles the packet and sends back a packet with destination 10.0.0.2&lt;/li&gt;
&lt;li&gt;The packet is going back to the client node, conntrack recognizes the packet and rewrites the source
address back to 192.169.0.2:80&lt;/li&gt;
&lt;li&gt;Client pod receives the response packet&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-03-26-kube-proxy-subtleties-debugging-an-intermittent-connection-resets/good-packet-flow.png&#34;
         alt=&#34;Good packet flow&#34; width=&#34;100%&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;Good packet flow&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;


&lt;h2 id=&#34;what-caused-the-connection-reset&#34;&gt;What caused the connection reset?&lt;/h2&gt;

&lt;p&gt;Enough of the background, so what really went wrong and caused the unexpected
connection reset?&lt;/p&gt;

&lt;p&gt;As the diagram below shows, the problem is packet 3. When conntrack cannot
recognize a returning packet, and mark it as &lt;em&gt;INVALID&lt;/em&gt;. The most common
reasons include: conntrack cannot keep track of a connection because it is out
of capacity, the packet itself is out of a TCP window, etc. For those packets
that have been marked as &lt;em&gt;INVALID&lt;/em&gt; state by conntrack, we don’t have the
iptables rule to drop it, so it will be forwarded to client pod, with source IP
address not rewritten (as shown in packet 4)! Client pod doesn’t recognize this
packet because it has a different source IP, which is pod IP, not service IP. As
a result, client pod says, &amp;ldquo;Wait a second, I don&amp;rsquo;t recall this connection to
this IP ever existed, why does this dude keep sending this packet to me?&amp;rdquo; Basically,
what the client does is simply send a RST packet to the server pod IP, which
is packet 5. Unfortunately, this is a totally legit pod-to-pod packet, which can
be delivered to server pod. Server pod doesn’t know all the address translations
that happened on the client side. From its view, packet 5 is a totally legit
packet, like packet 2 and 3. All server pod knows is, &amp;ldquo;Well, client pod doesn’t
want to talk to me, so let’s close the connection!&amp;rdquo; Boom! Of course, in order
for all these to happen, the RST packet has to be legit too, with the right TCP
sequence number, etc. But when it happens, both parties agree to close the
connection.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-03-26-kube-proxy-subtleties-debugging-an-intermittent-connection-resets/connection-reset-packet-flow.png&#34;
         alt=&#34;Connection reset packet flow&#34; width=&#34;100%&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;Connection reset packet flow&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;


&lt;h2 id=&#34;how-to-address-it&#34;&gt;How to address it?&lt;/h2&gt;

&lt;p&gt;Once we understand the root cause, the fix is not hard. There are at least 2
ways to address it.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Make conntrack more liberal on packets, and don’t mark the packets as
&lt;em&gt;INVALID&lt;/em&gt;. In Linux, you can do this by &lt;code&gt;echo 1 &amp;gt;
/proc/sys/net/ipv4/netfilter/ip_conntrack_tcp_be_liberal&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Specifically add an iptables rule to drop the packets that are marked as
&lt;em&gt;INVALID&lt;/em&gt;, so it won’t reach to client pod and cause harm.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The fix is drafted (&lt;a href=&#34;https://github.com/kubernetes/kubernetes/pull/74840&#34; target=&#34;_blank&#34;&gt;https://github.com/kubernetes/kubernetes/pull/74840&lt;/a&gt;), but
unfortunately it didn’t catch the v1.14 release window. However, for the users
that are affected by this bug, there is a way to mitigate the problem by applying
the following rule in your cluster.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;extensions/v1beta1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;DaemonSet&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;startup-script&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;labels:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;app:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;startup-script&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;template:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;labels:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;app:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;startup-script&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;hostPID:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;true&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;containers:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;startup-script&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;image:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;gcr.io/google-containers/startup-script:v1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;imagePullPolicy:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;IfNotPresent&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;securityContext:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;privileged:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;true&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;env:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;STARTUP_SCRIPT&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;value:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44;font-style:italic&#34;&gt;|
&lt;/span&gt;&lt;span style=&#34;color:#b44;font-style:italic&#34;&gt;            #! /bin/bash&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;echo&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&amp;gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/proc/sys/net/ipv4/netfilter/ip_conntrack_tcp_be_liberal&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;echo&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;done&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;

&lt;p&gt;Obviously, the bug has existed almost forever. I am surprised that it
hasn’t been noticed until recently. I believe the reasons could be: (1) this
happens more in a congested server serving large payloads, which might not be a
common use case; (2) the application layer handles the retry to be tolerant of
this kind of reset. Anyways, regardless of how fast Kubernetes has been growing,
it’s still a young project. There are no other secrets than listening closely to
customers’ feedback, not taking anything for granted but digging deep, we can
make it the best platform to run applications.&lt;/p&gt;

&lt;p&gt;Special thanks to &lt;a href=&#34;https://github.com/bowei&#34; target=&#34;_blank&#34;&gt;bowei&lt;/a&gt; for the consulting for both
debugging process and the blog, to &lt;a href=&#34;https://github.com/tcarmet&#34; target=&#34;_blank&#34;&gt;tcarmet&lt;/a&gt; for
reporting the issue and providing a reproduction.&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
