<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kubernetes – 生产级别的容器编排系统</title>
    <link>https://kubernetes.io/zh/</link>
    <description>The Kubernetes project blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <image>
      <url>https://raw.githubusercontent.com/kubernetes/kubernetes/master/logo/logo.png</url>
      <title>Kubernetes.io</title>
      <link>https://kubernetes.io/zh/</link>
    </image>
    
	<atom:link href="https://kubernetes.io/zh/feed.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Blog: Kubernetes 1.17：稳定</title>
      <link>https://kubernetes.io/zh/blog/2019/12/09/kubernetes-1-17-release-announcement/</link>
      <pubDate>Mon, 09 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/zh/blog/2019/12/09/kubernetes-1-17-release-announcement/</guid>
      <description>
        
        
        

&lt;!-- ---
layout: blog
title: &#34;Kubernetes 1.17: Stability&#34;
date: 2019-12-09T13:00:00-08:00
slug: kubernetes-1-17-release-announcement
--- --&gt;

&lt;p&gt;&lt;strong&gt;作者:&lt;/strong&gt; &lt;a href=&#34;https://github.com/kubernetes/sig-release/blob/master/releases/release-1.17/release_team.md&#34; target=&#34;_blank&#34;&gt;Kubernetes 1.17发布团队&lt;/a&gt;&lt;/p&gt;

&lt;!--
**Authors:** [Kubernetes 1.17 Release Team](https://github.com/kubernetes/sig-release/blob/master/releases/release-1.17/release_team.md)
--&gt;

&lt;p&gt;我们高兴的宣布Kubernetes 1.17版本的交付，它是我们2019年的第四个也是最后一个发布版本。Kubernetes v1.17包含22个增强功能：有14个增强已经逐步稳定(stable)，4个增强功能已经进入公开测试版(beta)，4个增强功能刚刚进入内部测试版(alpha)。
&lt;!--
We’re pleased to announce the delivery of Kubernetes 1.17, our fourth and final release of 2019! Kubernetes v1.17 consists of 22 enhancements: 14 enhancements have graduated to stable, 4 enhancements are moving to beta, and 4 enhancements are entering alpha.
--&gt;&lt;/p&gt;

&lt;h2 id=&#34;主要的主题&#34;&gt;主要的主题&lt;/h2&gt;

&lt;!--
## Major Themes
--&gt;

&lt;h3 id=&#34;云服务提供商标签基本可用&#34;&gt;云服务提供商标签基本可用&lt;/h3&gt;

&lt;!--
### Cloud Provider Labels reach General Availability
--&gt;

&lt;p&gt;作为公开测试版特性添加到v1.2，v1.7中可以看到云提供商标签达到基本可用。
&lt;!--
Added as a beta feature way back in v1.2, v1.17 sees the general availability of cloud provider labels.
--&gt;&lt;/p&gt;

&lt;h3 id=&#34;卷快照进入公开测试版&#34;&gt;卷快照进入公开测试版&lt;/h3&gt;

&lt;!--
### Volume Snapshot Moves to Beta
--&gt;

&lt;p&gt;在v1.7中，Kubernetes卷快照特性是公开测试版。这个特性是在v1.12中以内部测试版引入的，第二个有重大变化的内部测试版是v1.13。
&lt;!--
The Kubernetes Volume Snapshot feature is now beta in Kubernetes v1.17. It was introduced as alpha in Kubernetes v1.12, with a second alpha with breaking changes in Kubernetes v1.13.
--&gt;&lt;/p&gt;

&lt;h2 id=&#34;容器存储接口迁移公开测试版&#34;&gt;容器存储接口迁移公开测试版&lt;/h2&gt;

&lt;!--
### CSI Migration Beta
 --&gt;

&lt;p&gt;在v1.7中，Kubernetes树内存储插件到容器存储接口(CSI)的迁移基础架构是公开测试版。容器存储接口迁移最初是在Kubernetes v1.14中以内部测试版引入的。
&lt;!--
The Kubernetes in-tree storage plugin to Container Storage Interface (CSI) migration infrastructure is now beta in Kubernetes v1.17. CSI migration was introduced as alpha in Kubernetes v1.14.
--&gt;&lt;/p&gt;

&lt;h2 id=&#34;云服务提供商标签基本可用-1&#34;&gt;云服务提供商标签基本可用&lt;/h2&gt;

&lt;!--
## Cloud Provider Labels reach General Availability
--&gt;

&lt;p&gt;当节点和卷被创建，会基于基础云提供商的Kubernetes集群打上一系列标准标签。节点会获得一个实例类型标签。节点和卷都会得到两个描述资源在云提供商拓扑的位置标签,通常是以区域和地区的方式组织。
&lt;!--
When nodes and volumes are created, a set of standard labels are applied based on the underlying cloud provider of the Kubernetes cluster. Nodes get a label for the instance type. Both nodes and volumes get two labels describing the location of the resource in the cloud provider topology, usually organized in zones and regions.
--&gt;&lt;/p&gt;

&lt;p&gt;Kubernetes组件使用标准标签来支持一些特性。例如，调度者会保证pods和它们所声明的卷放置在相同的区域；当调度部署的pods时，调度器会优先将它们分布在不同的区域。你还可以在自己的pods标准中利用标签来配置，如节点亲和性，之类的事。标准标签使得你写的pod规范在不同的云提供商之间是可移植的。
&lt;!--
Standard labels are used by Kubernetes components to support some features. For example, the scheduler would ensure that pods are placed on the same zone as the volumes they claim; and when scheduling pods belonging to a deployment, the scheduler would prioritize spreading them across zones. You can also use the labels in your pod specs to configure things as such node affinity. Standard labels allow you to write pod specs that are portable among different cloud providers.
--&gt;&lt;/p&gt;

&lt;p&gt;在这个版本中，标签已经达到基本可用。Kubernetes组件都已经更新，可以填充基本可用和公开测试版标签，并对两者做出反应。然而，如果你的pod规范或自定义的控制器正在使用公开测试版标签，如节点亲和性，我们建议你可以将它们迁移到新的基本可用标签中。你可以从如下地方找到新标签的文档：
&lt;!--
The labels are reaching general availability in this release. Kubernetes components have been updated to populate the GA and beta labels and to react to both. However, if you are using the beta labels in your pod specs for features such as node affinity, or in your custom controllers, we recommend that you start migrating them to the new GA labels. You can find the documentation for the new labels here:
--&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#nodekubernetesioinstance-type&#34; target=&#34;_blank&#34;&gt;实例类型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#topologykubernetesioregion&#34; target=&#34;_blank&#34;&gt;地区&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#topologykubernetesiozone&#34; target=&#34;_blank&#34;&gt;区域&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
- [node.kubernetes.io/instance-type](https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#nodekubernetesioinstance-type)
- [topology.kubernetes.io/region](https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#topologykubernetesioregion)
- [topology.kubernetes.io/zone](https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#topologykubernetesiozone)
--&gt;

&lt;h2 id=&#34;卷快照进入公开测试版-1&#34;&gt;卷快照进入公开测试版&lt;/h2&gt;

&lt;!--
## Volume Snapshot Moves to Beta
--&gt;

&lt;p&gt;在v1.7中，Kubernetes卷快照是是公开测试版。最初是在v1.12中以内部测试版引入的，第二个有重大变化的内部测试版是v1.13。这篇文章总结它在公开版本中的变化。
&lt;!--
The Kubernetes Volume Snapshot feature is now beta in Kubernetes v1.17. It was introduced as alpha in Kubernetes v1.12, with a second alpha with breaking changes in Kubernetes v1.13.  This post summarizes the changes in the beta release.
--&gt;&lt;/p&gt;

&lt;h3 id=&#34;卷快照是什么&#34;&gt;卷快照是什么？&lt;/h3&gt;

&lt;!-- ### What is a Volume Snapshot? --&gt;

&lt;p&gt;许多的存储系统(如谷歌云持久化磁盘，亚马逊弹性块存储和许多的内部存储系统)支持为持久卷创建快照。快照代表卷在一个时间点的复制。它可用于配置新卷(使用快照数据提前填充)或恢复卷到一个之前的状态(用快照表示)。
&lt;!--
Many storage systems (like Google Cloud Persistent Disks, Amazon Elastic Block Storage, and many on-premise storage systems) provide the ability to create a “snapshot” of a persistent volume. A snapshot represents a point-in-time copy of a volume. A snapshot can be used either to provision a new volume (pre-populated with the snapshot data) or to restore an existing volume to a previous state (represented by the snapshot).
--&gt;&lt;/p&gt;

&lt;h3 id=&#34;为什么给kubernetes加入卷快照&#34;&gt;为什么给Kubernetes加入卷快照？&lt;/h3&gt;

&lt;!--
### Why add Volume Snapshots to Kubernetes?
--&gt;

&lt;p&gt;Kubernetes卷插件系统已经提供了功能强大的抽象用于自动配置、附加和挂载块文件系统。
&lt;!--
The Kubernetes volume plugin system already provides a powerful abstraction that automates the provisioning, attaching, and mounting of block and file storage.
--&gt;&lt;/p&gt;

&lt;p&gt;支持所有这些特性是Kubernets负载可移植的目标：Kubernetes旨在分布式系统应用和底层集群之间创建一个抽象层,使得应用可以不感知其运行集群的具体信息并且部署也不需特定集群的知识。
&lt;!--
Underpinning all these features is the Kubernetes goal of workload portability: Kubernetes aims to create an abstraction layer between distributed systems applications and underlying clusters so that applications can be agnostic to the specifics of the cluster they run on and application deployment requires no “cluster specific” knowledge.
--&gt;&lt;/p&gt;

&lt;p&gt;Kubernetes存储特别兴趣组(SIG)将快照操作确定为对很多有状态负载的关键功能。如数据库管理员希望在操作数据库前保存数据库卷快照。
&lt;!--
The Kubernetes Storage SIG identified snapshot operations as critical functionality for many stateful workloads. For example, a database administrator may want to snapshot a database volume before starting a database operation.
--&gt;&lt;/p&gt;

&lt;p&gt;在Kubernetes接口中提供一种标准的方式触发快照操作，Kubernetes用户可以处理这种用户场景，而不必使用Kubernetes API(并手动执行存储系统的具体操作)。
&lt;!--
By providing a standard way to trigger snapshot operations in the Kubernetes API, Kubernetes users can now handle use cases like this without having to go around the Kubernetes API (and manually executing storage system specific operations).
--&gt;&lt;/p&gt;

&lt;p&gt;取而代之的是，Kubernetes用户现在被授权以与集群无关的方式将快照操作放进他们的工具和策略中，并且确信它将对任意的Kubernetes集群有效，而与底层存储无关。
&lt;!--
Instead, Kubernetes users are now empowered to incorporate snapshot operations in a cluster agnostic way into their tooling and policy with the comfort of knowing that it will work against arbitrary Kubernetes clusters regardless of the underlying storage.
--&gt;&lt;/p&gt;

&lt;p&gt;此外，Kubernetes 快照原语作为基础构建能力解锁了为Kubernetes开发高级、企业级、存储管理特性的能力:包括应用或集群级别的备份方案。
&lt;!--
Additionally these Kubernetes snapshot primitives act as basic building blocks that unlock the ability to develop advanced, enterprise grade, storage administration features for Kubernetes: including application or cluster level backup solutions.
--&gt;&lt;/p&gt;

&lt;p&gt;你可以阅读更多关于&lt;a href=&#34;https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-cis-volume-snapshot-beta/&#34; target=&#34;_blank&#34;&gt;发布容器存储接口卷快照公开测试版&lt;/a&gt;
&lt;!--
You can read more in the blog entry about [releasing CSI volume snapshots to beta](https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-cis-volume-snapshot-beta/).
--&gt;&lt;/p&gt;

&lt;h2 id=&#34;容器存储接口迁移公测版&#34;&gt;容器存储接口迁移公测版&lt;/h2&gt;

&lt;!--
## CSI Migration Beta
--&gt;

&lt;h3 id=&#34;为什么我们迁移内建树插件到容器存储接口&#34;&gt;为什么我们迁移内建树插件到容器存储接口？&lt;/h3&gt;

&lt;!--
### Why are we migrating in-tree plugins to CSI?
--&gt;

&lt;p&gt;在容器存储接口之前，Kubernetes提供功能强大的卷插件系统。这些卷插件是树内的意味着它们的代码是核心Kubernetes代码的一部分并附带在核心Kubernetes二进制中。然而，为Kubernetes添加插件支持新卷是非常有挑战的。希望在Kubernetes上为自己存储系统添加支持(或修复现有卷插件的bug)的供应商被迫与Kubernetes发行进程对齐。此外，第三方存储代码在核心Kubernetes二进制中会造成可靠性和安全问题，并且这些代码对于Kubernetes的维护者来说是难以(一些场景是不可能)测试和维护的。在Kubernetes上采用容器存储接口可以解决大部分问题。
&lt;!--
Prior to CSI, Kubernetes provided a powerful volume plugin system. These volume plugins were “in-tree” meaning their code was part of the core Kubernetes code and shipped with the core Kubernetes binaries. However, adding support for new volume plugins to Kubernetes was challenging. Vendors that wanted to add support for their storage system to Kubernetes (or even fix a bug in an existing volume plugin) were forced to align with the Kubernetes release process. In addition, third-party storage code caused reliability and security issues in core Kubernetes binaries and the code was often difficult (and in some cases impossible) for Kubernetes maintainers to test and maintain. Using the Container Storage Interface in Kubernetes resolves these major issues.
 --&gt;&lt;/p&gt;

&lt;p&gt;随着更多容器存储接口驱动变成生产环境可用，我们希望所有的Kubernetes用户从容器存储接口模型中获益。然而，我们不希望强制用户以破坏现有基本可用的存储接口的方式去改变负载和配置。道路很明确，我们将不得不用CSI替换树内插件接口。什么是容器存储接口迁移？
&lt;!--
 As more CSI Drivers were created and became production ready, we wanted all Kubernetes users to reap the benefits of the CSI model. However, we did not want to force users into making workload/configuration changes by breaking the existing generally available storage APIs. The way forward was clear - we would have to replace the backend of the “in-tree plugin” APIs with CSI.What is CSI migration?
--&gt;&lt;/p&gt;

&lt;p&gt;在容器存储接口迁移上所做的努力使得替换现有的树内存储插件，如&lt;code&gt;kubernetes.io/gce-pd&lt;/code&gt;或&lt;code&gt;kubernetes.io/aws-ebs&lt;/code&gt;，为相应的容器存储接口驱动成为可能。如果容器存储接口迁移正常工作，Kubernetes终端用户不会注意到任何差别。迁移过后，Kubernetes用户可以继续使用现有接口来依赖树内存储插件的功能。
&lt;!--
The CSI migration effort enables the replacement of existing in-tree storage plugins such as `kubernetes.io/gce-pd` or `kubernetes.io/aws-ebs` with a corresponding CSI driver. If CSI Migration is working properly, Kubernetes end users shouldn’t notice a difference. After migration, Kubernetes users may continue to rely on all the functionality of in-tree storage plugins using the existing interface.
 --&gt;&lt;/p&gt;

&lt;p&gt;当Kubernetes集群管理者更新集群使得CSI迁移可用，现有的有状态部署和工作负载照常工作；然而，在幕后Kubernetes将存储管理操作交给了(以前是交给树内驱动)CSI驱动。
&lt;!--
When a Kubernetes cluster administrator updates a cluster to enable CSI migration, existing stateful deployments and workloads continue to function as they always have; however, behind the scenes Kubernetes hands control of all storage management operations (previously targeting in-tree drivers) to CSI drivers.
--&gt;&lt;/p&gt;

&lt;p&gt;Kubernetes组非常努力地保证存储接口的稳定性和平滑升级体验的承诺。这需要细致的考虑现有特性和行为来确保后向兼容和接口稳定性。你可以想像成在加速行驶的直线上给赛车换轮胎。
&lt;!--
The Kubernetes team has worked hard to ensure the stability of storage APIs and for the promise of a smooth upgrade experience. This involves meticulous accounting of all existing features and behaviors to ensure backwards compatibility and API stability. You can think of it like changing the wheels on a racecar while it’s speeding down the straightaway.
--&gt;&lt;/p&gt;

&lt;p&gt;你可以在这篇博客中阅读更多关于&lt;a href=&#34;https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-csi-migration-beta/&#34; target=&#34;_blank&#34;&gt;容器存储接口迁移成为公开测试版&lt;/a&gt;.
&lt;!--
You can read more in the blog entry about [CSI migration going to beta](https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-csi-migration-beta/). --&gt;&lt;/p&gt;

&lt;h2 id=&#34;其它更新&#34;&gt;其它更新&lt;/h2&gt;

&lt;!--
## Other Updates
 --&gt;

&lt;h3 id=&#34;稳定&#34;&gt;稳定💯&lt;/h3&gt;

&lt;!--
### Graduated to Stable 💯
--&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/382&#34; target=&#34;_blank&#34;&gt;按条件污染节点&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/495&#34; target=&#34;_blank&#34;&gt;可配置的Pod进程共享命名空间&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/548&#34; target=&#34;_blank&#34;&gt;采用kube-scheduler调度DaemonSet Pods&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/554&#34; target=&#34;_blank&#34;&gt;动态卷最大值&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/557&#34; target=&#34;_blank&#34;&gt;Kubernetes容器存储接口支持拓扑&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/559&#34; target=&#34;_blank&#34;&gt;在SubPath挂载提供环境变量扩展&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/575&#34; target=&#34;_blank&#34;&gt;为Custom Resources提供默认值&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/589&#34; target=&#34;_blank&#34;&gt;从频繁的Kublet心跳到租约接口&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/714&#34; target=&#34;_blank&#34;&gt;拆分Kubernetes测试Tarball&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/956&#34; target=&#34;_blank&#34;&gt;添加Watch书签支持&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/960&#34; target=&#34;_blank&#34;&gt;行为驱动一致性测试&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/980&#34; target=&#34;_blank&#34;&gt;服务负载均衡终结保护&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/1152&#34; target=&#34;_blank&#34;&gt;避免每一个Watcher独立序列化相同的对象&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
- [Taint Node by Condition](https://github.com/kubernetes/enhancements/issues/382)
- [Configurable Pod Process Namespace Sharing](https://github.com/kubernetes/enhancements/issues/495)
- [Schedule DaemonSet Pods by kube-scheduler](https://github.com/kubernetes/enhancements/issues/548)
- [Dynamic Maximum Volume Count](https://github.com/kubernetes/enhancements/issues/554)
- [Kubernetes CSI Topology Support](https://github.com/kubernetes/enhancements/issues/557)
- [Provide Environment Variables Expansion in SubPath Mount](https://github.com/kubernetes/enhancements/issues/559)
- [Defaulting of Custom Resources](https://github.com/kubernetes/enhancements/issues/575)
- [Move Frequent Kubelet Heartbeats To Lease Api](https://github.com/kubernetes/enhancements/issues/589)
- [Break Apart The Kubernetes Test Tarball](https://github.com/kubernetes/enhancements/issues/714)
- [Add Watch Bookmarks Support](https://github.com/kubernetes/enhancements/issues/956)
- [Behavior-Driven Conformance Testing](https://github.com/kubernetes/enhancements/issues/960)
- [Finalizer Protection For Service Loadbalancers](https://github.com/kubernetes/enhancements/issues/980)
- [Avoid Serializing The Same Object Independently For Every Watcher](https://github.com/kubernetes/enhancements/issues/1152)
--&gt;

&lt;h3 id=&#34;主要变化&#34;&gt;主要变化&lt;/h3&gt;

&lt;!--
### Major Changes
--&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/563&#34; target=&#34;_blank&#34;&gt;添加IPv4/IPv6双栈支持&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
- [Add IPv4/IPv6 Dual Stack Support](https://github.com/kubernetes/enhancements/issues/563)
--&gt;

&lt;h3 id=&#34;其它显著特性&#34;&gt;其它显著特性&lt;/h3&gt;

&lt;!--
### Other Notable Features
--&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/536&#34; target=&#34;_blank&#34;&gt;拓扑感知路由服务(内部测试版)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/1043&#34; target=&#34;_blank&#34;&gt;为Windows添加RunAsUserName&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
- [Topology Aware Routing of Services (Alpha)](https://github.com/kubernetes/enhancements/issues/536)
- [RunAsUserName for Windows](https://github.com/kubernetes/enhancements/issues/1043)
--&gt;

&lt;h3 id=&#34;可用性&#34;&gt;可用性&lt;/h3&gt;

&lt;!--
 ### Availability
--&gt;

&lt;p&gt;Kubernetes 1.17 可以&lt;a href=&#34;https://github.com/kubernetes/kubernetes/releases/tag/v1.17.0&#34; target=&#34;_blank&#34;&gt;在GitHub下载&lt;/a&gt;。开始使用Kubernetes，看看这些&lt;a href=&#34;https://kubernetes.io/docs/tutorials/&#34; target=&#34;_blank&#34;&gt;交互教学&lt;/a&gt;。你可以非常容易使用&lt;a href=&#34;https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/&#34; target=&#34;_blank&#34;&gt;kubeadm&lt;/a&gt;安装1.17。
&lt;!--
Kubernetes 1.17 is available for [download on GitHub](https://github.com/kubernetes/kubernetes/releases/tag/v1.17.0). To get started with Kubernetes, check out these [interactive tutorials](https://kubernetes.io/docs/tutorials/). You can also easily install 1.17 using
 [kubeadm](https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/).
 --&gt;&lt;/p&gt;

&lt;h3 id=&#34;发布团队&#34;&gt;发布团队&lt;/h3&gt;

&lt;!--
### Release Team
--&gt;

&lt;p&gt;正是因为有上千人参与技术或非技术内容的贡献才使这个版本成为可能。特别感谢由Guinevere Saenger领导的&lt;a href=&#34;https://github.com/kubernetes/sig-release/blob/master/releases/release-1.17/release_team.md&#34; target=&#34;_blank&#34;&gt;发布团队&lt;/a&gt;。发布团队的35名成员在发布版本的多方面进行了协调，从文档到测试，校验和特性的完善。
&lt;!--
This release is made possible through the efforts of hundreds of individuals who contributed both technical and non-technical content. Special thanks to the [release team](https://github.com/kubernetes/sig-release/blob/master/releases/release-1.17/release_team.md) led by Guinevere Saenger. The 35 individuals on the release team coordinated many aspects of the release, from documentation to testing, validation, and feature completeness.
--&gt;
随着Kubernetes社区的成长，我们的发布流程是在开源软件协作方面惊人的示例。Kubernetes快速并持续获得新用户。这一成长产生了良性的反馈循环，更多的贡献者贡献代码创造了更加活跃的生态。Kubernetes已经有超过&lt;a href=&#34;https://k8s.devstats.cncf.io/d/24/overall-project-statistics?orgId=1&#34; target=&#34;_blank&#34;&gt;39000位贡献者&lt;/a&gt;和一个超过66000人的活跃社区。
&lt;!--
As the Kubernetes community has grown, our release process represents an amazing demonstration of collaboration in open source software development. Kubernetes continues to gain new users at a rapid pace. This growth creates a positive feedback cycle where more contributors commit code creating a more vibrant ecosystem. Kubernetes has had over [39,000 individual contributors](https://k8s.devstats.cncf.io/d/24/overall-project-statistics?orgId=1) to date and an active community of more than 66,000 people.
--&gt;&lt;/p&gt;

&lt;h3 id=&#34;网络研讨会&#34;&gt;网络研讨会&lt;/h3&gt;

&lt;!--
### Webinar
--&gt;

&lt;p&gt;2020年1月7号，加入Kubernetes 1.17发布团队，学习关于这次发布的主要特性。&lt;a href=&#34;https://zoom.us/webinar/register/9315759188139/WN_kPOZA_6RTjeGdXTG7YFO3A&#34; target=&#34;_blank&#34;&gt;这里&lt;/a&gt;注册。
&lt;!--
Join members of the Kubernetes 1.17 release team on Jan 7th, 2020 to learn about the major features in this release. Register [here](https://zoom.us/webinar/register/9315759188139/WN_kPOZA_6RTjeGdXTG7YFO3A).
--&gt;&lt;/p&gt;

&lt;h3 id=&#34;参与其中&#34;&gt;参与其中&lt;/h3&gt;

&lt;!--
### Get Involved
--&gt;

&lt;p&gt;最简单的参与Kubernetes的方式是加入其中一个与你兴趣相同的&lt;a href=&#34;https://github.com/kubernetes/community/blob/master/sig-list.md&#34; target=&#34;_blank&#34;&gt;特别兴趣组&lt;/a&gt;（SIGs)。有什么想要广播到Kubernetes社区吗？通过如下的频道，在每周的&lt;a href=&#34;https://github.com/kubernetes/community/tree/master/communication&#34; target=&#34;_blank&#34;&gt;社区会议&lt;/a&gt;分享你的声音。感谢你的贡献和支持。
&lt;!--
The simplest way to get involved with Kubernetes is by joining one of the many [Special Interest Groups](https://github.com/kubernetes/community/blob/master/sig-list.md) (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly [community meeting](https://github.com/kubernetes/community/tree/master/communication), and through the channels below. Thank you for your continued feedback and support.
--&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;在Twitter上关注我们&lt;a href=&#34;https://twitter.com/kubernetesio&#34; target=&#34;_blank&#34;&gt;@Kubernetesio&lt;/a&gt;获取最新的更新&lt;/li&gt;
&lt;li&gt;在&lt;a href=&#34;https://discuss.kubernetes.io/&#34; target=&#34;_blank&#34;&gt;Discuss&lt;/a&gt;参与社区的讨论&lt;/li&gt;
&lt;li&gt;在&lt;a href=&#34;http://slack.k8s.io/&#34; target=&#34;_blank&#34;&gt;Slack&lt;/a&gt;加入社区&lt;/li&gt;
&lt;li&gt;在&lt;a href=&#34;http://stackoverflow.com/questions/tagged/kubernetes&#34; target=&#34;_blank&#34;&gt;Stack Overflow&lt;/a&gt;发布问题(或回答问题)&lt;/li&gt;
&lt;li&gt;分享你的Kubernetes&lt;a href=&#34;https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform&#34; target=&#34;_blank&#34;&gt;故事&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
- Follow us on Twitter [@Kubernetesio](https://twitter.com/kubernetesio) for latest updates
- Join the community discussion on [Discuss](https://discuss.kubernetes.io/)
- Join the community on [Slack](http://slack.k8s.io/)
- Post questions (or answer questions) on [Stack Overflow](http://stackoverflow.com/questions/tagged/kubernetes)
- Share your Kubernetes [story](https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform)
 --&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: 使用 Java 开发一个 Kubernetes controller</title>
      <link>https://kubernetes.io/zh/blog/2019/11/26/develop-a-kubernetes-controller-in-java/</link>
      <pubDate>Tue, 26 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/zh/blog/2019/11/26/develop-a-kubernetes-controller-in-java/</guid>
      <description>
        
        
        

&lt;!--
---
layout: blog
title: &#34;Develop a Kubernetes controller in Java&#34;
date: 2019-11-26
slug: Develop-A-Kubernetes-Controller-in-Java
---
--&gt;

&lt;!--
**Authors:** Min Kim (Ant Financial), Tony Ado (Ant Financial)
--&gt;

&lt;p&gt;&lt;strong&gt;作者:&lt;/strong&gt; Min Kim (蚂蚁金服), Tony Ado (蚂蚁金服)
&lt;!--
The official [Kubernetes Java SDK](https://github.com/kubernetes-client/java) project
recently released their latest work on providing the Java Kubernetes developers
a handy Kubernetes controller-builder SDK which is helpful for easily developing
advanced workloads or systems.  
--&gt;
&lt;a href=&#34;https://github.com/kubernetes-client/java&#34; target=&#34;_blank&#34;&gt;Kubernetes Java SDK&lt;/a&gt; 官方项目最近发布了他们的最新工作，为 Java Kubernetes 开发人员提供一个便捷的 Kubernetes 控制器-构建器 SDK，它有助于轻松开发高级工作负载或系统。
&amp;lt;!&amp;ndash;&lt;/p&gt;

&lt;h2 id=&#34;overall&#34;&gt;Overall&lt;/h2&gt;

&lt;p&gt;Java is no doubt one of the most popular programming languages in the world but
it&amp;rsquo;s been difficult for a period time for those non-Golang developers to build up
their customized controller/operator due to the lack of library resources in the
community. In the world of Golang, there&amp;rsquo;re already some excellent controller
frameworks, for example, &lt;a href=&#34;https://github.com/kubernetes-sigs/controller-runtime&#34; target=&#34;_blank&#34;&gt;controller runtime&lt;/a&gt;,
&lt;a href=&#34;https://github.com/operator-framework/operator-sdk&#34; target=&#34;_blank&#34;&gt;operator SDK&lt;/a&gt;. These
existing Golang frameworks are relying on the various utilities from the
&lt;a href=&#34;https://github.com/kubernetes/client-go&#34; target=&#34;_blank&#34;&gt;Kubernetes Golang SDK&lt;/a&gt; proven to
be stable over years. Driven by the emerging need of further integration into
the platform of Kubernetes, we not only ported many essential toolings from the Golang
SDK into the kubernetes Java SDK including informers, work-queues, leader-elections,
etc. but also developed a controller-builder SDK which wires up everything into
a runnable controller without hiccups.
&amp;ndash;&amp;gt;&lt;/p&gt;

&lt;h2 id=&#34;综述&#34;&gt;综述&lt;/h2&gt;

&lt;p&gt;Java 无疑是世界上最流行的编程语言之一，但由于社区中缺少库资源，一段时间以来，那些非 Golang 开发人员很难构建他们定制的 controller/operator。在 Golang 的世界里，已经有一些很好的 controller 框架了，例如，&lt;a href=&#34;https://github.com/kubernetes-sigs/controller-runtime&#34; target=&#34;_blank&#34;&gt;controller runtime&lt;/a&gt;，&lt;a href=&#34;https://github.com/operator-framework/operator-sdk&#34; target=&#34;_blank&#34;&gt;operator SDK&lt;/a&gt;。这些现有的 Golang 框架依赖于 &lt;a href=&#34;https://github.com/kubernetes/client-go&#34; target=&#34;_blank&#34;&gt;Kubernetes Golang SDK&lt;/a&gt; 提供的各种实用工具，这些工具经过多年证明是稳定的。受进一步集成到 Kubernetes 平台的需求驱动，我们不仅将 Golang SDK 中的许多基本工具移植到 kubernetes Java SDK 中，包括 informers、work-queues、leader-elections 等，也开发了一个控制器构建 SDK，它可以将所有东西连接到一个可运行的控制器中，而不会产生任何问题。&lt;/p&gt;

&lt;!--
## Backgrounds

Why use Java to implement Kubernetes tooling? You might pick Java for:

- __Integrating legacy enterprise Java systems__: Many companies have their legacy
systems or frameworks written in Java in favor of stability. We are not able to
move everything to Golang easily.

- __More open-source community resources__: Java is mature and has accumulated abundant open-source
libraries over decades, even though Golang is getting more and more fancy and
popular for developers. Additionally, nowadays developers are able to develop
their aggregated-apiservers over SQL-storage and Java has way better support on SQLs.
--&gt;

&lt;h2 id=&#34;背景&#34;&gt;背景&lt;/h2&gt;

&lt;p&gt;为什么要使用 Java 实现 kubernetes 工具？选择 Java 的原因可能是：
- &lt;strong&gt;集成遗留的企业级 Java 系统&lt;/strong&gt;：许多公司的遗留系统或框架都是用 Java 编写的，用以支持稳定性。我们不能轻易把所有东西搬到 Golang。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;更多开源社区的资源&lt;/strong&gt;：Java 是成熟的，并且在过去几十年中累计了丰富的开源库，尽管 Golang 对于开发人员来说越来越具有吸引力，越来越流行。此外，现在开发人员能够在 SQL 存储上开发他们的聚合-apiserver，而 Java 在 SQL 上有更好的支持。&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
## How to use?

Take maven project as example, adding the following dependencies into your dependencies:
--&gt;

&lt;h2 id=&#34;如何去使用&#34;&gt;如何去使用&lt;/h2&gt;

&lt;p&gt;以 maven 项目为例，将以下依赖项添加到您的依赖中：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-xml&#34; data-lang=&#34;xml&#34;&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;lt;dependency&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;lt;groupId&amp;gt;&lt;/span&gt;io.kubernetes&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;lt;/groupId&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;lt;artifactId&amp;gt;&lt;/span&gt;client-java-extended&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;lt;/artifactId&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;lt;version&amp;gt;&lt;/span&gt;6.0.1&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;lt;/version&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;lt;/dependency&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;!--
Then we can make use of the provided builder libraries to write your own controller.
For example, the following one is a simple controller prints out node information
on watch notification, see complete example [here](https://github.com/kubernetes-client/java/blob/master/examples/src/main/java/io/kubernetes/client/examples/ControllerExample.java):
--&gt;

&lt;p&gt;然后我们可以使用提供的生成器库来编写自己的控制器。例如，下面是一个简单的控制，它打印出关于监视通知的节点信息，请看完整的例子：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-java&#34; data-lang=&#34;java&#34;&gt;...
    &lt;span style=&#34;&#34;&gt;Reconciler reconciler &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;new&lt;/span&gt; Reconciler() {
      &lt;span style=&#34;color:#a2f&#34;&gt;@Override&lt;/span&gt;
      &lt;span style=&#34;&#34;&gt;public Result &lt;/span&gt;reconcile(&lt;span style=&#34;&#34;&gt;Request request&lt;/span&gt;) {
        &lt;span style=&#34;&#34;&gt;V1Node node &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; nodeLister.&lt;span style=&#34;color:#b44&#34;&gt;get&lt;/span&gt;(request.&lt;span style=&#34;color:#b44&#34;&gt;getName&lt;/span&gt;());
        System.&lt;span style=&#34;color:#b44&#34;&gt;out&lt;/span&gt;.&lt;span style=&#34;color:#b44&#34;&gt;println&lt;/span&gt;(&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;triggered reconciling &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; node.&lt;span style=&#34;color:#b44&#34;&gt;getMetadata&lt;/span&gt;().&lt;span style=&#34;color:#b44&#34;&gt;getName&lt;/span&gt;());
        &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;new&lt;/span&gt; Result(&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;false&lt;/span&gt;);
      }
    };
    &lt;span style=&#34;&#34;&gt;Controller controller &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;
        ControllerBuilder.&lt;span style=&#34;color:#b44&#34;&gt;defaultBuilder&lt;/span&gt;(informerFactory)
            .&lt;span style=&#34;color:#b44&#34;&gt;watch&lt;/span&gt;(
                (workQueue) &lt;span style=&#34;color:#666&#34;&gt;-&amp;gt;&lt;/span&gt; ControllerBuilder.&lt;span style=&#34;color:#b44&#34;&gt;controllerWatchBuilder&lt;/span&gt;(V1Node.&lt;span style=&#34;color:#b44&#34;&gt;class&lt;/span&gt;, workQueue).&lt;span style=&#34;color:#b44&#34;&gt;build&lt;/span&gt;())
            .&lt;span style=&#34;color:#b44&#34;&gt;withReconciler&lt;/span&gt;(nodeReconciler) &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// required, set the actual reconciler
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;&lt;/span&gt;            .&lt;span style=&#34;color:#b44&#34;&gt;withName&lt;/span&gt;(&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;node-printing-controller&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// optional, set name for controller for logging, thread-tracing
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;&lt;/span&gt;            .&lt;span style=&#34;color:#b44&#34;&gt;withWorkerCount&lt;/span&gt;(4) &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// optional, set worker thread count
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;&lt;/span&gt;            .&lt;span style=&#34;color:#b44&#34;&gt;withReadyFunc&lt;/span&gt;( nodeInformer&lt;span style=&#34;color:#666&#34;&gt;::&lt;/span&gt;hasSynced) &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// optional, only starts controller when the cache has synced up
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;&lt;/span&gt;            .&lt;span style=&#34;color:#b44&#34;&gt;build&lt;/span&gt;();&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;!--
If you notice, the new Java controller framework learnt a lot from the design of
[controller-runtime](https://github.com/kubernetes-sigs/controller-runtime) which
successfully encapsulates the complex components inside controller into several
clean interfaces. With the help of Java Generics, we even move on a bit and simply
the encapsulation in a better way.
--&gt;

&lt;p&gt;如果您留意，新的 Java 控制器框架很多地方借鉴于 &lt;a href=&#34;https://github.com/kubernetes-sigs/controller-runtime&#34; target=&#34;_blank&#34;&gt;controller-runtime&lt;/a&gt; 的设计，它成功地将控制器内部的复杂组件封装到几个干净的接口中。在 Java 泛型的帮助下，我们甚至更进一步，以更好的方式简化了封装。&lt;/p&gt;

&lt;!--
As for more advanced usage, we can wrap multiple controllers into a controller-manager
or a leader-electing controller which helps deploying in HA setup. In a word, we can
basically find most of the equivalence implementations here from Golang SDK and
more advanced features are under active development by us.
--&gt;

&lt;p&gt;我们可以将多个控制器封装到一个 controller-manager 或 leader-electing controller 中，这有助于在 HA 设置中进行部署。&lt;/p&gt;

&lt;!--
## Future steps

The community behind the official Kubernetes Java SDK project will be focusing on
providing more useful utilities for developers who hope to program cloud native
Java applications to extend Kubernetes. If you are interested in more details,
please look at our repo [kubernetes-client/java](https://github.com/kubernetes-client/java).
Feel free to share also your feedback with us, through Issues or [Slack](http://kubernetes.slack.com/messages/kubernetes-client/).
--&gt;

&lt;h2 id=&#34;未来计划&#34;&gt;未来计划&lt;/h2&gt;

&lt;p&gt;Kubernetes Java SDK 项目背后的社区将专注于为希望编写云原生 Java 应用程序来扩展 Kubernetes 的开发人员提供更有用的实用程序。如果您对更详细的信息感兴趣，请查看我们的仓库 &lt;a href=&#34;https://github.com/kubernetes-client/java&#34; target=&#34;_blank&#34;&gt;kubernetes-client/java&lt;/a&gt;。请通过问题或 &lt;a href=&#34;http://kubernetes.slack.com/messages/kubernetes-client/&#34; target=&#34;_blank&#34;&gt;Slack&lt;/a&gt; 与我们分享您的反馈。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: 使用 Microk8s 在 Linux 上本地运行 Kubernetes</title>
      <link>https://kubernetes.io/zh/blog/2019/11/26/%E4%BD%BF%E7%94%A8-microk8s-%E5%9C%A8-linux-%E4%B8%8A%E6%9C%AC%E5%9C%B0%E8%BF%90%E8%A1%8C-kubernetes/</link>
      <pubDate>Tue, 26 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/zh/blog/2019/11/26/%E4%BD%BF%E7%94%A8-microk8s-%E5%9C%A8-linux-%E4%B8%8A%E6%9C%AC%E5%9C%B0%E8%BF%90%E8%A1%8C-kubernetes/</guid>
      <description>
        
        
        

&lt;!--
---                                           
title: &#39;Running Kubernetes locally on Linux with Microk8s&#39;                                                           
date: 2019-11-26
---
--&gt;

&lt;!--
**Authors**: [Ihor Dvoretskyi](https://twitter.com/idvoretskyi), Developer Advocate, Cloud Native Computing Foundation; [Carmine Rimi](https://twitter.com/carminerimi)
--&gt;

&lt;p&gt;&lt;strong&gt;作者&lt;/strong&gt;: &lt;a href=&#34;https://twitter.com/idvoretskyi&#34; target=&#34;_blank&#34;&gt;Ihor Dvoretskyi&lt;/a&gt;，开发支持者，云原生计算基金会；&lt;a href=&#34;https://twitter.com/carminerimi&#34; target=&#34;_blank&#34;&gt;Carmine Rimi&lt;/a&gt;
&lt;!--
This article, the second in a [series](/blog/2019/03/28/running-kubernetes-locally-on-linux-with-minikube-now-with-kubernetes-1.14-support/) about local deployment options on Linux, and covers [MicroK8s](https://microk8s.io/). Microk8s is the click-and-run solution for deploying a Kubernetes cluster locally, originally developed by Canonical, the publisher of Ubuntu.
--&gt;
本文是关于 Linux 上的本地部署选项&lt;a href=&#34;https://twitter.com/idvoretskyi&#34; target=&#34;_blank&#34;&gt;系列&lt;/a&gt;的第二篇，涵盖了 &lt;a href=&#34;https://microk8s.io/&#34; target=&#34;_blank&#34;&gt;MicroK8s&lt;/a&gt;。Microk8s 是本地部署 Kubernetes 集群的 &amp;lsquo;click-and-run&amp;rsquo; 方案，最初由 Ubuntu 的发布者 Canonical 开发。
&lt;!--
While Minikube usually spins up a local virtual machine (VM) for the Kubernetes cluster, MicroK8s doesn’t require a VM. It uses [snap](https://snapcraft.io/) packages, an application packaging and isolation technology.
--&gt;
虽然 Minikube 通常为 Kubernetes 集群创建一个本地虚拟机（VM），但是 MicroK8s 不需要 VM。它使用&lt;a href=&#34;https://snapcraft.io/&#34; target=&#34;_blank&#34;&gt;snap&lt;/a&gt; 包，这是一种应用程序打包和隔离技术。
&lt;!--
This difference has its pros and cons. Here we’ll discuss a few of the interesting differences, and comparing the benefits of a VM based approach with the benefits of a non-VM approach. One of the first factors is cross-platform portability. While a Minikube VM is portable across operating systems - it supports not only Linux, but Windows, macOS, and even FreeBSD - Microk8s requires Linux, and only on those distributions [that support snaps](https://snapcraft.io/docs/installing-snapd). Most popular Linux distributions are supported. 
--&gt;
这种差异有其优点和缺点。在这里，我们将讨论一些有趣的区别，并且基于 VM 的方法和非 VM 方法的好处。第一个因素是跨平台的移植性。虽然 Minikube VM 可以跨操作系统移植——它不仅支持 Linux，还支持 Windows、macOS、甚至 FreeBSD，但 Microk9s 需要 Linux，而且只在&lt;a href=&#34;https://snapcraft.io/docs/installing-snapd&#34; target=&#34;_blank&#34;&gt;那些支持 snaps&lt;/a&gt; 的发行版上。支持大多数流行的 Linux 发行版。
&lt;!--
Another factor to consider is resource consumption. While a VM appliance gives you greater portability, it does mean you’ll consume more resources to run the VM, primarily because the VM ships a complete operating system, and runs on top of a hypervisor. You’ll consume more disk space when the VM is dormant. You’ll consume more RAM and CPU while it is running. Since Microk8s doesn’t require spinning up a virtual machine you’ll have more resources to run your workloads and other applications. Given its smaller footprint, MicroK8s is ideal for IoT devices - you can even use it on a Raspberry Pi device!
--&gt;
另一个考虑到的因素是资源消耗。虽然 VM 设备为您提供了更好的可移植性，但它确实意味着您将消耗更多资源来运行 VM，这主要是因为 VM 提供了一个完整的操作系统，并且运行在管理程序之上。当 VM 处于休眠时你将消耗更多的磁盘空间。当它运行时，你将会消耗更多的 RAM 和 CPU。因为 MIcrok8s 不需要创建虚拟机，你将会有更多的资源去运行你的工作负载和其他设备。考虑到所占用的空间更小，MIcroK8s 是物联网设备的理想选择-你甚至可以在 Paspberry Pi 和设备上使用它！
&lt;!--
Finally, the projects appear to follow a different release cadence and strategy. MicroK8s, and snaps in general provide [channels](https://snapcraft.io/docs/channels) that allow you to consume beta and release candidate versions of new releases of Kubernetes, as well as the previous stable release. Microk8s generally releases the stable release of upstream Kubernetes almost immediately.
--&gt;
最后，项目似乎遵循了不同的发布节奏和策略。Microk8s 和 snaps 通常提供&lt;a href=&#34;https://snapcraft.io/docs/channels&#34; target=&#34;_blank&#34;&gt;渠道&lt;/a&gt;允许你使用测试版和发布 KUbernetes 新版本的候选版本，同样也提供先前稳定版本。Microk8s 通常几乎立刻发布 Kubernetes 上游的稳定版本。
&lt;!--
But wait, there’s more! Minikube and MicroK8s both started as single-node clusters. Essentially, they allow you to create a Kubernetes cluster with a single worker node. That is about to change - there’s an early alpha release of MicroK8s that includes clustering. With this capability, you can create Kubernetes clusters with as many worker nodes as you wish. This is effectively an un-opinionated option for creating a cluster - the developer must create the network connectivity between the nodes, as well as integrate with other infrastructure that may be required, like an external load-balancer. In summary, MicroK8s offers a quick and easy way to turn a handful of computers or VMs into a multi-node Kubernetes cluster. We’ll write more about this kind of architecture in a future article.
--&gt;
但是等等，还有更多！Minikube 和 Microk8s 都是作为单节点集群启动的。本质上来说，它们允许你用单个工作节点创建 Kubernetes 集群。这种情况即将改变 - MicroK8s 早期的 alpha 版本包括集群。有了这个能力，你可以创建正如你希望多的工作节点的 KUbernetes 集群。对于创建集群来说，这是一个没有主见的选项 - 开发者在节点之间创建网络连接和集成了其他所需要的基础设施，比如一个外部的负载均衡。总的来说，MicroK8s 提供了一种快速简易的方法，使得少量的计算机和虚拟机变成一个多节点的 Kubernetes 集群。以后我们将撰写更多这种体系结构的文章。
&amp;lt;!&amp;ndash;&lt;/p&gt;

&lt;h2 id=&#34;disclaimer&#34;&gt;Disclaimer&lt;/h2&gt;

&lt;p&gt;This is not an official guide to MicroK8s. You may find detailed information on running and using MicroK8s on it&amp;rsquo;s official &lt;a href=&#34;https://microk8s.io/docs/&#34; target=&#34;_blank&#34;&gt;webpage&lt;/a&gt;, where different use cases, operating systems, environments, etc. are covered. Instead, the purpose of this post is to provide clear and easy guidelines for running MicroK8s on Linux.
&amp;ndash;&amp;gt;&lt;/p&gt;

&lt;h2 id=&#34;免责声明&#34;&gt;免责声明&lt;/h2&gt;

&lt;p&gt;这不是 MicroK8s 官方介绍文档。你可以在它的官方&lt;a href=&#34;https://microk8s.io/docs/&#34; target=&#34;_blank&#34;&gt;网页&lt;/a&gt;查询运行和使用 MicroK8s 的详情信息，其中覆盖了不同的用例，操作系统，环境等。相反，这篇文章的意图是提供在 Linux 上运行 MicroK8s 清晰易懂的指南。
&amp;lt;!&amp;ndash;&lt;/p&gt;

&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;

&lt;p&gt;A Linux distribution that &lt;a href=&#34;https://snapcraft.io/docs/installing-snapd&#34; target=&#34;_blank&#34;&gt;supports snaps&lt;/a&gt;, is required. In this guide, we’ll use Ubuntu 18.04 LTS, it supports snaps out-of-the-box.
If you are interested in running Microk8s on Windows or Mac, you should check out &lt;a href=&#34;https://multipass.run&#34; target=&#34;_blank&#34;&gt;Multipass&lt;/a&gt; to stand up a quick Ubuntu VM as the official way to run virtual Ubuntu on your system.
&amp;ndash;&amp;gt;&lt;/p&gt;

&lt;h2 id=&#34;前提条件&#34;&gt;前提条件&lt;/h2&gt;

&lt;p&gt;一个&lt;a href=&#34;https://snapcraft.io/docs/installing-snapd&#34; target=&#34;_blank&#34;&gt;支持 snaps&lt;/a&gt; 的 Linux 发行版是被需要的。这篇指南，我们将会用支持 snaps 且即开即用的 Ubuntu 18.04 LTS。如果你对运行在 Windows 或者 Mac 上的 MicroK8s 感兴趣，你应该检查&lt;a href=&#34;https://multipass.run&#34; target=&#34;_blank&#34;&gt;多通道&lt;/a&gt;，安装一个快速的 Ubuntu VM，作为在你的系统上运行虚拟机 Ubuntu 的官方方式。
&amp;lt;!&amp;ndash;&lt;/p&gt;

&lt;h2 id=&#34;microk8s-installation&#34;&gt;MicroK8s installation&lt;/h2&gt;

&lt;p&gt;MicroK8s installation is straightforward:
&amp;ndash;&amp;gt;&lt;/p&gt;

&lt;h2 id=&#34;microk8s-安装&#34;&gt;MicroK8s 安装&lt;/h2&gt;

&lt;p&gt;简洁的 MicroK8s 安装：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;sudo snap install microk8s --classic&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;center&gt;&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/001-install.png&#34; width=&#34;600&#34;/&gt; 
&lt;/figure&gt;
&lt;/center&gt;
&amp;lt;!&amp;ndash;
The command above installs a local single-node Kubernetes cluster in seconds. Once the command execution is finished, your Kubernetes cluster is up and running.&lt;/p&gt;

&lt;p&gt;You may verify the MicroK8s status with the following command:
&amp;ndash;&amp;gt;
以上的命令将会在几秒内安装一个本地单节点的 Kubernetes 集群。一旦命令执行结束，你的 Kubernetes 集群将会启动并运行。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;sudo microk8s.status&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;center&gt;&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/002-status.png&#34; width=&#34;600&#34;/&gt; 
&lt;/figure&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;!--
## Using microk8s

Using MicroK8s is as straightforward as installing it. MicroK8s itself includes a `kubectl` binary, which can be accessed by running the `microk8s.kubectl` command. As an example: 
--&gt;

&lt;h2 id=&#34;使用-microk8s&#34;&gt;使用 microk8s&lt;/h2&gt;

&lt;p&gt;使用 MicrosK8s 就像和安装它一样便捷。MicroK8s 本身包括一个 &lt;code&gt;kubectl&lt;/code&gt; 库，该库可以通过执行 &lt;code&gt;microk8s.kubectl&lt;/code&gt; 命令去访问。例如：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;microk8s.kubectl get nodes&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;center&gt;&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/003-nodes.png&#34; width=&#34;600&#34;/&gt; 
&lt;/figure&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;!--
While using the prefix `microk8s.kubectl` allows for a parallel install of another system-wide kubectl without impact, you can easily get rid of it by using the `snap alias` command:
--&gt;

&lt;p&gt;当使用前缀 &lt;code&gt;microk8s.kubectl&lt;/code&gt; 时，允许在没有影响的情况下并行地安装另一个系统级的 kubectl，你可以便捷地使用 &lt;code&gt;snap alias&lt;/code&gt; 命令摆脱它：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;sudo snap &lt;span style=&#34;color:#a2f&#34;&gt;alias&lt;/span&gt; microk8s.kubectl kubectl&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;!--
This will allow you to simply use `kubectl` after. You can revert this change using the `snap unalias` command.
--&gt;

&lt;p&gt;&lt;center&gt;&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/004-alias.png&#34; width=&#34;600&#34;/&gt; 
&lt;/figure&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;这将允许你以后便捷地使用 &lt;code&gt;kubectl&lt;/code&gt;，你可以用 &lt;code&gt;snap unalias&lt;/code&gt;命令恢复这个改变。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get nodes&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;center&gt;&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/005-nodes.png&#34; width=&#34;600&#34;/&gt; 
&lt;/figure&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;!--
## MicroK8s addons

One of the biggest benefits of using Microk8s is the fact that it also supports various add-ons and extensions. What is even more important is they are shipped out of the box, the user just has to enable them.

The full list of extensions can be checked by running the `microk8s.status` command:
--&gt;

&lt;h2 id=&#34;microk8s-插件&#34;&gt;MicroK8s 插件&lt;/h2&gt;

&lt;p&gt;使用 MIcroK8s 其中最大的好处之一事实上是也支持各种各样的插件和扩展。更重要的是它们是开箱即用的，用户仅仅需要启动它们。通过运行 &lt;code&gt;microk8s.status&lt;/code&gt; 命令检查出扩展的完整列表。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo microk8s.status
&lt;/code&gt;&lt;/pre&gt;

&lt;!--
As of the time of writing this article, the following add-ons are supported:
--&gt;

&lt;p&gt;截至到写这篇文章为止，MicroK8s 已支持以下插件：&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/006-status.png&#34; width=&#34;600&#34;/&gt; 
&lt;/figure&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;!--
More add-ons are being created and contributed by the community all the time, it definitely helps to check often!
--&gt;

&lt;p&gt;社区创建和贡献了越来越多的插件，经常检查他们是十分有帮助的。
&amp;lt;!&amp;ndash;&lt;/p&gt;

&lt;h2 id=&#34;release-channels&#34;&gt;Release channels&lt;/h2&gt;

&lt;p&gt;&amp;ndash;&amp;gt;&lt;/p&gt;

&lt;h2 id=&#34;发布渠道&#34;&gt;发布渠道&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;sudo snap info microk8s&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;center&gt;&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/010-releases.png&#34; width=&#34;600&#34;/&gt; 
&lt;/figure&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;!--
## Installing the sample application

In this tutorial we’ll use NGINX as a sample application ([the official Docker Hub image](https://hub.docker.com/_/nginx)).

It will be installed as a Kubernetes deployment:
--&gt;

&lt;h2 id=&#34;安装简单的应用&#34;&gt;安装简单的应用&lt;/h2&gt;

&lt;p&gt;在这篇指南中我将会用 NGINX 作为一个示例应用程序（&lt;a href=&#34;https://hub.docker.com/_/nginx&#34; target=&#34;_blank&#34;&gt;官方 Docker Hub 镜像&lt;/a&gt;）。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl create deployment nginx --image&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;nginx&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;!--
To verify the installation, let’s run the following:
--&gt;

&lt;p&gt;为了检查安装，让我们运行以下命令：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get deployments&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get pods&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;center&gt;&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/007-deployments.png&#34; width=&#34;600&#34;/&gt; 
&lt;/figure&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;!--
Also, we can retrieve the full output of all available objects within our Kubernetes cluster:
--&gt;

&lt;p&gt;我们也可以检索出 Kubernetes 集群中所有可用对象的完整输出。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get all --all-namespaces&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;center&gt;&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/008-all.png&#34; width=&#34;600&#34;/&gt; 
&lt;/figure&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;!--
## Uninstalling MicroK8s

Uninstalling your microk8s cluster is so easy as uninstalling the snap:
--&gt;

&lt;h2 id=&#34;卸载-mircrok8s&#34;&gt;卸载 MircroK8s&lt;/h2&gt;

&lt;p&gt;卸载您的 microk8s 集群与卸载 Snap 同样便捷。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;sudo snap remove microk8s&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;center&gt;&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/009-remove.png&#34; width=&#34;600&#34;/&gt; 
&lt;/figure&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;!--
## Screencast
--&gt;

&lt;h2 id=&#34;截屏视频&#34;&gt;截屏视频&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://asciinema.org/a/263394&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://asciinema.org/a/263394.svg&#34; alt=&#34;asciicast&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: 2019 指导委员会选举结果</title>
      <link>https://kubernetes.io/zh/blog/2019/10/03/2019-steering-committee-election-results/</link>
      <pubDate>Thu, 03 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/zh/blog/2019/10/03/2019-steering-committee-election-results/</guid>
      <description>
        
        
        

&lt;!--
---
layout: blog
title: &#34;2019 Steering Committee Election Results&#34;
date: 2019-10-03
slug: 2019-steering-committee-election-results
---
--&gt;

&lt;!--
**Authors**: Bob Killen (University of Michigan), Jorge Castro (VMware),
Brian Grant (Google), and Ihor Dvoretskyi (CNCF)
--&gt;

&lt;p&gt;&lt;strong&gt;作者&lt;/strong&gt;：Bob Killen (University of Michigan), Jorge Castro (VMware),
Brian Grant (Google), and Ihor Dvoretskyi (CNCF)
&lt;!--
The [2019 Steering Committee Election] is a landmark milestone for the
Kubernetes project. The initial bootstrap committee is graduating to emeritus
and the committee has now shrunk to its final allocation of seven seats. All
members of the Steering Committee are now fully elected by the Kubernetes
Community.
--&gt;
&lt;a href=&#34;https://git.k8s.io/community/events/elections/2021&#34; target=&#34;_blank&#34;&gt;2019 指导委员会选举&lt;/a&gt; 是 Kubernetes 项目的重要里程碑。最初的自助委员会正逐步退休，现在该委员会已缩减到最后分配的 7 个席位。指导委员会的所有成员现在都由 Kubernetes 社区选举产生。
&lt;!--
Moving forward elections will elect either 3 or 4 people to the committee for
two-year terms.
--&gt;
接下来的选举将选出 3 到 4 名委员，任期两年。
&amp;lt;!&amp;ndash;&lt;/p&gt;

&lt;h2 id=&#34;results&#34;&gt;&lt;strong&gt;Results&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;The Kubernetes Steering Committee Election is now complete and the following
candidates came ahead to secure two-year terms that start immediately
(in alphabetical order by GitHub handle):
&amp;ndash;&amp;gt;&lt;/p&gt;

&lt;h2 id=&#34;选举结果&#34;&gt;选举结果&lt;/h2&gt;

&lt;p&gt;Kubernetes 指导委员会选举现已完成，以下候选人提前获得立即开始的两年任期 (按 GitHub handle 的字母顺序排列) ：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Christoph Blecker (&lt;a href=&#34;https://github.com/cblecker&#34; target=&#34;_blank&#34;&gt;@cblecker&lt;/a&gt;), Red Hat&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Derek Carr (&lt;a href=&#34;https://github.com/derekwaynecarr&#34; target=&#34;_blank&#34;&gt;@derekwaynecarr&lt;/a&gt;), Red Hat&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Nikhita Raghunath (&lt;a href=&#34;https://github.com/nikhita&#34; target=&#34;_blank&#34;&gt;@nikhita&lt;/a&gt;), Loodse&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Paris Pittman (&lt;a href=&#34;https://github.com/parispittman&#34; target=&#34;_blank&#34;&gt;@parispittman&lt;/a&gt;)&lt;/strong&gt;, &lt;strong&gt;Google&lt;/strong&gt;
&lt;!--
They join Aaron Crickenberger ([@spiffxp]), Google; Davanum Srinivas ([@dims]),
VMware; and Timothy St. Clair ([@timothysc]), VMware, to round out the committee.
The seats held by Aaron, Davanum, and Timothy will be up for election around
this time next year.
--&gt;
他们加入了 Aaron Crickenberger (&lt;a href=&#34;https://github.com/spiffxp&#34; target=&#34;_blank&#34;&gt;@spiffxp&lt;/a&gt;)， Google；Davanum Srinivas (&lt;a href=&#34;https://github.com/dims&#34; target=&#34;_blank&#34;&gt;@dims&lt;/a&gt;)，VMware; and Timothy St. Clair (&lt;a href=&#34;https://github.com/timothysc&#34; target=&#34;_blank&#34;&gt;@timothysc&lt;/a&gt;), VMware，使得委员会更圆满。Aaron、Davanum 和 Timothy 占据的这些席位将会在明年的这个时候进行选举。&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
## Big Thanks!

* Thanks to the initial bootstrap committee for establishing the initial
  project governance and overseeing a multi-year transition period:
--&gt;

&lt;h2 id=&#34;诚挚的感谢&#34;&gt;诚挚的感谢！&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;感谢最初的引导委员会创立了最初项目的管理并监督了多年的过渡期：

&lt;ul&gt;
&lt;li&gt;Joe Beda (&lt;a href=&#34;https://github.com/jbeda&#34; target=&#34;_blank&#34;&gt;@jbeda&lt;/a&gt;), VMware&lt;/li&gt;
&lt;li&gt;Brendan Burns (&lt;a href=&#34;https://github.com/brendandburns&#34; target=&#34;_blank&#34;&gt;@brendandburns&lt;/a&gt;), Microsoft&lt;/li&gt;
&lt;li&gt;Clayton Coleman (&lt;a href=&#34;https://github.com/smarterclayton&#34; target=&#34;_blank&#34;&gt;@smarterclayton&lt;/a&gt;), Red Hat&lt;/li&gt;
&lt;li&gt;Brian Grant (&lt;a href=&#34;https://github.com/bgrant0607&#34; target=&#34;_blank&#34;&gt;@bgrant0607&lt;/a&gt;), Google&lt;/li&gt;
&lt;li&gt;Tim Hockin (&lt;a href=&#34;https://github.com/thockin&#34; target=&#34;_blank&#34;&gt;@thockin&lt;/a&gt;), Google&lt;/li&gt;
&lt;li&gt;Sarah Novotny (&lt;a href=&#34;https://github.com/sarahnovotny&#34; target=&#34;_blank&#34;&gt;@sarahnovotny&lt;/a&gt;), Microsoft&lt;/li&gt;
&lt;li&gt;Brandon Philips (&lt;a href=&#34;https://github.com/philips&#34; target=&#34;_blank&#34;&gt;@philips&lt;/a&gt;), Red Hat
&amp;lt;!&amp;ndash;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;And also thanks to the other Emeritus Steering Committee Members. Your
prior service is appreciated by the community:
&amp;ndash;&amp;gt;&lt;/li&gt;
&lt;li&gt;同样感谢其他的已退休指导委员会成员。社区对你们先前的服务表示赞赏：

&lt;ul&gt;
&lt;li&gt;Quinton Hoole (&lt;a href=&#34;https://github.com/quinton-hoole&#34; target=&#34;_blank&#34;&gt;@quinton-hoole&lt;/a&gt;), Huawei&lt;/li&gt;
&lt;li&gt;Michelle Noorali (&lt;a href=&#34;https://github.com/michelleN&#34; target=&#34;_blank&#34;&gt;@michelleN&lt;/a&gt;), Microsoft&lt;/li&gt;
&lt;li&gt;Phillip Wittrock (&lt;a href=&#34;https://github.com/pwittrock&#34; target=&#34;_blank&#34;&gt;@pwittrock&lt;/a&gt;), Google
&amp;lt;!&amp;ndash;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Thanks to the candidates that came forward to run for election. May we always
have a strong set of people who want to push the community forward like yours
in every election.&lt;/li&gt;
&lt;li&gt;Thanks to all 377 voters who cast a ballot.&lt;/li&gt;
&lt;li&gt;And last but not least…Thanks to Cornell University for hosting &lt;a href=&#34;https://civs.cs.cornell.edu/&#34; target=&#34;_blank&#34;&gt;CIVS&lt;/a&gt;!
&amp;ndash;&amp;gt;&lt;/li&gt;
&lt;li&gt;感谢参选的候选人。 愿在每次选举中，我们都能拥有一群像您一样推动社区向前发展的人。&lt;/li&gt;
&lt;li&gt;感谢所有投票的377位选民。&lt;/li&gt;

&lt;li&gt;&lt;p&gt;最后，感谢康奈尔大学举办的 &lt;a href=&#34;https://civs.cs.cornell.edu/&#34; target=&#34;_blank&#34;&gt;CIVS&lt;/a&gt;!
&amp;lt;!&amp;ndash;&lt;/p&gt;

&lt;h2 id=&#34;get-involved-with-the-steering-committee&#34;&gt;Get Involved with the Steering Committee&lt;/h2&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can follow along with Steering Committee &lt;a href=&#34;https://github.com/kubernetes/steering/projects/1&#34; target=&#34;_blank&#34;&gt;backlog items&lt;/a&gt; and weigh in by
filing an issue or creating a PR against their &lt;a href=&#34;https://github.com/kubernetes/steering&#34; target=&#34;_blank&#34;&gt;repo&lt;/a&gt;. They meet bi-weekly on
&lt;a href=&#34;https://github.com/kubernetes/steering&#34; target=&#34;_blank&#34;&gt;Wednesdays at 8pm UTC&lt;/a&gt; and regularly attend Meet Our Contributors.  They can
also be contacted at their public mailing list &lt;a href=&#34;mailto:steering@kubernetes.io&#34; target=&#34;_blank&#34;&gt;steering@kubernetes.io&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Steering Committee Meetings:
&amp;ndash;&amp;gt;&lt;/p&gt;

&lt;h2 id=&#34;参与指导委员会&#34;&gt;参与指导委员会&lt;/h2&gt;

&lt;p&gt;你可以跟进指导委员会的 &lt;a href=&#34;https://github.com/kubernetes/steering/projects/1&#34; target=&#34;_blank&#34;&gt;代办事项&lt;/a&gt;，通过提出问题或者向 &lt;a href=&#34;https://github.com/kubernetes/steering&#34; target=&#34;_blank&#34;&gt;仓库&lt;/a&gt; 提交一个 pr 。他们每两周一次，在 &lt;a href=&#34;https://github.com/kubernetes/steering&#34; target=&#34;_blank&#34;&gt;UTC 时间周三晚上 8 点&lt;/a&gt; 会面，并定期与我们的贡献者见面。也可以通过他们的公共邮件列表 &lt;a href=&#34;mailto:steering@kubernetes.io&#34; target=&#34;_blank&#34;&gt;steering@kubernetes.io&lt;/a&gt; 联系他们。&lt;/p&gt;

&lt;p&gt;指导委员会会议：
&lt;!--
* [YouTube Playlist]
--&gt;
* &lt;a href=&#34;https://www.youtube.com/playlist?list=PL69nYSiGNLP1yP1B_nd9-drjoxp0Q14qM&#34; target=&#34;_blank&#34;&gt;YouTube 播放列表&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: 欢迎参加在上海举行的贡献者峰会</title>
      <link>https://kubernetes.io/zh/blog/2019/06/11/%E6%AC%A2%E8%BF%8E%E5%8F%82%E5%8A%A0%E5%9C%A8%E4%B8%8A%E6%B5%B7%E4%B8%BE%E8%A1%8C%E7%9A%84%E8%B4%A1%E7%8C%AE%E8%80%85%E5%B3%B0%E4%BC%9A/</link>
      <pubDate>Tue, 11 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/zh/blog/2019/06/11/%E6%AC%A2%E8%BF%8E%E5%8F%82%E5%8A%A0%E5%9C%A8%E4%B8%8A%E6%B5%B7%E4%B8%BE%E8%A1%8C%E7%9A%84%E8%B4%A1%E7%8C%AE%E8%80%85%E5%B3%B0%E4%BC%9A/</guid>
      <description>
        
        
        

&lt;!-- ---
layout: blog
title: &#39;Join us at the Contributor Summit in Shanghai&#39;
date: 2019-06-11
--- --&gt;

&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Josh Berkus (Red Hat)&lt;/p&gt;

&lt;!-- ![Picture of contributor panel at 2018 Shanghai contributor summit.  Photo by Josh Berkus, licensed CC-BY 4.0](/images/blog/2019-
06-11-contributor-summit-shanghai/panel.png) --&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2019-
06-11-contributor-summit-shanghai/panel.png&#34; alt=&#34;贡献者小组讨论掠影，摄于 2018 年上海贡献者峰会，作者 Josh Berkus, 许可证 CC-BY 4.0&#34; /&gt;&lt;/p&gt;

&lt;!-- For the second year, we will have [a Contributor Summit event](https://www.lfasiallc.com/events/contributors-summit-china-2019/) the day before [KubeCon China](https://events.linuxfoundation.cn/events/kubecon-cloudnativecon-china-2019/) in Shanghai. If you already contribute to Kubernetes or would like to contribute, please consider attending and [register](https://www.lfasiallc.com/events/contributors-summit-china-2019/register/). The Summit will be held June 24th, at the Shanghai Expo Center (the same location where KubeCon will take place), and will include a Current Contributor Day as well as the New Contributor Workshop and the Documentation Sprints. --&gt;

&lt;p&gt;连续第二年，我们将在 &lt;a href=&#34;https://events.linuxfoundation.cn/events/kubecon-cloudnativecon-china-2019/&#34; target=&#34;_blank&#34;&gt;KubeCon China&lt;/a&gt; 之前举行一天的 &lt;a href=&#34;https://www.lfasiallc.com/events/contributors-summit-china-2019/&#34; target=&#34;_blank&#34;&gt;贡献者峰会&lt;/a&gt;。
不管您是否已经是一名 Kubernetes 贡献者，还是想要加入社区队伍，贡献一份力量，都请考虑&lt;a href=&#34;https://www.lfasiallc.com/events/contributors-summit-china-2019/register/&#34; target=&#34;_blank&#34;&gt;注册&lt;/a&gt;参加这次活动。
这次峰会将于六月 24 号，在上海世博中心（和 KubeCon 的举办地点相同）举行，
一天的活动将包含“现有贡献者活动”，以及“新贡献者工作坊”和“文档小组活动”。&lt;/p&gt;

&lt;!-- ### Current Contributor Day --&gt;

&lt;h3 id=&#34;现有贡献者活动&#34;&gt;现有贡献者活动&lt;/h3&gt;

&lt;!-- After last year&#39;s Contributor Day, our team received feedback that many of our contributors in Asia and Oceania would like content for current contributors as well. As such, we have added a Current Contributor track to the schedule. --&gt;

&lt;p&gt;去年的贡献者节之后，我们的团队收到了很多反馈意见，很多亚洲和大洋洲的贡献者也想要针对当前贡献者的峰会内容。
有鉴于此，我们在今年的安排中加入了当前贡献者的主题。&lt;/p&gt;

&lt;!-- While we do not yet have a full schedule up, the topics covered in the current contributor track will include: --&gt;

&lt;p&gt;尽管我们还没有一个完整的时间安排，下面是当前贡献者主题所会包含的话题：&lt;/p&gt;

&lt;!-- * How to write a KEP (Kubernetes Enhancement Proposal)
* Codebase and repository review
* Local Build &amp; Test troubleshooting session
* Guide to Non-Code Contribution opportunities
* SIG-Azure face-to-face meeting
* SIG-Scheduling face-to-face meeting
* Other SIG face-to-face meetings as we confirm them --&gt;

&lt;ul&gt;
&lt;li&gt;如何撰写 Kubernetes 改进议案 (KEP)&lt;/li&gt;
&lt;li&gt;代码库研习&lt;/li&gt;
&lt;li&gt;本地构建以及测试调试&lt;/li&gt;
&lt;li&gt;不写代码的贡献机会&lt;/li&gt;
&lt;li&gt;SIG-Azure 面对面交流&lt;/li&gt;
&lt;li&gt;SIG-Scheduling 面对面交流&lt;/li&gt;
&lt;li&gt;其他兴趣小组的面对面机会&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- The schedule will be on [the Community page](https://github.com/kubernetes/community/tree/master/events/2019/06-contributor-summit) once it is complete. --&gt;

&lt;p&gt;整个计划安排将会在完全确定之后，整理放在&lt;a href=&#34;https://github.com/kubernetes/community/tree/master/events/2019/06-contributor-summit&#34; target=&#34;_blank&#34;&gt;社区页面&lt;/a&gt;上。&lt;/p&gt;

&lt;!-- If your SIG wants to have a face-to-face meeting at Kubecon Shanghai, please contact [Josh Berkus](mailto:jberkus@redhat.com). --&gt;

&lt;p&gt;如果您的 SIG 想要在 Kubecon Shanghai 上进行面对面的交流，请联系 &lt;a href=&#34;mailto:jberkus@redhat.com&#34; target=&#34;_blank&#34;&gt;Josh Berkus&lt;/a&gt;。&lt;/p&gt;

&lt;!-- ### New Contributor Workshop --&gt;

&lt;h3 id=&#34;新贡献者工作坊&#34;&gt;新贡献者工作坊&lt;/h3&gt;

&lt;!-- Students at [last year&#39;s New Contributor Workshop](/blog/2018/12/05/new-contributor-workshop-shanghai/) (NCW) found it to be extremely valuable, and the event helped to orient a few of the many Asian and Pacific developers looking to participate in the Kubernetes community. --&gt;

&lt;p&gt;参与过&lt;a href=&#34;https://kubernetes.io/blog/2018/12/05/new-contributor-workshop-shanghai/&#34;&gt;去年新贡献者工作坊（NCW）&lt;/a&gt;的学生觉得这项活动非常的有价值，
这项活动也帮助、引导了很多亚洲和大洋洲的开发者更多地参与到 Kubernetes 社区之中。&lt;/p&gt;

&lt;!-- &gt; &#34;It&#39;s a one-stop-shop for becoming familiar with the community.&#34; said one participant. --&gt;

&lt;blockquote&gt;
&lt;p&gt;“这次活动可以让人一次快速熟悉社区。”其中的一位参与者提到。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!-- If you have not contributed to Kubernetes before, or have only done one or two things, please consider [enrolling](https://www.lfasiallc.com/events/contributors-summit-china-2019/register/) in the NCW. --&gt;

&lt;p&gt;如果您之前从没有参与过 Kubernetes 的贡献，或者只是做过一次或两次贡献，都请考虑&lt;a href=&#34;https://www.lfasiallc.com/events/contributors-summit-china-2019/register/&#34; target=&#34;_blank&#34;&gt;注册参加&lt;/a&gt;新贡献者工作坊。&lt;/p&gt;

&lt;!-- &gt; &#34;Got to know the process from signing CLA to PR and made friends with other contributors.&#34; said another. --&gt;

&lt;blockquote&gt;
&lt;p&gt;“熟悉了从 CLA 到 PR 的整个流程，也认识结交了很多贡献者。”另一位开发者提到。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!-- ### Documentation Sprints --&gt;

&lt;h3 id=&#34;文档小组活动&#34;&gt;文档小组活动&lt;/h3&gt;

&lt;!-- Both old and new contributors on our Docs Team will spend a day both improving our documentation and translating it into other languages. If you are interested in having better documentation, fully localized into Chinese and other languages, please [sign up](https://www.lfasiallc.com/events/contributors-summit-china-2019/register/) to help with the Doc Sprints. --&gt;

&lt;p&gt;文档小组的新老贡献者都会聚首一天，讨论如何提升文档质量，以及将文档翻译成更多的语言。
如果您对翻译文档，将这些知识和信息翻译成中文和其他语言感兴趣的话，请在这里&lt;a href=&#34;https://www.lfasiallc.com/events/contributors-summit-china-2019/register/&#34; target=&#34;_blank&#34;&gt;注册&lt;/a&gt;，报名参加文档小组活动。&lt;/p&gt;

&lt;!-- ### Before you attend --&gt;

&lt;h3 id=&#34;参与之前&#34;&gt;参与之前&lt;/h3&gt;

&lt;!-- Regardless of where you participate, everyone at the Contributor Summit should [sign the Kubernetes Contributor License Agreement](https://git.k8s.io/community/CLA.md#the-contributor-license-agreement) (CLA) before coming to the conference. You should also bring a laptop suitable for working on documentation or code development. --&gt;

&lt;p&gt;不论您参与的是哪一项活动，所有人都需要在到达贡献者峰会前签署 &lt;a href=&#34;https://git.k8s.io/community/CLA.md#the-contributor-license-agreement&#34; target=&#34;_blank&#34;&gt;Kubernetes CLA&lt;/a&gt;。
您也同时需要考虑带一个合适的笔记本电脑，帮助文档写作或是编程开发。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: 如何参与 Kubernetes 文档的本地化工作</title>
      <link>https://kubernetes.io/zh/blog/2019/04/26/%E5%A6%82%E4%BD%95%E5%8F%82%E4%B8%8E-kubernetes-%E6%96%87%E6%A1%A3%E7%9A%84%E6%9C%AC%E5%9C%B0%E5%8C%96%E5%B7%A5%E4%BD%9C/</link>
      <pubDate>Fri, 26 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/zh/blog/2019/04/26/%E5%A6%82%E4%BD%95%E5%8F%82%E4%B8%8E-kubernetes-%E6%96%87%E6%A1%A3%E7%9A%84%E6%9C%AC%E5%9C%B0%E5%8C%96%E5%B7%A5%E4%BD%9C/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;作者: Zach Corleissen（Linux 基金会）&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;去年我们对 Kubernetes 网站进行了优化，加入了&lt;a href=&#34;https://kubernetes.io/blog/2018/11/08/kubernetes-docs-updates-international-edition/&#34; target=&#34;_blank&#34;&gt;多语言内容的支持&lt;/a&gt;。贡献者们踊跃响应，加入了多种新的本地化内容：截至 2019 年 4 月，Kubernetes 文档有了 9 个不同语言的未完成版本，其中有 6 个是 2019 年加入的。在每个 Kubernetes 文档页面的上方，读者都可以看到一个语言选择器，其中列出了所有可用语言。&lt;/p&gt;

&lt;p&gt;不论是完成度最高的&lt;a href=&#34;https://v1-12.docs.kubernetes.io/zh/&#34; target=&#34;_blank&#34;&gt;中文版 v1.12&lt;/a&gt;，还是最新加入的&lt;a href=&#34;https://kubernetes.io/pt/&#34; target=&#34;_blank&#34;&gt;葡萄牙文版 v1.14&lt;/a&gt;，各语言的本地化内容还未完成，这是一个进行中的项目。如果读者有兴趣对现有本地化工作提供支持，请继续阅读。&lt;/p&gt;

&lt;h2 id=&#34;什么是本地化&#34;&gt;什么是本地化&lt;/h2&gt;

&lt;p&gt;翻译是以词表意的问题。而本地化在此基础之上，还包含了过程和设计方面的工作。&lt;/p&gt;

&lt;p&gt;本地化和翻译很像，但是包含更多内容。除了进行翻译之外，本地化还要为编写和发布过程的框架进行优化。例如，Kubernetes.io 多数的站点浏览功能（按钮文字）都保存在&lt;a href=&#34;https://github.com/kubernetes/website/tree/master/i18n&#34; target=&#34;_blank&#34;&gt;单独的文件&lt;/a&gt;之中。所以启动新本地化的过程中，需要包含加入对特定文件中字符串进行翻译的工作。&lt;/p&gt;

&lt;p&gt;本地化很重要，能够有效的降低 Kubernetes 的采纳和支持门槛。如果能用母语阅读 Kubernetes 文档，就能更轻松的开始使用 Kubernetes，并对其发展作出贡献。&lt;/p&gt;

&lt;h2 id=&#34;如何启动本地化工作&#34;&gt;如何启动本地化工作&lt;/h2&gt;

&lt;p&gt;不同语言的本地化工作都是单独的功能——和其它 Kubernetes 功能一致，贡献者们在一个 SIG 中进行本地化工作，分享出来进行评审，并加入项目。&lt;/p&gt;

&lt;p&gt;贡献者们在团队中进行内容的本地化工作。因为自己不能批准自己的 PR，所以一个本地化团队至少应该有两个人——例如意大利文的本地化团队有两个人。这个团队规模可能很大：中文团队有几十个成员。&lt;/p&gt;

&lt;p&gt;每个团队都有自己的工作流。有些团队手工完成所有的内容翻译；有些会使用带有翻译插件的编译器，并使用评审机来提供正确性的保障。SIG Docs 专注于输出的标准；这就给了本地化团队采用适合自己工作情况的工作流。这样一来，团队可以根据最佳实践进行协作，并以 Kubernetes 的社区精神进行分享。&lt;/p&gt;

&lt;h2 id=&#34;为本地化工作添砖加瓦&#34;&gt;为本地化工作添砖加瓦&lt;/h2&gt;

&lt;p&gt;如果你有兴趣为 Kubernetes 文档加入新语种的本地化内容，&lt;a href=&#34;https://kubernetes.io/docs/contribute/localization/&#34; target=&#34;_blank&#34;&gt;Kubernetes contribution guide&lt;/a&gt; 中包含了这方面的相关内容。&lt;/p&gt;

&lt;p&gt;已经启动的的本地化工作同样需要支持。如果有兴趣为现存项目做出贡献，可以加入本地化团队的 Slack 频道，去做个自我介绍。各团队的成员会帮助你开始工作。&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;语种&lt;/th&gt;
&lt;th&gt;Slack 频道&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;中文&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://kubernetes.slack.com/messages/CE3LNFYJ1/&#34; target=&#34;_blank&#34;&gt;#kubernetes-docs-zh&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;英文&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://kubernetes.slack.com/messages/C1J0BPD2M/&#34; target=&#34;_blank&#34;&gt;#sig-docs&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;法文&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://kubernetes.slack.com/messages/CG838BFT9/&#34; target=&#34;_blank&#34;&gt;#kubernetes-docs-fr&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;德文&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://kubernetes.slack.com/messages/CH4UJ2BAL/&#34; target=&#34;_blank&#34;&gt;#kubernetes-docs-de&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;印地&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://kubernetes.slack.com/messages/CJ14B9BDJ/&#34; target=&#34;_blank&#34;&gt;#kubernetes-docs-hi&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;印度尼西亚文&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://kubernetes.slack.com/messages/CJ1LUCUHM/&#34; target=&#34;_blank&#34;&gt;#kubernetes-docs-id&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;意大利文&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://kubernetes.slack.com/messages/CGB1MCK7X/&#34; target=&#34;_blank&#34;&gt;#kubernetes-docs-it&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;日文&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://kubernetes.slack.com/messages/CAG2M83S8/&#34; target=&#34;_blank&#34;&gt;#kubernetes-docs-ja&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;韩文&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://kubernetes.slack.com/messages/CA1MMR86S/&#34; target=&#34;_blank&#34;&gt;#kubernetes-docs-ko&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;葡萄牙文&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://kubernetes.slack.com/messages/CJ21AS0NA/&#34; target=&#34;_blank&#34;&gt;#kubernetes-docs-pt&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;西班牙文&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://kubernetes.slack.com/messages/CH7GB2E3B/&#34; target=&#34;_blank&#34;&gt;#kubernetes-docs-es&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;下一步&#34;&gt;下一步？&lt;/h2&gt;

&lt;p&gt;最新的&lt;a href=&#34;https://kubernetes.slack.com/messages/CJ14B9BDJ/&#34; target=&#34;_blank&#34;&gt;印地文本地化&lt;/a&gt;工作正在启动。为什么不加入你的语言？&lt;/p&gt;

&lt;p&gt;身为 SIG Docs 的主席，我甚至希望本地化工作跳出文档范畴，直接为 Kubernetes 组件提供本地化支持。有什么组件是你希望支持不同语言的么？可以提交一个 &lt;a href=&#34;https://github.com/kubernetes/enhancements/tree/master/keps&#34; target=&#34;_blank&#34;&gt;Kubernetes Enhancement Proposal&lt;/a&gt; 来促成这一进步。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: 新贡献者工作坊上海站</title>
      <link>https://kubernetes.io/zh/blog/2018/12/05/%E6%96%B0%E8%B4%A1%E7%8C%AE%E8%80%85%E5%B7%A5%E4%BD%9C%E5%9D%8A%E4%B8%8A%E6%B5%B7%E7%AB%99/</link>
      <pubDate>Wed, 05 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/zh/blog/2018/12/05/%E6%96%B0%E8%B4%A1%E7%8C%AE%E8%80%85%E5%B7%A5%E4%BD%9C%E5%9D%8A%E4%B8%8A%E6%B5%B7%E7%AB%99/</guid>
      <description>
        
        
        &lt;!-- 
---
layout: blog
title: &#39;New Contributor Workshop Shanghai&#39;
date: 2018-12-05
---
 --&gt;

&lt;!--
**Authors**: Josh Berkus (Red Hat), Yang Li (The Plant), Puja Abbassi (Giant Swarm), XiangPeng Zhao (ZTE)
 --&gt;

&lt;p&gt;&lt;strong&gt;作者&lt;/strong&gt;: Josh Berkus (红帽), Yang Li (The Plant), Puja Abbassi (Giant Swarm), XiangPeng Zhao (中兴通讯)&lt;/p&gt;

&lt;!--
&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2018-12-05-new-contributor-shanghai/attendees.png&#34;
         alt=&#34;KubeCon Shanghai New Contributor Summit attendees. Photo by Jerry Zhang&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;KubeCon Shanghai New Contributor Summit attendees. Photo by Jerry Zhang&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;

 --&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2018-12-05-new-contributor-shanghai/attendees.png&#34;
         alt=&#34;KubeCon 上海站新贡献者峰会与会者，摄影：Jerry Zhang&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;KubeCon 上海站新贡献者峰会与会者，摄影：Jerry Zhang&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;


&lt;!--
We recently completed our first New Contributor Summit in China, at the first KubeCon in China. It was very exciting to see all of the Chinese and Asian developers (plus a few folks from around the world) interested in becoming contributors. Over the course of a long day, they learned how, why, and where to contribute to Kubernetes, created pull requests, attended a panel of current contributors, and got their CLAs signed.
 --&gt;

&lt;p&gt;最近，在中国的首次 KubeCon 上，我们完成了在中国的首次新贡献者峰会。看到所有中国和亚洲的开发者（以及来自世界各地的一些人）有兴趣成为贡献者，这令人非常兴奋。在长达一天的课程中，他们了解了如何、为什么以及在何处为 Kubernetes 作出贡献，创建了 PR，参加了贡献者圆桌讨论，并签署了他们的 CLA。&lt;/p&gt;

&lt;!--
This was our second New Contributor Workshop (NCW), building on the one created and led by SIG Contributor Experience members in Copenhagen. Because of the audience, it was held in both Chinese and English, taking advantage of the superb simultaneous interpretation services the CNCF sponsored. Likewise, the NCW team included both English and Chinese-speaking members of the community: Yang Li, XiangPeng Zhao, Puja Abbassi, Noah Abrahams, Tim Pepper, Zach Corleissen, Sen Lu, and Josh Berkus. In addition to presenting and helping students, the bilingual members of the team translated all of the slides into Chinese. Fifty-one students attended.
 --&gt;

&lt;p&gt;这是我们的第二届新贡献者工作坊（NCW），它由前一次贡献者体验 SIG 成员创建和领导的哥本哈根研讨会延伸而来。根据受众情况，本次活动采用了中英文两种语言，充分利用了 CNCF 赞助的一流的同声传译服务。同样，NCW 团队由社区成员组成，既有说英语的，也有说汉语的：Yang Li、XiangPeng Zhao、Puja Abbassi、Noah Abrahams、Tim Pepper、Zach Corleissen、Sen Lu 和 Josh Berkus。除了演讲和帮助学员外，团队的双语成员还将所有幻灯片翻译成了中文。共有五十一名学员参加。&lt;/p&gt;

&lt;!--
&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2018-12-05-new-contributor-shanghai/noahabrahams.png&#34;
         alt=&#34;Noah Abrahams explains Kubernetes communications channels. Photo by Jerry Zhang&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;Noah Abrahams explains Kubernetes communications channels. Photo by Jerry Zhang&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;

 --&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2018-12-05-new-contributor-shanghai/noahabrahams.png&#34;
         alt=&#34;Noah Abrahams 讲解 Kubernetes 沟通渠道。摄影：Jerry Zhang&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;Noah Abrahams 讲解 Kubernetes 沟通渠道。摄影：Jerry Zhang&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;


&lt;!--
The NCW takes participants through the stages of contributing to Kubernetes, starting from deciding where to contribute, followed by an introduction to the SIG system and our repository structure. We also have &#34;guest speakers&#34; from Docs and Test Infrastructure who cover contributing in those areas. We finally wind up with some hands-on exercises in filing issues and creating and approving PRs.
 --&gt;

&lt;p&gt;NCW 让参与者完成了为 Kubernetes 作出贡献的各个阶段，从决定在哪里作出贡献开始，接着介绍了 SIG 系统和我们的代码仓库结构。我们还有来自文档和测试基础设施领域的「客座讲者」，他们负责讲解有关的贡献。最后，我们在创建 issue、提交并批准 PR 的实践练习后，结束了工作坊。&lt;/p&gt;

&lt;!--
Those hands-on exercises use a repository known as [the contributor playground](https://github.com/kubernetes-sigs/contributor-playground), created by SIG Contributor Experience as a place for new contributors to try out performing various actions on a Kubernetes repo. It has modified Prow and Tide automation, uses Owners files like in the real repositories. This lets students learn how the mechanics of contributing to our repositories work without disrupting normal development.
 --&gt;

&lt;p&gt;这些实践练习使用一个名为&lt;a href=&#34;https://github.com/kubernetes-sigs/contributor-playground&#34; target=&#34;_blank&#34;&gt;贡献者游乐场&lt;/a&gt;的代码仓库，由贡献者体验 SIG 创建，让新贡献者尝试在一个 Kubernetes 仓库中执行各种操作。它修改了 Prow 和 Tide 自动化，使用与真实代码仓库类似的 Owners 文件。这可以让学员了解为我们的仓库做出贡献的有关机制，同时又不妨碍正常的开发流程。&lt;/p&gt;

&lt;!--
&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2018-12-05-new-contributor-shanghai/yangli.png&#34;
         alt=&#34;Yang Li talks about getting your PRs reviewed. Photo by Josh Berkus&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;Yang Li talks about getting your PRs reviewed. Photo by Josh Berkus&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;

 --&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2018-12-05-new-contributor-shanghai/yangli.png&#34;
         alt=&#34;Yang Li 讲到如何让你的 PR 通过评审。摄影：Josh Berkus&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;Yang Li 讲到如何让你的 PR 通过评审。摄影：Josh Berkus&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;


&lt;!--
Both the &#34;Great Firewall&#34; and the language barrier prevent contributing Kubernetes from China from being straightforward. What&#39;s more, because open source business models are not mature in China, the time for employees work on open source projects is limited.
 --&gt;

&lt;p&gt;「防火长城」和语言障碍都使得在中国为 Kubernetes 作出贡献变得困难。而且，中国的开源商业模式并不成熟，员工在开源项目上工作的时间有限。&lt;/p&gt;

&lt;!--
Chinese engineers are eager to participate in the development of Kubernetes, but many of them don&#39;t know where to start since Kubernetes is such a large project. With this workshop, we hope to help those who want to contribute, whether they wish to fix some bugs they encountered, improve or localize documentation, or they need to work with Kubernetes at their work. We are glad to see more and more Chinese contributors joining the community in the past few years, and we hope to see more of them in the future.
 --&gt;

&lt;p&gt;中国工程师渴望参与 Kubernetes 的研发，但他们中的许多人不知道从何处开始，因为 Kubernetes 是一个如此庞大的项目。通过本次工作坊，我们希望帮助那些想要参与贡献的人，不论他们希望修复他们遇到的一些错误、改进或本地化文档，或者他们需要在工作中用到 Kubernetes。我们很高兴看到越来越多的中国贡献者在过去几年里加入社区，我们也希望将来可以看到更多。&lt;/p&gt;

&lt;!--
&#34;I have been participating in the Kubernetes community for about three years,&#34; said XiangPeng Zhao. &#34;In the community, I notice that more and more Chinese developers are showing their interest in contributing to Kubernetes. However, it&#39;s not easy to start contributing to such a project. I tried my best to help those who I met in the community, but I think there might still be some new contributors leaving the community due to not knowing where to get help when in trouble. Fortunately, the community initiated NCW at KubeCon Copenhagen and held a second one at KubeCon Shanghai. I was so excited to be invited by Josh Berkus to help organize this workshop. During the workshop, I met community friends in person, mentored attendees in the exercises, and so on. All of this was a memorable experience for me. I also learned a lot as a contributor who already has years of contributing experience. I wish I had attended such a workshop when I started contributing to Kubernetes years ago.&#34;
 --&gt;

&lt;p&gt;「我已经参与了 Kubernetes 社区大约三年」，XiangPeng Zhao 说，「在社区，我注意到越来越多的中国开发者表现出对 Kubernetes 贡献的兴趣。但是，开始为这样一个项目做贡献并不容易。我尽力帮助那些我在社区遇到的人，但是，我认为可能仍有一些新的贡献者离开社区，因为他们在遇到麻烦时不知道从哪里获得帮助。幸运的是，社区在 KubeCon 哥本哈根站发起了 NCW，并在 KubeCon 上海站举办了第二届。我很高兴受到 Josh Berkus 的邀请，帮助组织这个工作坊。在工作坊期间，我当面见到了社区里的朋友，在练习中指导了与会者，等等。所有这些对我来说都是难忘的经历。作为有着多年贡献者经验的我，也学习到了很多。我希望几年前我开始为 Kubernetes 做贡献时参加过这样的工作坊」。&lt;/p&gt;

&lt;!--
&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2018-12-05-new-contributor-shanghai/panel.png&#34;
         alt=&#34;Panel of contributors. Photo by Jerry Zhang&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;Panel of contributors. Photo by Jerry Zhang&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;

 --&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2018-12-05-new-contributor-shanghai/panel.png&#34;
         alt=&#34;贡献者圆桌讨论。摄影：Jerry Zhang&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;贡献者圆桌讨论。摄影：Jerry Zhang&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;


&lt;!--
The workshop ended with a panel of current contributors, featuring Lucas Käldström, Janet Kuo, Da Ma, Pengfei Ni, Zefeng Wang, and Chao Xu. The panel aimed to give both new and current contributors a look behind the scenes on the day-to-day of some of the most active contributors and maintainers, both from China and around the world. Panelists talked about where to begin your contributor&#39;s journey, but also how to interact with reviewers and maintainers. They further touched upon the main issues of contributing from China and gave attendees an outlook into exciting features they can look forward to in upcoming releases of Kubernetes.
 --&gt;

&lt;p&gt;工作坊以现有贡献者圆桌讨论结束，嘉宾包括 Lucas Käldström、Janet Kuo、Da Ma、Pengfei Ni、Zefeng Wang 和 Chao Xu。这场圆桌讨论旨在让新的和现有的贡献者了解一些最活跃的贡献者和维护者的幕后日常工作，不论他们来自中国还是世界各地。嘉宾们讨论了从哪里开始贡献者的旅程，以及如何与评审者和维护者进行互动。他们进一步探讨了在中国参与贡献的主要问题，并向与会者预告了在 Kubernetes 的未来版本中可以期待的令人兴奋的功能。&lt;/p&gt;

&lt;!--
After the workshop, XiangPeng Zhao chatted with some attendees on WeChat and Twitter about their experiences. They were very glad to have attended the NCW and had some suggestions on improving the workshop. One attendee, Mohammad, said, &#34;I had a great time at the workshop and learned a lot about the entire process of k8s for a contributor.&#34; Another attendee, Jie Jia, said, &#34;The workshop was wonderful. It systematically explained how to contribute to Kubernetes. The attendee could understand the process even if s/he knew nothing about that before. For those who were already contributors, they could also learn something new. Furthermore, I could make new friends from inside or outside of China in the workshop. It was awesome!&#34;
 --&gt;

&lt;p&gt;工作坊结束后，XiangPeng Zhao 和一些与会者就他们的经历在微信和 Twitter 上进行了交谈。他们很高兴参加了 NCW，并就改进工作坊提出了一些建议。一位名叫 Mohammad 的与会者说：「我在工作坊上玩得很开心，学习了参与 k8s 贡献的整个过程。」另一位与会者 Jie Jia 说：「工作坊非常精彩。它系统地解释了如何为 Kubernetes 做出贡献。即使参与者之前对此一无所知，他（她）也可以理解这个过程。对于那些已经是贡献者的人，他们也可以学习到新东西。此外，我还可以在工作坊上结识来自国内外的新朋友。真是棒极了！」&lt;/p&gt;

&lt;!--
SIG Contributor Experience will continue to run New Contributor Workshops at each upcoming KubeCon, including Seattle, Barcelona, and the return to Shanghai in June 2019. If you failed to get into one this year, register for one at a future KubeCon. And, when you meet an NCW attendee, make sure to welcome them to the community.
 --&gt;

&lt;p&gt;贡献者体验 SIG 将继续在未来的 KubeCon 上举办新贡献者工作坊，包括西雅图站、巴塞罗那站，然后在 2019 年六月回到上海。如果你今年未能参加，请在未来的 KubeCon 上注册。并且，如果你遇到工作坊的与会者，请务必欢迎他们加入社区。&lt;/p&gt;

&lt;!--
Links:
 --&gt;

&lt;p&gt;链接：&lt;/p&gt;

&lt;!--
* English versions of the slides: [PDF](https://gist.github.com/jberkus/889be25c234b01761ce44eccff816380#file-kubernetes-shanghai-english-pdf) or [Google Docs with speaker notes](https://docs.google.com/presentation/d/1l5f_iAFsKg50LFq3N80KbZKUIEL_tyCaUoWPzSxColo/edit?usp=sharing)
* Chinese version of the slides: [PDF](https://gist.github.com/jberkus/889be25c234b01761ce44eccff816380#file-kubernetes-shanghai-cihinese-pdf)
* [Contributor playground](https://github.com/kubernetes-sigs/contributor-playground)
 --&gt;

&lt;ul&gt;
&lt;li&gt;中文版幻灯片：&lt;a href=&#34;https://gist.github.com/jberkus/889be25c234b01761ce44eccff816380#file-kubernetes-shanghai-cihinese-pdf&#34; target=&#34;_blank&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;英文版幻灯片：&lt;a href=&#34;https://gist.github.com/jberkus/889be25c234b01761ce44eccff816380#file-kubernetes-shanghai-english-pdf&#34; target=&#34;_blank&#34;&gt;PDF&lt;/a&gt; 或 &lt;a href=&#34;https://docs.google.com/presentation/d/1l5f_iAFsKg50LFq3N80KbZKUIEL_tyCaUoWPzSxColo/edit?usp=sharing&#34; target=&#34;_blank&#34;&gt;带有演讲者笔记的 Google Docs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-sigs/contributor-playground&#34; target=&#34;_blank&#34;&gt;贡献者游乐场&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes 文档更新，国际版</title>
      <link>https://kubernetes.io/zh/blog/2018/11/08/kubernetes-%E6%96%87%E6%A1%A3%E6%9B%B4%E6%96%B0%E5%9B%BD%E9%99%85%E7%89%88/</link>
      <pubDate>Thu, 08 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/zh/blog/2018/11/08/kubernetes-%E6%96%87%E6%A1%A3%E6%9B%B4%E6%96%B0%E5%9B%BD%E9%99%85%E7%89%88/</guid>
      <description>
        
        
        

&lt;!--
---
layout: blog
title: &#39;Kubernetes Docs Updates, International Edition&#39;
date: 2018-11-08
---
--&gt;

&lt;!-- **Author**: Zach Corleissen (Linux Foundation) --&gt;

&lt;p&gt;&lt;strong&gt;作者&lt;/strong&gt;：Zach Corleissen （Linux 基金会）&lt;/p&gt;

&lt;!-- As a co-chair of SIG Docs, I&#39;m excited to share that Kubernetes docs have a fully mature workflow for localization (l10n).  --&gt;

&lt;p&gt;作为文档特别兴趣小组（SIG Docs）的联合主席，我很高兴能与大家分享 Kubernetes 文档在本地化（l10n）方面所拥有的一个完全成熟的工作流。&lt;/p&gt;

&lt;!-- ## Abbreviations galore --&gt;

&lt;h2 id=&#34;丰富的缩写&#34;&gt;丰富的缩写&lt;/h2&gt;

&lt;!-- L10n is an abbreviation for _localization_. --&gt;

&lt;p&gt;L10n 是 &lt;em&gt;localization&lt;/em&gt; 的缩写。&lt;/p&gt;

&lt;!-- I18n is an abbreviation for _internationalization_.  --&gt;

&lt;p&gt;I18n 是 &lt;em&gt;internationalization&lt;/em&gt; 的缩写。&lt;/p&gt;

&lt;!-- I18n is [what you do](https://www.w3.org/International/questions/qa-i18n) to make l10n easier. L10n is a fuller, more comprehensive process than translation (_t9n_). --&gt;

&lt;p&gt;I18n 定义了&lt;a href=&#34;https://www.w3.org/International/questions/qa-i18n&#34; target=&#34;_blank&#34;&gt;做什么&lt;/a&gt; 能让 l10n 更容易。而 L10n 更全面，相比翻译（ &lt;em&gt;t9n&lt;/em&gt; ）具备更完善的流程。&lt;/p&gt;

&lt;!-- ## Why localization matters --&gt;

&lt;h2 id=&#34;为什么本地化很重要&#34;&gt;为什么本地化很重要&lt;/h2&gt;

&lt;!-- The goal of SIG Docs is to make Kubernetes easier to use for as many people as possible. --&gt;

&lt;p&gt;SIG Docs 的目标是让 Kubernetes 更容易为尽可能多的人使用。&lt;/p&gt;

&lt;!-- One year ago, we looked at whether it was possible to host the output of a Chinese team working independently to translate the Kubernetes docs. After many conversations (including experts on OpenStack l10n), [much transformation](https://kubernetes.io/blog/2018/05/05/hugo-migration/), and [renewed commitment to easier localization](https://github.com/kubernetes/website/pull/10485), we realized that open source documentation is, like open source software, an ongoing exercise at the edges of what&#39;s possible. --&gt;

&lt;p&gt;一年前，我们研究了是否有可能由一个独立翻译 Kubernetes 文档的中国团队来主持文档输出。经过多次交谈（包括 OpenStack l10n 的专家），&lt;a href=&#34;https://kubernetes.io/blog/2018/05/05/hugo-migration/&#34; target=&#34;_blank&#34;&gt;多次转变&lt;/a&gt;，以及&lt;a href=&#34;https://github.com/kubernetes/website/pull/10485&#34; target=&#34;_blank&#34;&gt;重新致力于更轻松的本地化&lt;/a&gt;，我们意识到，开源文档就像开源软件一样，是在可能的边缘不断进行实践。&lt;/p&gt;

&lt;!-- Consolidating workflows, language labels, and team-level ownership may seem like simple improvements, but these features make l10n scalable for increasing numbers of l10n teams. While SIG Docs continues to iterate improvements, we&#39;ve paid off a significant amount of technical debt and streamlined l10n in a single workflow. That&#39;s great for the future as well as the present. --&gt;

&lt;p&gt;整合工作流程、语言标签和团队级所有权可能看起来像是十分简单的改进，但是这些功能使 l10n 可以扩展到规模越来越大的 l10n 团队。随着 SIG Docs 不断改进，我们已经在单一工作流程中偿还了大量技术债务并简化了 l10n。这对未来和现在都很有益。&lt;/p&gt;

&lt;!-- ## Consolidated workflow --&gt;

&lt;h2 id=&#34;整合的工作流程&#34;&gt;整合的工作流程&lt;/h2&gt;

&lt;!-- Localization is now consolidated in the [kubernetes/website](https://github.com/kubernetes/website) repository. We&#39;ve configured the Kubernetes CI/CD system, [Prow](https://github.com/kubernetes/test-infra/tree/master/prow), to handle automatic language label assignment as well as team-level PR review and approval. --&gt;

&lt;p&gt;现在，本地化已整合到 &lt;a href=&#34;https://github.com/kubernetes/website&#34; target=&#34;_blank&#34;&gt;kubernetes/website&lt;/a&gt; 存储库。我们已经配置了 Kubernetes CI/CD 系统，&lt;a href=&#34;https://github.com/kubernetes/test-infra/tree/master/prow&#34; target=&#34;_blank&#34;&gt;Prow&lt;/a&gt; 来处理自动语言标签分配以及团队级 PR 审查和批准。&lt;/p&gt;

&lt;!-- ### Language labels  --&gt;

&lt;h3 id=&#34;语言标签&#34;&gt;语言标签&lt;/h3&gt;

&lt;!-- Prow automatically applies language labels based on file path. Thanks to SIG Docs contributor [June Yi](https://github.com/kubernetes/test-infra/pull/9835), folks can also manually assign language labels in pull request (PR) comments. For example, when left as a comment on an issue or PR, this command assigns the label `language/ko` (Korean). --&gt;

&lt;p&gt;Prow 根据文件路径自动添加语言标签。感谢 SIG Docs 贡献者 &lt;a href=&#34;https://github.com/kubernetes/test-infra/pull/9835&#34; target=&#34;_blank&#34;&gt;June Yi&lt;/a&gt;，他让人们还可以在 pull request（PR）注释中手动分配语言标签。例如，当为 issue 或 PR 留下下述注释时，将为之分配标签 &lt;code&gt;language/ko&lt;/code&gt;（Korean）。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/language ko
&lt;/code&gt;&lt;/pre&gt;

&lt;!-- These repo labels let reviewers filter for PRs and issues by language. For example, you can now filter the k/website dashboard for [PRs with Chinese content](https://github.com/kubernetes/website/pulls?utf8=%E2%9C%93&amp;q=is%3Aopen+is%3Apr+label%3Alanguage%2Fzh).   --&gt;

&lt;p&gt;这些存储库标签允许审阅者按语言过滤 PR 和 issue。例如，您现在可以过滤 kubernetes/website 面板中&lt;a href=&#34;https://github.com/kubernetes/website/pulls?utf8=%E2%9C%93&amp;amp;q=is%3Aopen+is%3Apr+label%3Alanguage%2Fzh&#34; target=&#34;_blank&#34;&gt;具有中文内容的 PR&lt;/a&gt;。&lt;/p&gt;

&lt;!-- ### Team review  --&gt;

&lt;h3 id=&#34;团队审核&#34;&gt;团队审核&lt;/h3&gt;

&lt;!-- L10n teams can now review and approve their own PRs. For example, review and approval permissions for English are [assigned in an OWNERS file](https://github.com/kubernetes/website/blob/master/content/en/OWNERS) in the top subfolder for English content.  --&gt;

&lt;p&gt;L10n 团队现在可以审查和批准他们自己的 PR。例如，英语的审核和批准权限在位于用于显示英语内容的顶级子文件夹中的 &lt;a href=&#34;https://github.com/kubernetes/website/blob/master/content/en/OWNERS&#34; target=&#34;_blank&#34;&gt;OWNERS 文件中指定&lt;/a&gt;。&lt;/p&gt;

&lt;!-- Adding `OWNERS` files to subdirectories lets localization teams review and approve changes without requiring a rubber stamp approval from reviewers who may lack fluency. --&gt;

&lt;p&gt;将 &lt;code&gt;OWNERS&lt;/code&gt; 文件添加到子目录可以让本地化团队审查和批准更改，而无需由可能并不擅长该门语言的审阅者进行批准。&lt;/p&gt;

&lt;!-- ## What&#39;s next --&gt;

&lt;h2 id=&#34;下一步是什么&#34;&gt;下一步是什么&lt;/h2&gt;

&lt;!-- We&#39;re looking forward to the [doc sprint in Shanghai](https://kccncchina2018english.sched.com/event/HVb2/contributor-summit-doc-sprint-additional-registration-required) to serve as a resource for the Chinese l10n team. --&gt;

&lt;p&gt;我们期待着&lt;a href=&#34;https://kccncchina2018english.sched.com/event/HVb2/contributor-summit-doc-sprint-additional-registration-required&#34; target=&#34;_blank&#34;&gt;上海的 doc sprint&lt;/a&gt; 能作为中国 l10n 团队的资源。&lt;/p&gt;

&lt;!-- We&#39;re excited to continue supporting the Japanese and Korean l10n teams, who are making excellent progress. --&gt;

&lt;p&gt;我们很高兴继续支持正在取得良好进展的日本和韩国 l10n 队伍。&lt;/p&gt;

&lt;!-- If you&#39;re interested in localizing Kubernetes for your own language or region, check out our [guide to localizing Kubernetes docs](https://kubernetes.io/docs/contribute/localization/) and reach out to a [SIG Docs chair](https://github.com/kubernetes/community/tree/master/sig-docs#leadership) for support. --&gt;

&lt;p&gt;如果您有兴趣将 Kubernetes 本地化为您自己的语言或地区，请查看我们的&lt;a href=&#34;https://kubernetes.io/docs/contribute/localization/&#34; target=&#34;_blank&#34;&gt;本地化 Kubernetes 文档指南&lt;/a&gt;，并联系 &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-docs#leadership&#34; target=&#34;_blank&#34;&gt;SIG Docs 主席团&lt;/a&gt;获取支持。&lt;/p&gt;

&lt;!-- ### Get involved with SIG Docs  --&gt;

&lt;h3 id=&#34;加入sig-docs&#34;&gt;加入SIG Docs&lt;/h3&gt;

&lt;!-- If you&#39;re interested in Kubernetes documentation, come to a SIG Docs [weekly meeting](https://github.com/kubernetes/community/tree/master/sig-docs#meetings), or join [#sig-docs in Kubernetes Slack](https://kubernetes.slack.com/messages/C1J0BPD2M/details/). --&gt;

&lt;p&gt;如果您对 Kubernetes 文档感兴趣，请参加 SIG Docs &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-docs#meetings&#34; target=&#34;_blank&#34;&gt;每周会议&lt;/a&gt;，或在 &lt;a href=&#34;https://kubernetes.slack.com/messages/C1J0BPD2M/details/&#34; target=&#34;_blank&#34;&gt;Kubernetes Slack 加入 #sig-docs&lt;/a&gt;。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes 2018 年北美贡献者峰会</title>
      <link>https://kubernetes.io/zh/blog/2018/10/16/kubernetes-2018-%E5%B9%B4%E5%8C%97%E7%BE%8E%E8%B4%A1%E7%8C%AE%E8%80%85%E5%B3%B0%E4%BC%9A/</link>
      <pubDate>Tue, 16 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/zh/blog/2018/10/16/kubernetes-2018-%E5%B9%B4%E5%8C%97%E7%BE%8E%E8%B4%A1%E7%8C%AE%E8%80%85%E5%B3%B0%E4%BC%9A/</guid>
      <description>
        
        
        

&lt;!--
---
layout: &#34;Blog&#34;
title: &#34;Kubernetes 2018 North American Contributor Summit&#34;
date: 2018-10-16    
---
--&gt;

&lt;!--
**Authors:**
--&gt;

&lt;p&gt;&lt;strong&gt;作者：&lt;/strong&gt;
&lt;!--
[Bob Killen][bob] (University of Michigan)
[Sahdev Zala][sahdev] (IBM),
[Ihor Dvoretskyi][ihor] (CNCF) 
--&gt;
&lt;a href=&#34;https://twitter.com/mrbobbytables&#34; target=&#34;_blank&#34;&gt;Bob Killen&lt;/a&gt;（密歇根大学）
&lt;a href=&#34;https://twitter.com/sp_zala&#34; target=&#34;_blank&#34;&gt;Sahdev Zala&lt;/a&gt;（IBM），
&lt;a href=&#34;https://twitter.com/idvoretskyi&#34; target=&#34;_blank&#34;&gt;Ihor Dvoretskyi&lt;/a&gt;（CNCF）&lt;/p&gt;

&lt;!--
The 2018 North American Kubernetes Contributor Summit to be hosted right before
[KubeCon + CloudNativeCon][kubecon] Seattle is shaping up to be the largest yet.
--&gt;

&lt;p&gt;2018 年北美 Kubernetes 贡献者峰会将在西雅图 &lt;a href=&#34;https://events.linuxfoundation.org/events/kubecon-cloudnativecon-north-america-2018/&#34; target=&#34;_blank&#34;&gt;KubeCon + CloudNativeCon&lt;/a&gt; 会议之前举办，这将是迄今为止规模最大的一次盛会。
&lt;!--
It is an event that brings together new and current contributors alike to
connect and share face-to-face; and serves as an opportunity for existing
contributors to help shape the future of community development. For new
community members, it offers a welcoming space to learn, explore and put the
contributor workflow to practice.
--&gt;
这是一个将新老贡献者聚集在一起，面对面交流和分享的活动；并为现有的贡献者提供一个机会，帮助塑造社区发展的未来。它为新的社区成员提供了一个学习、探索和实践贡献工作流程的良好空间。&lt;/p&gt;

&lt;!--
Unlike previous Contributor Summits, the event now spans two-days with a more
relaxed ‘hallway’ track and general Contributor get-together to be hosted from
5-8pm on Sunday December 9th at the [Garage Lounge and Gaming Hall][garage], just
a short walk away from the Convention Center. There, contributors can enjoy
billiards, bowling, trivia and more; accompanied by a variety of food and drink. 
--&gt;

&lt;p&gt;与之前的贡献者峰会不同，本次活动为期两天，有一个更为轻松的行程安排，一般贡献者将于 12 月 9 日（周日）下午 5 点至 8 点在距离会议中心仅几步远的 &lt;a href=&#34;https://www.garagebilliards.com/&#34; target=&#34;_blank&#34;&gt;Garage Lounge and Gaming Hall&lt;/a&gt; 举办峰会。在那里，贡献者也可以进行台球、保龄球等娱乐活动，而且还有各种食品和饮料。&lt;/p&gt;

&lt;!--
Things pick up the following day, Monday the 10th with three separate tracks: 
--&gt;

&lt;p&gt;接下来的一天，也就是 10 号星期一，有三个独立的会议你可以选择参与：&lt;/p&gt;

&lt;!--
### New Contributor Workshop:
A half day workshop aimed at getting new and first time contributors onboarded
and comfortable with working within the Kubernetes Community. Staying for the
duration is required; this is not a workshop you can drop into. 
--&gt;

&lt;h3 id=&#34;新贡献者研讨会&#34;&gt;新贡献者研讨会：&lt;/h3&gt;

&lt;p&gt;为期半天的研讨会旨在让新贡献者加入社区，并营造一个良好的 Kubernetes 社区工作环境。
请在开会期间保持在场，该讨论会不允许随意进出。&lt;/p&gt;

&lt;!--
### Current Contributor Track:
Reserved for those that are actively engaged with the development of the
project; the Current Contributor Track includes Talks, Workshops, Birds of a
Feather, Unconferences, Steering Committee Sessions, and more! Keep an eye on
the [schedule in GitHub][schedule] as content is frequently being updated.
--&gt;

&lt;h3 id=&#34;当前贡献者追踪&#34;&gt;当前贡献者追踪：&lt;/h3&gt;

&lt;p&gt;保留给那些积极参与项目开发的贡献者；目前的贡献者追踪包括讲座、研讨会、聚会、Unconferences 会议、指导委员会会议等等!
请留意 &lt;a href=&#34;https://git.k8s.io/community/events/2018/12-contributor-summit#agenda&#34; target=&#34;_blank&#34;&gt;GitHub 中的时间表&lt;/a&gt;，因为内容经常更新。&lt;/p&gt;

&lt;!--
### Docs Sprint:
SIG-Docs will have a curated list of issues and challenges to be tackled closer
to the event date.
--&gt;

&lt;h3 id=&#34;docs-冲刺&#34;&gt;Docs 冲刺：&lt;/h3&gt;

&lt;p&gt;SIG-Docs 将在活动日期临近的时候列出一个需要处理的问题和挑战列表。&lt;/p&gt;

&lt;!--
## To Register:
To register for the Contributor Summit, see the [Registration section of the
Event Details in GitHub][register]. Please note that registrations are being
reviewed. If you select the “Current Contributor Track” and are not an active
contributor, you will be asked to attend the New Contributor Workshop, or asked
to be put on a waitlist. With thousands of contributors and only 300 spots, we
need to make sure the right folks are in the room. 
--&gt;

&lt;h2 id=&#34;注册&#34;&gt;注册：&lt;/h2&gt;

&lt;p&gt;要注册贡献者峰会，请参阅 Git Hub 上的&lt;a href=&#34;https://git.k8s.io/community/events/2018/12-contributor-summit#registration&#34; target=&#34;_blank&#34;&gt;活动详情注册部分&lt;/a&gt;。请注意报名正在审核中。
如果您选择了 “当前贡献者追踪”，而您却不是一个活跃的贡献者，您将被要求参加新贡献者研讨会，或者被要求进入候补名单。
成千上万的贡献者只有 300 个位置，我们需要确保正确的人被安排席位。&lt;/p&gt;

&lt;!--
If you have any questions or concerns, please don’t hesitate to reach out to
the Contributor Summit Events Team at community@kubernetes.io.
--&gt;

&lt;p&gt;如果您有任何问题或疑虑，请随时通过 community@kubernetes.io 联系贡献者峰会组织团队。&lt;/p&gt;

&lt;!--
Look forward to seeing everyone there!
--&gt;

&lt;p&gt;期待在那里看到每个人！&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: 2018 年督导委员会选举结果</title>
      <link>https://kubernetes.io/zh/blog/2018/10/15/2018-%E5%B9%B4%E7%9D%A3%E5%AF%BC%E5%A7%94%E5%91%98%E4%BC%9A%E9%80%89%E4%B8%BE%E7%BB%93%E6%9E%9C/</link>
      <pubDate>Mon, 15 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/zh/blog/2018/10/15/2018-%E5%B9%B4%E7%9D%A3%E5%AF%BC%E5%A7%94%E5%91%98%E4%BC%9A%E9%80%89%E4%B8%BE%E7%BB%93%E6%9E%9C/</guid>
      <description>
        
        
        

&lt;!--
---
layout: blog
title: &#39;2018 Steering Committee Election Results&#39;
date: 2018-10-15
---
--&gt;

&lt;!-- **Authors**: Jorge Castro (Heptio), Ihor Dvoretskyi (CNCF), Paris Pittman (Google) --&gt;

&lt;p&gt;&lt;strong&gt;作者&lt;/strong&gt;: Jorge Castro (Heptio), Ihor Dvoretskyi (CNCF), Paris Pittman (Google)&lt;/p&gt;

&lt;!--
## Results
--&gt;

&lt;h2 id=&#34;结果&#34;&gt;结果&lt;/h2&gt;

&lt;!--
The [Kubernetes Steering Committee Election](https://kubernetes.io/blog/2018/09/06/2018-steering-committee-election-cycle-kicks-off/) is now complete and the following candidates came ahead to secure two year terms that start immediately:
--&gt;

&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/blog/2018/09/06/2018-steering-committee-election-cycle-kicks-off/&#34; target=&#34;_blank&#34;&gt;Kubernetes 督导委员会选举&lt;/a&gt;现已完成，以下候选人获得了立即开始的两年任期：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Aaron Crickenberger, Google, &lt;a href=&#34;https://github.com/spiffxp&#34; target=&#34;_blank&#34;&gt;@spiffxp&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Davanum Srinivas, Huawei, &lt;a href=&#34;https://github.com/dims&#34; target=&#34;_blank&#34;&gt;@dims&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Tim St. Clair, Heptio, &lt;a href=&#34;https://github.com/timothysc&#34; target=&#34;_blank&#34;&gt;@timothysc&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
## Big Thanks!
--&gt;

&lt;h2 id=&#34;十分感谢&#34;&gt;十分感谢！&lt;/h2&gt;

&lt;!-- 
* Steering Committee Member Emeritus [Quinton Hoole](https://github.com/quinton-hoole) for his service to the community over the past year. We look forward to
* The candidates that came forward to run for election. May we always have a strong set of people who want to push community forward like yours in every election.
* All 307 voters who cast a ballot.
* And last but not least...Cornell University for hosting [CIVS](https://civs.cs.cornell.edu/)! 
--&gt;

&lt;ul&gt;
&lt;li&gt;督导委员会荣誉退休成员 &lt;a href=&#34;https://github.com/quinton-hoole&#34; target=&#34;_blank&#34;&gt;Quinton Hoole&lt;/a&gt;，表扬他在过去一年为社区所作的贡献。我们期待着&lt;/li&gt;
&lt;li&gt;参加竞选的候选人。愿我们永远拥有一群强大的人，他们希望在每一次选举中都能像你们一样推动社区向前发展。&lt;/li&gt;
&lt;li&gt;共计 307 名选民参与投票。&lt;/li&gt;
&lt;li&gt;本次选举由康奈尔大学主办 &lt;a href=&#34;https://civs.cs.cornell.edu/&#34; target=&#34;_blank&#34;&gt;CIVS&lt;/a&gt;！&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
## Get Involved with the Steering Committee
--&gt;

&lt;h2 id=&#34;加入督导委员会&#34;&gt;加入督导委员会&lt;/h2&gt;

&lt;!--
You can follow along to Steering Committee [backlog items](https://git.k8s.io/steering/backlog.md) and weigh in by filing an issue or creating a PR against their [repo](https://github.com/kubernetes/steering). They meet bi-weekly on [Wednesdays at 8pm UTC](https://github.com/kubernetes/steering) and regularly attend Meet Our Contributors.
--&gt;

&lt;p&gt;你可以关注督导委员会的&lt;a href=&#34;https://git.k8s.io/steering/backlog.md&#34; target=&#34;_blank&#34;&gt;任务清单&lt;/a&gt;，并通过向他们的&lt;a href=&#34;https://github.com/kubernetes/steering&#34; target=&#34;_blank&#34;&gt;代码仓库&lt;/a&gt;提交 issue 或 PR 的方式来参与。他们也会在&lt;a href=&#34;https://github.com/kubernetes/steering&#34; target=&#34;_blank&#34;&gt;UTC 时间每周三晚 8 点&lt;/a&gt;举行会议，并定期与我们的贡献者见面。&lt;/p&gt;

&lt;!--
Steering Committee Meetings:
--&gt;

&lt;p&gt;督导委员会会议：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PL69nYSiGNLP1yP1B_nd9-drjoxp0Q14qM&#34; target=&#34;_blank&#34;&gt;YouTube 播放列表&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
Meet Our Contributors Steering AMA’s: 
--&gt;

&lt;p&gt;与我们的贡献者会面：&lt;/p&gt;

&lt;!--
* [Oct  3 2018](https://youtu.be/x6Jm8p0K-IQ)
* [Sept 5 2018](https://youtu.be/UbxWV12Or58)
--&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://youtu.be/x6Jm8p0K-IQ&#34; target=&#34;_blank&#34;&gt;2018 年 10 月 3 日&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://youtu.be/UbxWV12Or58&#34; target=&#34;_blank&#34;&gt;2018 年 7 月 5 日&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes 中的拓扑感知数据卷供应</title>
      <link>https://kubernetes.io/zh/blog/2018/10/11/kubernetes-%E4%B8%AD%E7%9A%84%E6%8B%93%E6%89%91%E6%84%9F%E7%9F%A5%E6%95%B0%E6%8D%AE%E5%8D%B7%E4%BE%9B%E5%BA%94/</link>
      <pubDate>Thu, 11 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/zh/blog/2018/10/11/kubernetes-%E4%B8%AD%E7%9A%84%E6%8B%93%E6%89%91%E6%84%9F%E7%9F%A5%E6%95%B0%E6%8D%AE%E5%8D%B7%E4%BE%9B%E5%BA%94/</guid>
      <description>
        
        
        

&lt;!--
---
layout: blog
title: &#39;Topology-Aware Volume Provisioning in Kubernetes&#39;
date: 2018-10-11
---
--&gt;

&lt;!--
**Author**: Michelle Au (Google)
--&gt;

&lt;p&gt;&lt;strong&gt;作者&lt;/strong&gt;: Michelle Au（谷歌）&lt;/p&gt;

&lt;!--
The multi-zone cluster experience with persistent volumes is improving in Kubernetes 1.12 with the topology-aware dynamic provisioning beta feature. This feature allows Kubernetes to make intelligent decisions when dynamically provisioning volumes by getting scheduler input on the best place to provision a volume for a pod.  In multi-zone clusters, this means that volumes will get provisioned in an appropriate zone that can run your pod, allowing you to easily deploy and scale your stateful workloads across failure domains to provide high availability and fault tolerance.
--&gt;

&lt;p&gt;通过提供拓扑感知动态卷供应功能，具有持久卷的多区域集群体验在 Kubernetes 1.12 中得到了改进。此功能使得 Kubernetes 在动态供应卷时能做出明智的决策，方法是从调度器获得为 Pod 提供数据卷的最佳位置。在多区域集群环境，这意味着数据卷能够在满足你的 Pod 运行需要的合适的区域被供应，从而允许您跨故障域轻松部署和扩展有状态工作负载，从而提供高可用性和容错能力。&lt;/p&gt;

&lt;!--
## Previous challenges
--&gt;

&lt;h2 id=&#34;以前的挑战&#34;&gt;以前的挑战&lt;/h2&gt;

&lt;!--
Before this feature, running stateful workloads with zonal persistent disks (such as AWS ElasticBlockStore, Azure Disk, GCE PersistentDisk) in multi-zone clusters had many challenges. Dynamic provisioning was handled independently from pod scheduling, which meant that as soon as you created a PersistentVolumeClaim (PVC), a volume would get provisioned. This meant that the provisioner had no knowledge of what pods were using the volume, and any pod constraints it had that could impact scheduling.
--&gt;

&lt;p&gt;在此功能被提供之前，在多区域集群中使用区域化的持久磁盘（例如 AWS ElasticBlockStore，Azure Disk，GCE PersistentDisk）运行有状态工作负载存在许多挑战。动态供应独立于 Pod 调度处理，这意味着只要您创建了一个 PersistentVolumeClaim（PVC），一个卷就会被供应。这也意味着供应者不知道哪些 Pod 正在使用该卷，也不清楚任何可能影响调度的 Pod 约束。&lt;/p&gt;

&lt;!--
This resulted in unschedulable pods because volumes were provisioned in zones that:
--&gt;

&lt;p&gt;这导致了不可调度的 Pod，因为在以下区域中配置了卷：&lt;/p&gt;

&lt;!--
* did not have enough CPU or memory resources to run the pod
* conflicted with node selectors, pod affinity or anti-affinity policies
* could not run the pod due to taints
--&gt;

&lt;ul&gt;
&lt;li&gt;没有足够的 CPU 或内存资源来运行 Pod&lt;/li&gt;
&lt;li&gt;与节点选择器、Pod 亲和或反亲和策略冲突&lt;/li&gt;
&lt;li&gt;由于污点（taint）不能运行 Pod&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
Another common issue was that a non-StatefulSet pod using multiple persistent volumes could have each volume provisioned in a different zone, again resulting in an unschedulable pod.
--&gt;

&lt;p&gt;另一个常见问题是，使用多个持久卷的非有状态 Pod 可能会在不同的区域中配置每个卷，从而导致一个不可调度的 Pod。&lt;/p&gt;

&lt;!--
Suboptimal workarounds included overprovisioning of nodes, or manual creation of volumes in the correct zones, making it difficult to dynamically deploy and scale stateful workloads.
--&gt;

&lt;p&gt;次优的解决方法包括节点超配，或在正确的区域中手动创建卷，但这会造成难以动态部署和扩展有状态工作负载的问题。&lt;/p&gt;

&lt;!--
The topology-aware dynamic provisioning feature addresses all of the above issues.
--&gt;

&lt;p&gt;拓扑感知动态供应功能解决了上述所有问题。&lt;/p&gt;

&lt;!--
## Supported Volume Types
--&gt;

&lt;h2 id=&#34;支持的卷类型&#34;&gt;支持的卷类型&lt;/h2&gt;

&lt;!--
In 1.12, the following drivers support topology-aware dynamic provisioning:
--&gt;

&lt;p&gt;在 1.12 中，以下驱动程序支持拓扑感知动态供应：&lt;/p&gt;

&lt;!--
* AWS EBS
* Azure Disk
* GCE PD (including Regional PD)
* CSI (alpha) - currently only the GCE PD CSI driver has implemented topology support
--&gt;

&lt;ul&gt;
&lt;li&gt;AWS EBS&lt;/li&gt;
&lt;li&gt;Azure Disk&lt;/li&gt;
&lt;li&gt;GCE PD （包括 Regional PD）&lt;/li&gt;
&lt;li&gt;CSI（alpha） - 目前只有 GCE PD CSI 驱动实现了拓扑支持&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
## Design Principles
--&gt;

&lt;h2 id=&#34;设计原则&#34;&gt;设计原则&lt;/h2&gt;

&lt;!--
While the initial set of supported plugins are all zonal-based, we designed this feature to adhere to the Kubernetes principle of portability across environments. Topology specification is generalized and uses a similar label-based specification like in Pod nodeSelectors and nodeAffinity. This mechanism allows you to define your own topology boundaries, such as racks in on-premise clusters, without requiring modifications to the scheduler to understand these custom topologies.
--&gt;

&lt;p&gt;虽然最初支持的插件集都是基于区域的，但我们设计此功能时遵循 Kubernetes 跨环境可移植性的原则。
拓扑规范是通用的，并使用类似于基于标签的规范，如 Pod nodeSelectors 和 nodeAffinity。
该机制允许您定义自己的拓扑边界，例如内部部署集群中的机架，而无需修改调度程序以了解这些自定义拓扑。&lt;/p&gt;

&lt;!--
In addition, the topology information is abstracted away from the pod specification, so a pod does not need knowledge of the underlying storage system’s topology characteristics. This means that you can use the same pod specification across multiple clusters, environments, and storage systems.
--&gt;

&lt;p&gt;此外，拓扑信息是从 Pod 规范中抽象出来的，因此 Pod 不需要了解底层存储系统的拓扑特征。
这意味着您可以在多个集群、环境和存储系统中使用相同的 Pod 规范。&lt;/p&gt;

&lt;!--
## Getting Started
--&gt;

&lt;h2 id=&#34;入门&#34;&gt;入门&lt;/h2&gt;

&lt;!--
To enable this feature, all you need to do is to create a StorageClass with `volumeBindingMode` set to `WaitForFirstConsumer`:
--&gt;

&lt;p&gt;要启用此功能，您需要做的就是创建一个将 &lt;code&gt;volumeBindingMode&lt;/code&gt; 设置为 &lt;code&gt;WaitForFirstConsumer&lt;/code&gt; 的 StorageClass：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: topology-aware-standard
provisioner: kubernetes.io/gce-pd
volumeBindingMode: WaitForFirstConsumer
parameters:
  type: pd-standard
&lt;/code&gt;&lt;/pre&gt;

&lt;!--
This new setting instructs the volume provisioner to not create a volume immediately, and instead, wait for a pod using an associated PVC to run through scheduling. Note that previous StorageClass `zone` and `zones` parameters do not need to be specified anymore, as pod policies now drive the decision of which zone to provision a volume in.
--&gt;

&lt;p&gt;这个新设置表明卷配置器不立即创建卷，而是等待使用关联的 PVC 的 Pod 通过调度运行。
请注意，不再需要指定以前的 StorageClass &lt;code&gt;zone&lt;/code&gt; 和 &lt;code&gt;zones&lt;/code&gt; 参数，因为现在在哪个区域中配置卷由 Pod 策略决定。&lt;/p&gt;

&lt;!--
Next, create a pod and PVC with this StorageClass. This sequence is the same as before, but with a different StorageClass specified in the PVC. The following is a hypothetical example, demonstrating the capabilities of the new feature by specifying many pod constraints and scheduling policies:
--&gt;

&lt;p&gt;接下来，使用此 StorageClass 创建一个 Pod 和 PVC。
此过程与之前相同，但在 PVC 中指定了不同的 StorageClass。
以下是一个假设示例，通过指定许多 Pod 约束和调度策略来演示新功能特性：&lt;/p&gt;

&lt;!--
* multiple PVCs in a pod
* nodeAffinity across a subset of zones
* pod anti-affinity on zones
--&gt;

&lt;ul&gt;
&lt;li&gt;一个 Pod 多个 PVC&lt;/li&gt;
&lt;li&gt;跨子区域的节点亲和&lt;/li&gt;

&lt;li&gt;&lt;p&gt;同一区域 Pod 反亲和&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: StatefulSet
metadata:
name: web
spec:   
serviceName: &amp;quot;nginx&amp;quot;
replicas: 2
selector:
matchLabels:
  app: nginx
template:
metadata:
  labels:
    app: nginx
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: failure-domain.beta.kubernetes.io/zone
            operator: In
            values:
            - us-central1-a
            - us-central1-f
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - nginx
        topologyKey: failure-domain.beta.kubernetes.io/zone
  containers:
  - name: nginx
    image: gcr.io/google_containers/nginx-slim:0.8
    ports:
    - containerPort: 80
      name: web
    volumeMounts:
    - name: www
      mountPath: /usr/share/nginx/html
    - name: logs
      mountPath: /logs
volumeClaimTemplates:
- metadata:
  name: www
spec:
  accessModes: [ &amp;quot;ReadWriteOnce&amp;quot; ]
  storageClassName: topology-aware-standard
  resources:
    requests:
      storage: 10Gi
- metadata:
  name: logs
spec:
  accessModes: [ &amp;quot;ReadWriteOnce&amp;quot; ]
  storageClassName: topology-aware-standard
  resources:
    requests:
      storage: 1Gi
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
Afterwards, you can see that the volumes were provisioned in zones according to the policies set by the pod:
--&gt;

&lt;p&gt;之后，您可以看到根据 Pod 设置的策略在区域中配置卷：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pv -o=jsonpath=&#39;{range .items[*]}{.spec.claimRef.name}{&amp;quot;\t&amp;quot;}{.metadata.labels.failure\-domain\.beta\.kubernetes\.io/zone}{&amp;quot;\n&amp;quot;}{end}&#39;
www-web-0       us-central1-f
logs-web-0      us-central1-f
www-web-1       us-central1-a
logs-web-1      us-central1-a
&lt;/code&gt;&lt;/pre&gt;

&lt;!--
## How can I learn more?
--&gt;

&lt;h2 id=&#34;我怎样才能了解更多&#34;&gt;我怎样才能了解更多？&lt;/h2&gt;

&lt;!--
Official documentation on the topology-aware dynamic provisioning feature is available here:https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode
--&gt;

&lt;p&gt;有关拓扑感知动态供应功能的官方文档可在此处获取：&lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode&#34; target=&#34;_blank&#34;&gt;https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode&lt;/a&gt;&lt;/p&gt;

&lt;!--
Documentation for CSI drivers is available at https://kubernetes-csi.github.io/docs/
--&gt;

&lt;p&gt;有关 CSI 驱动程序的文档，请访问：&lt;a href=&#34;https://kubernetes-csi.github.io/docs/&#34; target=&#34;_blank&#34;&gt;https://kubernetes-csi.github.io/docs/&lt;/a&gt;&lt;/p&gt;

&lt;!--
## What’s next?
--&gt;

&lt;h2 id=&#34;下一步是什么&#34;&gt;下一步是什么？&lt;/h2&gt;

&lt;!--
We are actively working on improving this feature to support:
--&gt;

&lt;p&gt;我们正积极致力于改进此功能以支持：&lt;/p&gt;

&lt;!--
* more volume types, including dynamic provisioning for local volumes
* dynamic volume attachable count and capacity limits per node
--&gt;

&lt;ul&gt;
&lt;li&gt;更多卷类型，包括本地卷的动态供应&lt;/li&gt;
&lt;li&gt;动态容量可附加计数和每个节点的容量限制&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
## How do I get involved?
--&gt;

&lt;h2 id=&#34;我如何参与&#34;&gt;我如何参与？&lt;/h2&gt;

&lt;!--
If you have feedback for this feature or are interested in getting involved with the design and development, join the [Kubernetes Storage Special-Interest-Group](https://github.com/kubernetes/community/tree/master/sig-storage) (SIG). We’re rapidly growing and always welcome new contributors.
--&gt;

&lt;p&gt;如果您对此功能有反馈意见或有兴趣参与设计和开发，请加入 &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-storage&#34; target=&#34;_blank&#34;&gt;Kubernetes 存储特别兴趣小组&lt;/a&gt;（SIG）。我们正在快速成长，并始终欢迎新的贡献者。&lt;/p&gt;

&lt;!--
Special thanks to all the contributors that helped bring this feature to beta, including Cheng Xing ([verult](https://github.com/verult)), Chuqiang Li ([lichuqiang](https://github.com/lichuqiang)), David Zhu ([davidz627](https://github.com/davidz627)), Deep Debroy ([ddebroy](https://github.com/ddebroy)), Jan Šafránek ([jsafrane](https://github.com/jsafrane)), Jordan Liggitt ([liggitt](https://github.com/liggitt)), Michelle Au ([msau42](https://github.com/msau42)), Pengfei Ni ([feiskyer](https://github.com/feiskyer)), Saad Ali ([saad-ali](https://github.com/saad-ali)), Tim Hockin ([thockin](https://github.com/thockin)), and Yecheng Fu ([cofyc](https://github.com/cofyc)).
--&gt;

&lt;p&gt;特别感谢帮助推出此功能的所有贡献者，包括 Cheng Xing (&lt;a href=&#34;https://github.com/verult&#34; target=&#34;_blank&#34;&gt;verult&lt;/a&gt;)、Chuqiang Li (&lt;a href=&#34;https://github.com/lichuqiang&#34; target=&#34;_blank&#34;&gt;lichuqiang&lt;/a&gt;)、David Zhu (&lt;a href=&#34;https://github.com/davidz627&#34; target=&#34;_blank&#34;&gt;davidz627&lt;/a&gt;)、Deep Debroy (&lt;a href=&#34;https://github.com/ddebroy&#34; target=&#34;_blank&#34;&gt;ddebroy&lt;/a&gt;)、Jan Šafránek (&lt;a href=&#34;https://github.com/jsafrane&#34; target=&#34;_blank&#34;&gt;jsafrane&lt;/a&gt;)、Jordan Liggitt (&lt;a href=&#34;https://github.com/liggitt&#34; target=&#34;_blank&#34;&gt;liggitt&lt;/a&gt;)、Michelle Au (&lt;a href=&#34;https://github.com/msau42&#34; target=&#34;_blank&#34;&gt;msau42&lt;/a&gt;)、Pengfei Ni (&lt;a href=&#34;https://github.com/feiskyer&#34; target=&#34;_blank&#34;&gt;feiskyer&lt;/a&gt;)、Saad Ali (&lt;a href=&#34;https://github.com/saad-ali&#34; target=&#34;_blank&#34;&gt;saad-ali&lt;/a&gt;)、Tim Hockin (&lt;a href=&#34;https://github.com/thockin&#34; target=&#34;_blank&#34;&gt;thockin&lt;/a&gt;)，以及 Yecheng Fu (&lt;a href=&#34;https://github.com/cofyc&#34; target=&#34;_blank&#34;&gt;cofyc&lt;/a&gt;)。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: KubeDirector：在 Kubernetes 上运行复杂状态应用程序的简单方法</title>
      <link>https://kubernetes.io/zh/blog/2018/10/03/kubedirector%E5%9C%A8-kubernetes-%E4%B8%8A%E8%BF%90%E8%A1%8C%E5%A4%8D%E6%9D%82%E7%8A%B6%E6%80%81%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E7%9A%84%E7%AE%80%E5%8D%95%E6%96%B9%E6%B3%95/</link>
      <pubDate>Wed, 03 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/zh/blog/2018/10/03/kubedirector%E5%9C%A8-kubernetes-%E4%B8%8A%E8%BF%90%E8%A1%8C%E5%A4%8D%E6%9D%82%E7%8A%B6%E6%80%81%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E7%9A%84%E7%AE%80%E5%8D%95%E6%96%B9%E6%B3%95/</guid>
      <description>
        
        
        

&lt;!--
layout: blog
title: &#39;KubeDirector: The easy way to run complex stateful applications on Kubernetes&#39;
date: 2018-10-03
--&gt;

&lt;!--
**Author**: Thomas Phelan (BlueData)
--&gt;

&lt;p&gt;&lt;strong&gt;作者&lt;/strong&gt;：Thomas Phelan（BlueData）&lt;/p&gt;

&lt;!--
KubeDirector is an open source project designed to make it easy to run complex stateful scale-out application clusters on Kubernetes. KubeDirector is built using the custom resource definition (CRD) framework and leverages the native Kubernetes API extensions and design philosophy. This enables transparent integration with Kubernetes user/resource management as well as existing clients and tools.
--&gt;

&lt;p&gt;KubeDirector 是一个开源项目，旨在简化在 Kubernetes 上运行复杂的有状态扩展应用程序集群。KubeDirector 使用自定义资源定义（CRD）
框架构建，并利用了本地 Kubernetes API 扩展和设计哲学。这支持与 Kubernetes 用户/资源 管理以及现有客户端和工具的透明集成。&lt;/p&gt;

&lt;!--
We recently [introduced the KubeDirector project](https://medium.com/@thomas_phelan/operation-stateful-introducing-bluek8s-and-kubernetes-director-aa204952f619/), as part of a broader open source Kubernetes initiative we call BlueK8s. I’m happy to announce that the pre-alpha
code for [KubeDirector](https://github.com/bluek8s/kubedirector/) is now available. And in this blog post, I’ll show how it works.
--&gt;

&lt;p&gt;我们最近&lt;a href=&#34;https://medium.com/@thomas_phelan/operation-stateful-introducing-bluek8s-and-kubernetes-director-aa204952f619/&#34; target=&#34;_blank&#34;&gt;介绍了 KubeDirector 项目&lt;/a&gt;，作为我们称为 BlueK8s 的更广泛的 Kubernetes 开源项目的一部分。我很高兴地宣布 &lt;a href=&#34;https://github.com/bluek8s/kubedirector/&#34; target=&#34;_blank&#34;&gt;KubeDirector&lt;/a&gt; 的
pre-alpha 代码现在已经可用。在这篇博客文章中，我将展示它是如何工作的。&lt;/p&gt;

&lt;!--
KubeDirector provides the following capabilities:
--&gt;

&lt;p&gt;KubeDirector 提供以下功能：&lt;/p&gt;

&lt;!--
*   The ability to run non-cloud native stateful applications on Kubernetes without modifying the code. In other words, it’s not necessary to decompose these existing applications to fit a microservices design pattern.
*   Native support for preserving application-specific configuration and state.
*   An application-agnostic deployment pattern, minimizing the time to onboard new stateful applications to Kubernetes.
--&gt;

&lt;ul&gt;
&lt;li&gt;无需修改代码即可在 Kubernetes 上运行非云原生有状态应用程序。换句话说，不需要分解这些现有的应用程序来适应微服务设计模式。&lt;/li&gt;
&lt;li&gt;本机支持保存特定于应用程序的配置和状态。&lt;/li&gt;
&lt;li&gt;与应用程序无关的部署模式，最大限度地减少将新的有状态应用程序装载到 Kubernetes 的时间。&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
KubeDirector enables data scientists familiar with data-intensive distributed applications such as Hadoop, Spark, Cassandra, TensorFlow, Caffe2, etc. to run these applications on Kubernetes -- with a minimal learning curve and no need to write GO code. The applications controlled by KubeDirector are defined by some basic metadata and an associated package of configuration artifacts.  The application metadata is referred to as a KubeDirectorApp resource.
--&gt;

&lt;p&gt;KubeDirector 使熟悉数据密集型分布式应用程序（如 Hadoop、Spark、Cassandra、TensorFlow、Caffe2 等）的数据科学家能够在 Kubernetes 上运行这些应用程序 &amp;ndash; 只需极少的学习曲线，无需编写 GO 代码。由 KubeDirector 控制的应用程序由一些基本元数据和相关的配置工件包定义。应用程序元数据称为 KubeDirectorApp 资源。&lt;/p&gt;

&lt;!--
To understand the components of KubeDirector, clone the repository on [GitHub](https://github.com/bluek8s/kubedirector/) using a command similar to:
--&gt;

&lt;p&gt;要了解 KubeDirector 的组件，请使用类似于以下的命令在 &lt;a href=&#34;https://github.com/bluek8s/kubedirector/&#34; target=&#34;_blank&#34;&gt;GitHub&lt;/a&gt; 上克隆存储库：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone http://&amp;lt;userid&amp;gt;@github.com/bluek8s/kubedirector.
&lt;/code&gt;&lt;/pre&gt;

&lt;!--
The KubeDirectorApp definition for the Spark 2.2.1 application is located
in the file `kubedirector/deploy/example_catalog/cr-app-spark221e2.json`.
--&gt;

&lt;p&gt;Spark 2.2.1 应用程序的 KubeDirectorApp 定义位于文件 &lt;code&gt;kubedirector/deploy/example_catalog/cr-app-spark221e2.json&lt;/code&gt; 中。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; ~&amp;gt; cat kubedirector/deploy/example_catalog/cr-app-spark221e2.json
 {
    &amp;quot;apiVersion&amp;quot;: &amp;quot;kubedirector.bluedata.io/v1alpha1&amp;quot;,
    &amp;quot;kind&amp;quot;: &amp;quot;KubeDirectorApp&amp;quot;,
    &amp;quot;metadata&amp;quot;: {
        &amp;quot;name&amp;quot; : &amp;quot;spark221e2&amp;quot;
    },
    &amp;quot;spec&amp;quot; : {
        &amp;quot;systemctlMounts&amp;quot;: true,
        &amp;quot;config&amp;quot;: {
            &amp;quot;node_services&amp;quot;: [
                {
                    &amp;quot;service_ids&amp;quot;: [
                        &amp;quot;ssh&amp;quot;,
                        &amp;quot;spark&amp;quot;,
                        &amp;quot;spark_master&amp;quot;,
                        &amp;quot;spark_worker&amp;quot;
                    ],
…
&lt;/code&gt;&lt;/pre&gt;

&lt;!--
The configuration of an application cluster is referred to as a KubeDirectorCluster resource. The
KubeDirectorCluster definition for a sample Spark 2.2.1 cluster is located in the file
`kubedirector/deploy/example_clusters/cr-cluster-spark221.e1.yaml`.
--&gt;

&lt;p&gt;应用程序集群的配置称为 KubeDirectorCluster 资源。示例 Spark 2.2.1 集群的 KubeDirectorCluster 定义位于文件
&lt;code&gt;kubedirector/deploy/example_clusters/cr-cluster-spark221.e1.yaml&lt;/code&gt; 中。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;~&amp;gt; cat kubedirector/deploy/example_clusters/cr-cluster-spark221.e1.yaml
apiVersion: &amp;quot;kubedirector.bluedata.io/v1alpha1&amp;quot;
kind: &amp;quot;KubeDirectorCluster&amp;quot;
metadata:
  name: &amp;quot;spark221e2&amp;quot;
spec:
  app: spark221e2
  roles:
  - name: controller
    replicas: 1
    resources:
      requests:
        memory: &amp;quot;4Gi&amp;quot;
        cpu: &amp;quot;2&amp;quot;
      limits:
        memory: &amp;quot;4Gi&amp;quot;
        cpu: &amp;quot;2&amp;quot;
  - name: worker
    replicas: 2
    resources:
      requests:
        memory: &amp;quot;4Gi&amp;quot;
        cpu: &amp;quot;2&amp;quot;
      limits:
        memory: &amp;quot;4Gi&amp;quot;
        cpu: &amp;quot;2&amp;quot;
  - name: jupyter
…
&lt;/code&gt;&lt;/pre&gt;

&lt;!--
## Running Spark on Kubernetes with KubeDirector
--&gt;

&lt;h2 id=&#34;使用-kubedirector-在-kubernetes-上运行-spark&#34;&gt;使用 KubeDirector 在 Kubernetes 上运行 Spark&lt;/h2&gt;

&lt;!--
With KubeDirector, it’s easy to run Spark clusters on Kubernetes.
--&gt;

&lt;p&gt;使用 KubeDirector，可以轻松在 Kubernetes 上运行 Spark 集群。&lt;/p&gt;

&lt;!--
First, verify that Kubernetes (version 1.9 or later) is running, using the command `kubectl version`
--&gt;

&lt;p&gt;首先，使用命令 &lt;code&gt;kubectl version&lt;/code&gt; 验证 Kubernetes（版本 1.9 或更高）是否正在运行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;~&amp;gt; kubectl version
Client Version: version.Info{Major:&amp;quot;1&amp;quot;, Minor:&amp;quot;11&amp;quot;, GitVersion:&amp;quot;v1.11.3&amp;quot;, GitCommit:&amp;quot;a4529464e4629c21224b3d52edfe0ea91b072862&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;, BuildDate:&amp;quot;2018-09-09T18:02:47Z&amp;quot;, GoVersion:&amp;quot;go1.10.3&amp;quot;, Compiler:&amp;quot;gc&amp;quot;, Platform:&amp;quot;linux/amd64&amp;quot;}
Server Version: version.Info{Major:&amp;quot;1&amp;quot;, Minor:&amp;quot;11&amp;quot;, GitVersion:&amp;quot;v1.11.3&amp;quot;, GitCommit:&amp;quot;a4529464e4629c21224b3d52edfe0ea91b072862&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;, BuildDate:&amp;quot;2018-09-09T17:53:03Z&amp;quot;, GoVersion:&amp;quot;go1.10.3&amp;quot;, Compiler:&amp;quot;gc&amp;quot;, Platform:&amp;quot;linux/amd64&amp;quot;}                                    
&lt;/code&gt;&lt;/pre&gt;

&lt;!--
Deploy the KubeDirector service and the example KubeDirectorApp resource definitions with the commands:
--&gt;

&lt;p&gt;使用以下命令部署 KubeDirector 服务和示例 KubeDirectorApp 资源定义：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd kubedirector
make deploy
&lt;/code&gt;&lt;/pre&gt;

&lt;!--
These will start the KubeDirector pod:
--&gt;

&lt;p&gt;这些将启动 KubeDirector pod：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;~&amp;gt; kubectl get pods
NAME                           READY     STATUS     RESTARTS     AGE
kubedirector-58cf59869-qd9hb   1/1       Running    0            1m     
&lt;/code&gt;&lt;/pre&gt;

&lt;!--
List the installed KubeDirector applications with `kubectl get KubeDirectorApp`
--&gt;

&lt;p&gt;&lt;code&gt;kubectl get KubeDirectorApp&lt;/code&gt; 列出中已安装的 KubeDirector 应用程序&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;~&amp;gt; kubectl get KubeDirectorApp
NAME           AGE
cassandra311   30m
spark211up     30m
spark221e2     30m
&lt;/code&gt;&lt;/pre&gt;

&lt;!--
Now you can launch a Spark 2.2.1 cluster using the example KubeDirectorCluster file and the
`kubectl create -f deploy/example_clusters/cr-cluster-spark211up.yaml` command.
Verify that the Spark cluster has been started:
--&gt;

&lt;p&gt;现在，您可以使用示例 KubeDirectorCluster 文件和 &lt;code&gt;kubectl create -f deploy/example_clusters/cr-cluster-spark211up.yaml&lt;/code&gt; 命令
启动 Spark 2.2.1 集群。验证 Spark 集群已经启动:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;~&amp;gt; kubectl get pods
NAME                             READY     STATUS    RESTARTS   AGE
kubedirector-58cf59869-djdwl     1/1       Running   0          19m
spark221e2-controller-zbg4d-0    1/1       Running   0          23m
spark221e2-jupyter-2km7q-0       1/1       Running   0          23m
spark221e2-worker-4gzbz-0        1/1       Running   0          23m
spark221e2-worker-4gzbz-1        1/1       Running   0          23m
&lt;/code&gt;&lt;/pre&gt;

&lt;!--
The running services now include the Spark services:
--&gt;

&lt;p&gt;现在运行的服务包括 Spark 服务：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;~&amp;gt; kubectl get service
NAME                                TYPE         CLUSTER-IP        EXTERNAL-IP    PORT(S)                                                    AGE
kubedirector                        ClusterIP    10.98.234.194     &amp;lt;none&amp;gt;         60000/TCP                                                  1d
kubernetes                          ClusterIP    10.96.0.1         &amp;lt;none&amp;gt;         443/TCP                                                    1d
svc-spark221e2-5tg48                ClusterIP    None              &amp;lt;none&amp;gt;         8888/TCP                                                   21s
svc-spark221e2-controller-tq8d6-0   NodePort     10.104.181.123    &amp;lt;none&amp;gt;         22:30534/TCP,8080:31533/TCP,7077:32506/TCP,8081:32099/TCP  20s
svc-spark221e2-jupyter-6989v-0      NodePort     10.105.227.249    &amp;lt;none&amp;gt;         22:30632/TCP,8888:30355/TCP                                20s
svc-spark221e2-worker-d9892-0       NodePort     10.107.131.165    &amp;lt;none&amp;gt;         22:30358/TCP,8081:32144/TCP                                20s
svc-spark221e2-worker-d9892-1       NodePort     10.110.88.221     &amp;lt;none&amp;gt;         22:30294/TCP,8081:31436/TCP                                20s
&lt;/code&gt;&lt;/pre&gt;

&lt;!--
Pointing the browser at port 31533 connects to the Spark Master UI:
--&gt;

&lt;p&gt;将浏览器指向端口 31533 连接到 Spark 主节点 UI：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2018-10-03-kubedirector/kubedirector.png&#34; alt=&#34;kubedirector&#34; /&gt;&lt;/p&gt;

&lt;!--
That’s all there is to it!
In fact, in the example above we also deployed a Jupyter notebook along with the Spark cluster.
--&gt;

&lt;p&gt;就是这样!
事实上，在上面的例子中，我们还部署了一个 Jupyter notebook 和 Spark 集群。&lt;/p&gt;

&lt;!--
To start another application (e.g. Cassandra), just specify another KubeDirectorApp file:
--&gt;

&lt;p&gt;要启动另一个应用程序（例如 Cassandra），只需指定另一个 KubeDirectorApp 文件：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl create -f deploy/example_clusters/cr-cluster-cassandra311.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;!--
See the running Cassandra cluster:
--&gt;

&lt;p&gt;查看正在运行的 Cassandra 集群：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;~&amp;gt; kubectl get pods
NAME                              READY     STATUS    RESTARTS   AGE
cassandra311-seed-v24r6-0         1/1       Running   0          1m
cassandra311-seed-v24r6-1         1/1       Running   0          1m
cassandra311-worker-rqrhl-0       1/1       Running   0          1m
cassandra311-worker-rqrhl-1       1/1       Running   0          1m
kubedirector-58cf59869-djdwl      1/1       Running   0          1d
spark221e2-controller-tq8d6-0     1/1       Running   0          22m
spark221e2-jupyter-6989v-0        1/1       Running   0          22m
spark221e2-worker-d9892-0         1/1       Running   0          22m
spark221e2-worker-d9892-1         1/1       Running   0          22m
&lt;/code&gt;&lt;/pre&gt;

&lt;!--
Now you have a Spark cluster (with a Jupyter notebook) and a Cassandra cluster running on Kubernetes.
Use `kubectl get service` to see the set of services.
--&gt;

&lt;p&gt;现在，您有一个 Spark 集群（带有 Jupyter notebook ）和一个运行在 Kubernetes 上的 Cassandra 集群。
使用 &lt;code&gt;kubectl get service&lt;/code&gt; 查看服务集。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;~&amp;gt; kubectl get service
NAME                                TYPE         CLUSTER-IP       EXTERNAL-IP   PORT(S)                                                   AGE
kubedirector                        ClusterIP    10.98.234.194    &amp;lt;none&amp;gt;        60000/TCP                                                 1d
kubernetes                          ClusterIP    10.96.0.1        &amp;lt;none&amp;gt;        443/TCP                                                   1d
svc-cassandra311-seed-v24r6-0       NodePort     10.96.94.204     &amp;lt;none&amp;gt;        22:31131/TCP,9042:30739/TCP                               3m
svc-cassandra311-seed-v24r6-1       NodePort     10.106.144.52    &amp;lt;none&amp;gt;        22:30373/TCP,9042:32662/TCP                               3m
svc-cassandra311-vhh29              ClusterIP    None             &amp;lt;none&amp;gt;        8888/TCP                                                  3m
svc-cassandra311-worker-rqrhl-0     NodePort     10.109.61.194    &amp;lt;none&amp;gt;        22:31832/TCP,9042:31962/TCP                               3m
svc-cassandra311-worker-rqrhl-1     NodePort     10.97.147.131    &amp;lt;none&amp;gt;        22:31454/TCP,9042:31170/TCP                               3m
svc-spark221e2-5tg48                ClusterIP    None             &amp;lt;none&amp;gt;        8888/TCP                                                  24m
svc-spark221e2-controller-tq8d6-0   NodePort     10.104.181.123   &amp;lt;none&amp;gt;        22:30534/TCP,8080:31533/TCP,7077:32506/TCP,8081:32099/TCP 24m
svc-spark221e2-jupyter-6989v-0      NodePort     10.105.227.249   &amp;lt;none&amp;gt;        22:30632/TCP,8888:30355/TCP                               24m
svc-spark221e2-worker-d9892-0       NodePort     10.107.131.165   &amp;lt;none&amp;gt;        22:30358/TCP,8081:32144/TCP                               24m
svc-spark221e2-worker-d9892-1       NodePort     10.110.88.221    &amp;lt;none&amp;gt;        22:30294/TCP,8081:31436/TCP                               24m
&lt;/code&gt;&lt;/pre&gt;

&lt;!--
## Get Involved
--&gt;

&lt;h2 id=&#34;参与其中&#34;&gt;参与其中&lt;/h2&gt;

&lt;!--
KubeDirector is a fully open source, Apache v2 licensed, project – the first of multiple open source projects within a broader initiative we call BlueK8s.
The pre-alpha code for KubeDirector has just been released and we would love for you to join the growing community of developers, contributors, and adopters.
Follow [@BlueK8s](https://twitter.com/BlueK8s/) on Twitter and get involved through these channels:
--&gt;

&lt;p&gt;KubeDirector 是一个完全开放源码的 Apache v2 授权项目 – 在我们称为 BlueK8s 的更广泛的计划中，它是多个开放源码项目中的第一个。
KubeDirector 的 pre-alpha 代码刚刚发布，我们希望您加入到不断增长的开发人员、贡献者和使用者社区。
在 Twitter 上关注 &lt;a href=&#34;https://twitter.com/BlueK8s/&#34; target=&#34;_blank&#34;&gt;@BlueK8s&lt;/a&gt;，并通过以下渠道参与:&lt;/p&gt;

&lt;!--
* KubeDirector [chat room on Slack](https://join.slack.com/t/bluek8s/shared_invite/enQtNDUwMzkwODY5OTM4LTRhYmRmZmE4YzY3OGUzMjA1NDg0MDVhNDQ2MGNkYjRhM2RlMDNjMTI1NDQyMjAzZGVlMDFkNThkNGFjZGZjMGY/)
*   KubeDirector [GitHub repo](https://github.com/bluek8s/kubedirector/)
--&gt;

&lt;ul&gt;
&lt;li&gt;KubeDirector &lt;a href=&#34;https://join.slack.com/t/bluek8s/shared_invite/enQtNDUwMzkwODY5OTM4LTRhYmRmZmE4YzY3OGUzMjA1NDg0MDVhNDQ2MGNkYjRhM2RlMDNjMTI1NDQyMjAzZGVlMDFkNThkNGFjZGZjMGY/&#34; target=&#34;_blank&#34;&gt;Slack 聊天室&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;KubeDirector &lt;a href=&#34;https://github.com/bluek8s/kubedirector/&#34; target=&#34;_blank&#34;&gt;GitHub 仓库&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: The Machines Can Do the Work, a Story of Kubernetes Testing, CI, and Automating the Contributor Experience</title>
      <link>https://kubernetes.io/zh/blog/2018/08/29/the-machines-can-do-the-work-a-story-of-kubernetes-testing-ci-and-automating-the-contributor-experience/</link>
      <pubDate>Wed, 29 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/zh/blog/2018/08/29/the-machines-can-do-the-work-a-story-of-kubernetes-testing-ci-and-automating-the-contributor-experience/</guid>
      <description>
        
        
        

&lt;hr /&gt;

&lt;p&gt;layout: blog
title: &amp;lsquo;机器可以完成这项工作，一个关于 kubernetes 测试、CI 和自动化贡献者体验的故事&amp;rsquo;&lt;/p&gt;

&lt;h2 id=&#34;date-2019-08-29&#34;&gt;date: 2019-08-29&lt;/h2&gt;

&lt;!--
**Author**: Aaron Crickenberger (Google) and Benjamin Elder (Google)
--&gt;

&lt;p&gt;&lt;strong&gt;作者&lt;/strong&gt;：Aaron Crickenberger（谷歌）和 Benjamin Elder（谷歌）&lt;/p&gt;

&lt;!--
_“Large projects have a lot of less exciting, yet, hard work. We value time spent automating repetitive work more highly than toil. Where that work cannot be automated, it is our culture to recognize and reward all types of contributions. However, heroism is not sustainable.”_ - [Kubernetes Community Values](https://git.k8s.io/community/values.md#automation-over-process)
--&gt;

&lt;p&gt;&lt;em&gt;”大型项目有很多不那么令人兴奋，但却很辛苦的工作。比起辛苦工作，我们更重视把时间花在自动化重复性工作上，如果这项工作无法实现自动化，我们的文化就是承认并奖励所有类型的贡献。然而，英雄主义是不可持续的。“&lt;/em&gt; - &lt;a href=&#34;https://git.k8s.io/community/values.md#automation-over-process&#34; target=&#34;_blank&#34;&gt;Kubernetes Community Values&lt;/a&gt;&lt;/p&gt;

&lt;!--
Like many open source projects, Kubernetes is hosted on GitHub. We felt the barrier to participation would be lowest if the project lived where developers already worked, using tools and processes developers already knew. Thus the project embraced the service fully: it was the basis of our workflow, our issue tracker, our documentation, our blog platform, our team structure, and more.
--&gt;

&lt;p&gt;像许多开源项目一样，Kubernetes 托管在 GitHub 上。 如果项目位于在开发人员已经工作的地方，使用的开发人员已经知道的工具和流程，那么参与的障碍将是最低的。 因此，该项目完全接受了这项服务：它是我们工作流程，问题跟踪，文档，博客平台，团队结构等的基础。&lt;/p&gt;

&lt;!--
This strategy worked. It worked so well that the project quickly scaled past its contributors’ capacity as humans. What followed was an incredible journey of automation and innovation. We didn’t just need to rebuild our airplane mid-flight without crashing, we needed to convert it into a rocketship and launch into orbit. We needed machines to do the work.
--&gt;

&lt;p&gt;这个策略奏效了。 它运作良好，以至于该项目迅速超越了其贡献者的人类能力。 接下来是一次令人难以置信的自动化和创新之旅。 我们不仅需要在飞行途中重建我们的飞机而不会崩溃，我们需要将其转换为火箭飞船并发射到轨道。 我们需要机器来完成这项工作。&lt;/p&gt;

&lt;!--
## The Work
--&gt;

&lt;p&gt;##　工作&lt;/p&gt;

&lt;!--
Initially, we focused on the fact that we needed to support the sheer volume of tests mandated by a complex distributed system such as Kubernetes. Real world failure scenarios had to be exercised via end-to-end (e2e) tests to ensure proper functionality. Unfortunately, e2e tests were susceptible to flakes (random failures) and took anywhere from an hour to a day to complete.
--&gt;

&lt;p&gt;最初，我们关注的事实是，我们需要支持复杂的分布式系统（如 Kubernetes）所要求的大量测试。 真实世界中的故障场景必须通过端到端（e2e）测试来执行，确保正确的功能。 不幸的是，e2e 测试容易受到薄片（随机故障）的影响，并且需要花费一个小时到一天才能完成。&lt;/p&gt;

&lt;!--
Further experience revealed other areas where machines could do the work for us:
--&gt;

&lt;p&gt;进一步的经验揭示了机器可以为我们工作的其他领域：&lt;/p&gt;

&lt;!--
* PR Workflow
  * Did the contributor sign our CLA?
  * Did the PR pass tests?
  * Is the PR mergeable?
  * Did the merge commit pass tests?
* Triage
  * Who should be reviewing PRs?
  * Is there enough information to route an issue to the right people?
  * Is an issue still relevant?
* Project Health
  * What is happening in the project?
  * What should we be paying attention to?
  --&gt;

&lt;ul&gt;
&lt;li&gt;Pull Request 工作流程

&lt;ul&gt;
&lt;li&gt;贡献者是否签署了我们的 CLA？&lt;/li&gt;
&lt;li&gt;Pull Request 通过测试吗？&lt;/li&gt;
&lt;li&gt;Pull Request 可以合并吗？&lt;/li&gt;
&lt;li&gt;合并提交是否通过了测试？&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;鉴别分类

&lt;ul&gt;
&lt;li&gt;谁应该审查 Pull Request？&lt;/li&gt;
&lt;li&gt;是否有足够的信息将问题发送给合适的人？&lt;/li&gt;
&lt;li&gt;问题是否依旧存在？&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;项目健康

&lt;ul&gt;
&lt;li&gt;项目中发生了什么？&lt;/li&gt;
&lt;li&gt;我们应该注意什么？&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
As we developed automation to improve our situation, we followed a few guiding principles:
--&gt;

&lt;p&gt;当我们开发自动化来改善我们的情况时，我们遵循了以下几个指导原则：&lt;/p&gt;

&lt;!--
* Follow the push/pull control loop patterns that worked well for Kubernetes
* Prefer stateless loosely coupled services that do one thing well
* Prefer empowering the entire community over empowering a few core contributors
* Eat our own dogfood and avoid reinventing wheels
--&gt;

&lt;ul&gt;
&lt;li&gt;遵循适用于 Kubernetes 的推送/拉取控制循环模式&lt;/li&gt;
&lt;li&gt;首选无状态松散耦合服务&lt;/li&gt;
&lt;li&gt;更倾向于授权整个社区权利，而不是赋予少数核心贡献者权力&lt;/li&gt;
&lt;li&gt;做好自己的事，而不要重新造轮子&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
## Enter Prow
--&gt;

&lt;h2 id=&#34;了解-prow&#34;&gt;了解 Prow&lt;/h2&gt;

&lt;!--
This led us to create [Prow](https://git.k8s.io/test-infra/prow) as the central component for our automation. Prow is sort of like an [If This, Then That](https://ifttt.com/) for GitHub events, with a built-in library of [commands](https://prow.k8s.io/command-help), [plugins](https://prow.k8s.io/plugins), and utilities. We built Prow on top of Kubernetes to free ourselves from worrying about resource management and scheduling, and ensure a more pleasant operational experience.
--&gt;

&lt;p&gt;这促使我们创建 &lt;a href=&#34;https://git.k8s.io/test-infra/prow&#34; target=&#34;_blank&#34;&gt;Prow&lt;/a&gt; 作为我们自动化的核心组件。 Prow有点像 &lt;a href=&#34;https://ifttt.com/&#34; target=&#34;_blank&#34;&gt;If This, Then That&lt;/a&gt; 用于 GitHub 事件， 内置 &lt;a href=&#34;https://prow.k8s.io/command-help&#34; target=&#34;_blank&#34;&gt;commands&lt;/a&gt;， &lt;a href=&#34;https://prow.k8s.io/plugins&#34; target=&#34;_blank&#34;&gt;plugins&lt;/a&gt;， 和实用程序。 我们在  Kubernetes 之上建立了 Prow，让我们不必担心资源管理和日程安排，并确保更愉快的运营体验。&lt;/p&gt;

&lt;!--
Prow lets us do things like:
--&gt;

&lt;p&gt;Prow 让我们做以下事情：&lt;/p&gt;

&lt;!--
* Allow our community to triage issues/PRs by commenting commands such as “/priority critical-urgent”, “/assign mary” or “/close”
* Auto-label PRs based on how much code they change, or which files they touch
* Age out issues/PRs that have remained inactive for too long
* Auto-merge PRs that meet our PR workflow requirements
* Run CI jobs defined as [Knative Builds](https://github.com/knative/build), Kubernetes Pods, or Jenkins jobs
* Enforce org-wide and per-repo GitHub policies like [branch protection](https://github.com/kubernetes/test-infra/tree/master/prow/cmd/branchprotector) and [GitHub labels](https://github.com/kubernetes/test-infra/tree/master/label_sync)
--&gt;

&lt;ul&gt;
&lt;li&gt;允许我们的社区通过评论诸如“/priority critical-urgent”，“/assign mary”或“/close”之类的命令对 issues/Pull Requests 进行分类&lt;/li&gt;
&lt;li&gt;根据用户更改的代码数量或创建的文件自动标记 Pull Requests&lt;/li&gt;
&lt;li&gt;标出长时间保持不活动状态 issues/Pull Requests&lt;/li&gt;
&lt;li&gt;自动合并符合我们PR工作流程要求的 Pull Requests&lt;/li&gt;
&lt;li&gt;运行定义为&lt;a href=&#34;https://github.com/knative/build&#34; target=&#34;_blank&#34;&gt;Knative Builds&lt;/a&gt;的 Kubernetes Pods或 Jenkins jobs的 CI 作业&lt;/li&gt;
&lt;li&gt;实施组织范围和重构 GitHub 仓库策略，如&lt;a href=&#34;https://github.com/kubernetes/test-infra/tree/master/prow/cmd/branchprotector&#34; target=&#34;_blank&#34;&gt;Knative Builds&lt;/a&gt;和&lt;a href=&#34;https://github.com/kubernetes/test-infra/tree/master/label_sync&#34; target=&#34;_blank&#34;&gt;GitHub labels&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
Prow was initially developed by the engineering productivity team building Google Kubernetes Engine, and is actively contributed to by multiple members of Kubernetes SIG Testing. Prow has been adopted by several other open source projects, including Istio, JetStack, Knative and OpenShift. [Getting started with Prow](https://github.com/kubernetes/test-infra/blob/master/prow/getting_started.md) takes a Kubernetes cluster and `kubectl apply starter.yaml` (running pods on a Kubernetes cluster).
--&gt;

&lt;p&gt;Prow最初由构建 Google Kubernetes Engine 的工程效率团队开发，并由 Kubernetes SIG Testing 的多个成员积极贡献。 Prow 已被其他几个开源项目采用，包括 Istio，JetStack，Knative 和 OpenShift。 &lt;a href=&#34;https://github.com/kubernetes/test-infra/blob/master/prow/getting_started.md&#34; target=&#34;_blank&#34;&gt;Getting started with Prow&lt;/a&gt;需要一个 Kubernetes 集群和 &lt;code&gt;kubectl apply starter.yaml&lt;/code&gt;（在 Kubernetes 集群上运行 pod）。&lt;/p&gt;

&lt;!--
Once we had Prow in place, we began to hit other scaling bottlenecks, and so produced additional tooling to support testing at the scale required by Kubernetes, including:
--&gt;

&lt;p&gt;一旦我们安装了 Prow，我们就开始遇到其他的问题，因此需要额外的工具以支持 Kubernetes 所需的规模测试，包括：&lt;/p&gt;

&lt;!--
- [Boskos](https://github.com/kubernetes/test-infra/tree/master/boskos): manages job resources (such as GCP projects) in pools, checking them out for jobs and cleaning them up automatically ([with monitoring](http://velodrome.k8s.io/dashboard/db/boskos-dashboard?orgId=1))
- [ghProxy](https://github.com/kubernetes/test-infra/tree/master/ghproxy): a reverse proxy HTTP cache optimized for use with the GitHub API, to ensure our token usage doesn’t hit API limits ([with monitoring](http://velodrome.k8s.io/dashboard/db/github-cache?refresh=1m&amp;orgId=1))
- [Greenhouse](https://github.com/kubernetes/test-infra/tree/master/greenhouse): allows us to use a remote bazel cache to provide faster build and test results for PRs ([with monitoring](http://velodrome.k8s.io/dashboard/db/bazel-cache?orgId=1))
- [Splice](https://github.com/kubernetes/test-infra/tree/master/prow/cmd/splice): allows us to test and merge PRs in a batch, ensuring our merge velocity is not limited to our test velocity
- [Tide](https://github.com/kubernetes/test-infra/tree/master/prow/cmd/tide): allows us to merge PRs selected via GitHub queries rather than ordered in a queue, allowing for significantly higher merge velocity in tandem with splice
--&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/test-infra/tree/master/boskos&#34; target=&#34;_blank&#34;&gt;Boskos&lt;/a&gt;: 管理池中的作业资源（例如 GCP 项目），检查它们是否有工作并自动清理它们 (&lt;a href=&#34;http://velodrome.k8s.io/dashboard/db/boskos-dashboard?orgId=1&#34; target=&#34;_blank&#34;&gt;with monitoring&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/test-infra/tree/master/ghproxy&#34; target=&#34;_blank&#34;&gt;ghProxy&lt;/a&gt;: 优化用于 GitHub API 的反向代理 HTTP 缓存，以确保我们的令牌使用不会达到 API 限制 (&lt;a href=&#34;http://velodrome.k8s.io/dashboard/db/github-cache?refresh=1m&amp;amp;orgId=1&#34; target=&#34;_blank&#34;&gt;with monitoring&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/test-infra/tree/master/greenhouse&#34; target=&#34;_blank&#34;&gt;Greenhouse&lt;/a&gt;: 允许我们使用远程 bazel 缓存为 Pull requests 提供更快的构建和测试结果 (&lt;a href=&#34;http://velodrome.k8s.io/dashboard/db/bazel-cache?orgId=1&#34; target=&#34;_blank&#34;&gt;with monitoring&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/test-infra/tree/master/prow/cmd/splice&#34; target=&#34;_blank&#34;&gt;Splice&lt;/a&gt;: 允许我们批量测试和合并 Pull requests，确保我们的合并速度不仅限于我们的测试速度&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/test-infra/tree/master/prow/cmd/tide&#34; target=&#34;_blank&#34;&gt;Tide&lt;/a&gt;: 允许我们合并通过 GitHub 查询选择的 Pull requests，而不是在队列中排序，允许显着更高合并速度与拼接一起&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
## Scaling Project Health
--&gt;

&lt;p&gt;##　关注项目健康状况&lt;/p&gt;

&lt;!--
With workflow automation addressed, we turned our attention to project health. We chose to use Google Cloud Storage (GCS) as our source of truth for all test data, allowing us to lean on established infrastructure, and allowed the community to contribute results. We then built a variety of tools to help individuals and the project as a whole make sense of this data, including:
--&gt;

&lt;p&gt;随着工作流自动化的实施，我们将注意力转向了项目健康。我们选择使用 Google Cloud Storage (GCS)作为所有测试数据的真实来源，允许我们依赖已建立的基础设施，并允许社区贡献结果。然后，我们构建了各种工具来帮助个人和整个项目理解这些数据，包括：&lt;/p&gt;

&lt;!--
* [Gubernator](https://github.com/kubernetes/test-infra/tree/master/gubernator): display the results and test history for a given PR
* [Kettle](https://github.com/kubernetes/test-infra/tree/master/kettle): transfer data from GCS to a publicly accessible bigquery dataset
* [PR dashboard](https://k8s-gubernator.appspot.com/pr): a workflow-aware dashboard that allows contributors to understand which PRs require attention and why
* [Triage](https://storage.googleapis.com/k8s-gubernator/triage/index.html): identify common failures that happen across all jobs and tests
* [Testgrid](https://k8s-testgrid.appspot.com/): display test results for a given job across all runs, summarize test results across groups of jobs
--&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/test-infra/tree/master/gubernator&#34; target=&#34;_blank&#34;&gt;Gubernator&lt;/a&gt;: 显示给定 Pull Request 的结果和测试历史&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/test-infra/tree/master/kettle&#34; target=&#34;_blank&#34;&gt;Kettle&lt;/a&gt;: 将数据从 GCS 传输到可公开访问的 bigquery 数据集&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://k8s-gubernator.appspot.com/pr&#34; target=&#34;_blank&#34;&gt;PR dashboard&lt;/a&gt;: 一个工作流程识别仪表板，允许参与者了解哪些 Pull Request 需要注意以及为什么&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://storage.googleapis.com/k8s-gubernator/triage/index.html&#34; target=&#34;_blank&#34;&gt;Triage&lt;/a&gt;: 识别所有作业和测试中发生的常见故障&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://k8s-testgrid.appspot.com/&#34; target=&#34;_blank&#34;&gt;Testgrid&lt;/a&gt;: 显示所有运行中给定作业的测试结果，汇总各组作业的测试结果&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
We approached the Cloud Native Computing Foundation (CNCF) to develop DevStats to glean insights from our GitHub events such as:
--&gt;

&lt;p&gt;我们与云计算本地计算基金会（CNCF）联系，开发 DevStats，以便从我们的 GitHub 活动中收集见解，例如：&lt;/p&gt;

&lt;!--
* [Which prow commands are people most actively using](https://k8s.devstats.cncf.io/d/5/bot-commands-repository-groups?orgId=1)
* [PR reviews by contributor over time](https://k8s.devstats.cncf.io/d/46/pr-reviews-by-contributor?orgId=1&amp;var-period=d7&amp;var-repo_name=All&amp;var-reviewers=All)
* [Time spent in each phase of our PR workflow](https://k8s.devstats.cncf.io/d/44/pr-time-to-approve-and-merge?orgId=1)
--&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://k8s.devstats.cncf.io/d/5/bot-commands-repository-groups?orgId=1&#34; target=&#34;_blank&#34;&gt;Which prow commands are people most actively using&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://k8s.devstats.cncf.io/d/46/pr-reviews-by-contributor?orgId=1&amp;amp;var-period=d7&amp;amp;var-repo_name=All&amp;amp;var-reviewers=All&#34; target=&#34;_blank&#34;&gt;PR reviews by contributor over time&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://k8s.devstats.cncf.io/d/44/pr-time-to-approve-and-merge?orgId=1&#34; target=&#34;_blank&#34;&gt;Time spent in each phase of our PR workflow&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
## Into the Beyond
--&gt;

&lt;h2 id=&#34;into-the-beyond&#34;&gt;Into the Beyond&lt;/h2&gt;

&lt;!--
Today, the Kubernetes project spans over 125 repos across five orgs. There are 31 Special Interests Groups and 10 Working Groups coordinating development within the project. In the last year the project has had [participation from over 13,800 unique developers](https://k8s.devstats.cncf.io/d/13/developer-activity-counts-by-repository-group?orgId=1&amp;var-period_name=Last%20year&amp;var-metric=contributions&amp;var-repogroup_name=All) on GitHub.
--&gt;

&lt;p&gt;今天，Kubernetes 项目跨越了5个组织125个仓库。有31个特殊利益集团和10个工作组在项目内协调发展。在过去的一年里，该项目有 &lt;a href=&#34;https://k8s.devstats.cncf.io/d/13/developer-activity-counts-by-repository-group?orgId=1&amp;amp;var-period_name=Last%20year&amp;amp;var-metric=contributions&amp;amp;var-repogroup_name=All&#34; target=&#34;_blank&#34;&gt;来自13800多名独立开发人员的参与&lt;/a&gt;。&lt;/p&gt;

&lt;!--
On any given weekday our Prow instance [runs over 10,000 CI jobs](http://velodrome.k8s.io/dashboard/db/bigquery-metrics?panelId=10&amp;fullscreen&amp;orgId=1&amp;from=now-6M&amp;to=now); from March 2017 to March 2018 it ran 4.3 million jobs. Most of these jobs involve standing up an entire Kubernetes cluster, and exercising it using real world scenarios. They allow us to ensure all supported releases of Kubernetes work across cloud providers, container engines, and networking plugins. They make sure the latest releases of Kubernetes work with various optional features enabled, upgrade safely, meet performance requirements, and work across architectures.
--&gt;

&lt;p&gt;在任何给定的工作日，我们的 Prow 实例&lt;a href=&#34;http://velodrome.k8s.io/dashboard/db/bigquery-metrics?panelId=10&amp;amp;fullscreen&amp;amp;orgId=1&amp;amp;from=now-6M&amp;amp;to=now&#34; target=&#34;_blank&#34;&gt;运行超过10,000个 CI 工作&lt;/a&gt;; 从2017年3月到2018年3月，它有430万个工作岗位。 这些工作中的大多数涉及建立整个 Kubernetes 集群，并使用真实场景来实施它。 它们使我们能够确保所有受支持的 Kubernetes 版本跨云提供商，容器引擎和网络插件工作。 他们确保最新版本的 Kubernetes 能够启用各种可选功能，安全升级，满足性能要求，并跨架构工作。&lt;/p&gt;

&lt;!--
With today’s [announcement from CNCF](https://www.cncf.io/announcement/2018/08/29/cncf-receives-9-million-cloud-credit-grant-from-google) – noting that Google Cloud has begun transferring ownership and management of the Kubernetes project’s cloud resources to CNCF community contributors, we are excited to embark on another journey. One that allows the project infrastructure to be owned and operated by the community of contributors, following the same open governance model that has worked for the rest of the project. Sound exciting to you? Come talk to us at #sig-testing on kubernetes.slack.com.
--&gt;

&lt;p&gt;今天&lt;a href=&#34;https://www.cncf.io/announcement/2018/08/29/cncf-receives-9-million-cloud-credit-grant-from-google&#34; target=&#34;_blank&#34;&gt;来自CNCF的公告&lt;/a&gt; - 注意到    Google Cloud 有开始将 Kubernetes 项目的云资源的所有权和管理权转让给 CNCF 社区贡献者，我们很高兴能够开始另一个旅程。 允许项目基础设施由贡献者社区拥有和运营，遵循对项目其余部分有效的相同开放治理模型。 听起来令人兴奋。 请来 kubernetes.slack.com 上的 #sig-testing on kubernetes.slack.com 与我们联系。&lt;/p&gt;

&lt;!--
Want to find out more? Come check out these resources:
--&gt;

&lt;p&gt;想了解更多？ 快来看看这些资源：&lt;/p&gt;

&lt;!--
* [Prow: Testing the way to Kubernetes Next](https://bentheelder.io/posts/prow)
* [Automation and the Kubernetes Contributor Experience](https://www.youtube.com/watch?v=BsIC7gPkH5M)
--&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://bentheelder.io/posts/prow&#34; target=&#34;_blank&#34;&gt;Prow: Testing the way to Kubernetes Next&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=BsIC7gPkH5M&#34; target=&#34;_blank&#34;&gt;Automation and the Kubernetes Contributor Experience&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: IPVS-Based In-Cluster Load Balancing Deep Dive</title>
      <link>https://kubernetes.io/zh/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/</link>
      <pubDate>Mon, 09 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/zh/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/</guid>
      <description>
        
        
        &lt;!--

Author: Jun Du(Huawei), Haibin Xie(Huawei), Wei Liang(Huawei)

Editor’s note: this post is part of a series of in-depth articles on what’s new in Kubernetes 1.11

--&gt;

&lt;p&gt;作者: Jun Du(华为), Haibin Xie(华为), Wei Liang(华为)&lt;/p&gt;

&lt;p&gt;注意: 这篇文章出自 系列深度文章 介绍 Kubernetes 1.11 的新特性&lt;/p&gt;

&lt;!--

Introduction

Per the Kubernetes 1.11 release blog post , we announced that IPVS-Based In-Cluster Service Load Balancing graduates to General Availability. In this blog, we will take you through a deep dive of the feature.

--&gt;

&lt;p&gt;介绍&lt;/p&gt;

&lt;p&gt;根据 Kubernetes 1.11 发布的博客文章, 我们宣布基于 IPVS 的集群内部服务负载均衡已达到一般可用性。 在这篇博客中，我们将带您深入了解该功能。&lt;/p&gt;

&lt;!--

What Is IPVS?

IPVS (IP Virtual Server) is built on top of the Netfilter and implements transport-layer load balancing as part of the Linux kernel.

IPVS is incorporated into the LVS (Linux Virtual Server), where it runs on a host and acts as a load balancer in front of a cluster of real servers. IPVS can direct requests for TCP- and UDP-based services to the real servers, and make services of the real servers appear as virtual services on a single IP address. Therefore, IPVS naturally supports Kubernetes Service.

--&gt;

&lt;p&gt;什么是 IPVS ?&lt;/p&gt;

&lt;p&gt;IPVS (IP Virtual Server)是在 Netfilter 上层构建的，并作为 Linux 内核的一部分，实现传输层负载均衡。&lt;/p&gt;

&lt;p&gt;IPVS 集成在 LVS（Linux Virtual Server，Linux 虚拟服务器）中，它在主机上运行，并在物理服务器集群前作为负载均衡器。IPVS 可以将基于 TCP 和 UDP 服务的请求定向到真实服务器，并使真实服务器的服务在单个IP地址上显示为虚拟服务。 因此，IPVS 自然支持 Kubernetes 服务。&lt;/p&gt;

&lt;!--

Why IPVS for Kubernetes?

As Kubernetes grows in usage, the scalability of its resources becomes more and more important. In particular, the scalability of services is paramount to the adoption of Kubernetes by developers/companies running large workloads.

Kube-proxy, the building block of service routing has relied on the battle-hardened iptables to implement the core supported Service types such as ClusterIP and NodePort. However, iptables struggles to scale to tens of thousands of Services because it is designed purely for firewalling purposes and is based on in-kernel rule lists.

Even though Kubernetes already support 5000 nodes in release v1.6, the kube-proxy with iptables is actually a bottleneck to scale the cluster to 5000 nodes. One example is that with NodePort Service in a 5000-node cluster, if we have 2000 services and each services have 10 pods, this will cause at least 20000 iptable records on each worker node, and this can make the kernel pretty busy.

On the other hand, using IPVS-based in-cluster service load balancing can help a lot for such cases. IPVS is specifically designed for load balancing and uses more efficient data structures (hash tables) allowing for almost unlimited scale under the hood.

--&gt;

&lt;p&gt;为什么为 Kubernetes 选择 IPVS ?&lt;/p&gt;

&lt;p&gt;随着 Kubernetes 的使用增长，其资源的可扩展性变得越来越重要。特别是，服务的可扩展性对于运行大型工作负载的开发人员/公司采用 Kubernetes 至关重要。&lt;/p&gt;

&lt;p&gt;Kube-proxy 是服务路由的构建块，它依赖于经过强化攻击的 iptables 来实现支持核心的服务类型，如 ClusterIP 和 NodePort。 但是，iptables 难以扩展到成千上万的服务，因为它纯粹是为防火墙而设计的，并且基于内核规则列表。&lt;/p&gt;

&lt;p&gt;尽管 Kubernetes 在版本v1.6中已经支持5000个节点，但使用 iptables 的 kube-proxy 实际上是将集群扩展到5000个节点的瓶颈。 一个例子是，在5000节点集群中使用 NodePort 服务，如果我们有2000个服务并且每个服务有10个 pod，这将在每个工作节点上至少产生20000个 iptable 记录，这可能使内核非常繁忙。&lt;/p&gt;

&lt;p&gt;另一方面，使用基于 IPVS 的集群内服务负载均衡可以为这种情况提供很多帮助。 IPVS 专门用于负载均衡，并使用更高效的数据结构（哈希表），允许几乎无限的规模扩张。&lt;/p&gt;

&lt;!--

IPVS-based Kube-proxy

Parameter Changes

Parameter: --proxy-mode In addition to existing userspace and iptables modes, IPVS mode is configured via --proxy-mode=ipvs. It implicitly uses IPVS NAT mode for service port mapping.

--&gt;

&lt;p&gt;基于 IPVS 的 Kube-proxy&lt;/p&gt;

&lt;p&gt;参数更改&lt;/p&gt;

&lt;p&gt;参数: &amp;ndash;proxy-mode 除了现有的用户空间和 iptables 模式，IPVS 模式通过&amp;ndash;proxy-mode = ipvs 进行配置。 它隐式使用 IPVS NAT 模式进行服务端口映射。&lt;/p&gt;

&lt;!--

Parameter: --ipvs-scheduler

A new kube-proxy parameter has been added to specify the IPVS load balancing algorithm, with the parameter being --ipvs-scheduler. If it’s not configured, then round-robin (rr) is the default value.

- rr: round-robin
- lc: least connection
- dh: destination hashing
- sh: source hashing
- sed: shortest expected delay
- nq: never queue

In the future, we can implement Service specific scheduler (potentially via annotation), which has higher priority and overwrites the value.

--&gt;

&lt;p&gt;参数: &amp;ndash;ipvs-scheduler&lt;/p&gt;

&lt;p&gt;添加了一个新的 kube-proxy 参数来指定 IPVS 负载均衡算法，参数为 &amp;ndash;ipvs-scheduler。 如果未配置，则默认为 round-robin 算法（rr）。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;rr: round-robin&lt;/li&gt;
&lt;li&gt;lc: least connection&lt;/li&gt;
&lt;li&gt;dh: destination hashing&lt;/li&gt;
&lt;li&gt;sh: source hashing&lt;/li&gt;
&lt;li&gt;sed: shortest expected delay&lt;/li&gt;
&lt;li&gt;nq: never queue&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;将来，我们可以实现特定于服务的调度程序（可能通过注释），该调度程序具有更高的优先级并覆盖该值。&lt;/p&gt;

&lt;!--

Parameter: --cleanup-ipvs Similar to the --cleanup-iptables parameter, if true, cleanup IPVS configuration and IPTables rules that are created in IPVS mode.

Parameter: --ipvs-sync-period Maximum interval of how often IPVS rules are refreshed (e.g. &#39;5s&#39;, &#39;1m&#39;). Must be greater than 0.

Parameter: --ipvs-min-sync-period Minimum interval of how often the IPVS rules are refreshed (e.g. &#39;5s&#39;, &#39;1m&#39;). Must be greater than 0.

--&gt;

&lt;p&gt;参数: &amp;ndash;cleanup-ipvs 类似于 &amp;ndash;cleanup-iptables 参数，如果为 true，则清除在 IPVS 模式下创建的 IPVS 配置和 IPTables 规则。&lt;/p&gt;

&lt;p&gt;参数: &amp;ndash;ipvs-sync-period 刷新 IPVS 规则的最大间隔时间（例如&amp;rsquo;5s&amp;rsquo;，&amp;rsquo;1m&amp;rsquo;）。 必须大于0。&lt;/p&gt;

&lt;p&gt;参数: &amp;ndash;ipvs-min-sync-period 刷新 IPVS 规则的最小间隔时间间隔（例如&amp;rsquo;5s&amp;rsquo;，&amp;rsquo;1m&amp;rsquo;）。 必须大于0。&lt;/p&gt;

&lt;!--

Parameter: --ipvs-exclude-cidrs  A comma-separated list of CIDR&#39;s which the IPVS proxier should not touch when cleaning up IPVS rules because IPVS proxier can&#39;t distinguish kube-proxy created IPVS rules from user original IPVS rules. If you are using IPVS proxier with your own IPVS rules in the environment, this parameter should be specified, otherwise your original rule will be cleaned.

--&gt;

&lt;p&gt;参数: &amp;ndash;ipvs-exclude-cidrs  清除 IPVS 规则时 IPVS 代理不应触及的 CIDR 的逗号分隔列表，因为 IPVS 代理无法区分 kube-proxy 创建的 IPVS 规则和用户原始规则 IPVS 规则。 如果您在环境中使用 IPVS proxier 和您自己的 IPVS 规则，则应指定此参数，否则将清除原始规则。&lt;/p&gt;

&lt;!--

Design Considerations

IPVS Service Network Topology

When creating a ClusterIP type Service, IPVS proxier will do the following three things:

- Make sure a dummy interface exists in the node, defaults to kube-ipvs0
- Bind Service IP addresses to the dummy interface
- Create IPVS virtual servers for each Service IP address respectively
  --&gt;

&lt;p&gt;设计注意事项&lt;/p&gt;

&lt;p&gt;IPVS 服务网络拓扑&lt;/p&gt;

&lt;p&gt;创建 ClusterIP 类型服务时，IPVS proxier 将执行以下三项操作：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;确保节点中存在虚拟接口，默认为 kube-ipvs0&lt;/li&gt;
&lt;li&gt;将服务 IP 地址绑定到虚拟接口&lt;/li&gt;
&lt;li&gt;分别为每个服务 IP 地址创建 IPVS  虚拟服务器&lt;/li&gt;
&lt;/ul&gt;

&lt;!--

Here comes an example:

    # kubectl describe svc nginx-service
    Name:           nginx-service
    ...
    Type:           ClusterIP
    IP:             10.102.128.4
    Port:           http    3080/TCP
    Endpoints:      10.244.0.235:8080,10.244.1.237:8080
    Session Affinity:   None

    # ip addr
    ...
    73: kube-ipvs0: &lt;BROADCAST,NOARP&gt; mtu 1500 qdisc noop state DOWN qlen 1000
        link/ether 1a:ce:f5:5f:c1:4d brd ff:ff:ff:ff:ff:ff
        inet 10.102.128.4/32 scope global kube-ipvs0
           valid_lft forever preferred_lft forever

    # ipvsadm -ln
    IP Virtual Server version 1.2.1 (size=4096)
    Prot LocalAddress:Port Scheduler Flags
      -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn
    TCP  10.102.128.4:3080 rr
      -&gt; 10.244.0.235:8080            Masq    1      0          0
      -&gt; 10.244.1.237:8080            Masq    1      0          0

--&gt;

&lt;p&gt;这是一个例子:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# kubectl describe svc nginx-service
Name:           nginx-service
...
Type:           ClusterIP
IP:             10.102.128.4
Port:           http    3080/TCP
Endpoints:      10.244.0.235:8080,10.244.1.237:8080
Session Affinity:   None

# ip addr
...
73: kube-ipvs0: &amp;lt;BROADCAST,NOARP&amp;gt; mtu 1500 qdisc noop state DOWN qlen 1000
    link/ether 1a:ce:f5:5f:c1:4d brd ff:ff:ff:ff:ff:ff
    inet 10.102.128.4/32 scope global kube-ipvs0
       valid_lft forever preferred_lft forever

# ipvsadm -ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -&amp;gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.102.128.4:3080 rr
  -&amp;gt; 10.244.0.235:8080            Masq    1      0          0
  -&amp;gt; 10.244.1.237:8080            Masq    1      0          0
&lt;/code&gt;&lt;/pre&gt;

&lt;!--

Please note that the relationship between a Kubernetes Service and IPVS virtual servers is 1:N. For example, consider a Kubernetes Service that has more than one IP address. An External IP type Service has two IP addresses - ClusterIP and External IP. Then the IPVS proxier will create 2 IPVS virtual servers - one for Cluster IP and another one for External IP. The relationship between a Kubernetes Endpoint (each IP+Port pair) and an IPVS virtual server is 1:1.

Deleting of a Kubernetes service will trigger deletion of the corresponding IPVS virtual server, IPVS real servers and its IP addresses bound to the dummy interface.

Port Mapping

There are three proxy modes in IPVS: NAT (masq), IPIP and DR. Only NAT mode supports port mapping. Kube-proxy leverages NAT mode for port mapping. The following example shows IPVS mapping Service port 3080 to Pod port 8080.

--&gt;

&lt;p&gt;请注意，Kubernetes 服务和 IPVS 虚拟服务器之间的关系是“1：N”。 例如，考虑具有多个 IP 地址的 Kubernetes 服务。 外部 IP 类型服务有两个 IP 地址 - 集群IP和外部 IP。 然后，IPVS 代理将创建2个 IPVS 虚拟服务器 - 一个用于集群 IP，另一个用于外部 IP。 Kubernetes 的 endpoint（每个IP +端口对）与 IPVS 虚拟服务器之间的关系是“1：1”。&lt;/p&gt;

&lt;p&gt;删除 Kubernetes 服务将触发删除相应的 IPVS 虚拟服务器，IPVS 物理服务器及其绑定到虚拟接口的 IP 地址。&lt;/p&gt;

&lt;p&gt;端口映射&lt;/p&gt;

&lt;p&gt;IPVS 中有三种代理模式：NAT（masq），IPIP 和 DR。 只有 NAT 模式支持端口映射。 Kube-proxy 利用 NAT 模式进行端口映射。 以下示例显示 IPVS 服务端口3080到Pod端口8080的映射。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;TCP  10.102.128.4:3080 rr
  -&amp;gt; 10.244.0.235:8080            Masq    1      0          0
  -&amp;gt; 10.244.1.237:8080            Masq    1      0
&lt;/code&gt;&lt;/pre&gt;

&lt;!--

Session Affinity

IPVS supports client IP session affinity (persistent connection). When a Service specifies session affinity, the IPVS proxier will set a timeout value (180min=10800s by default) in the IPVS virtual server. For example:

--&gt;

&lt;p&gt;会话关系&lt;/p&gt;

&lt;p&gt;IPVS 支持客户端 IP 会话关联（持久连接）。 当服务指定会话关系时，IPVS 代理将在 IPVS 虚拟服务器中设置超时值（默认为180分钟= 10800秒）。 例如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# kubectl describe svc nginx-service
Name:           nginx-service
...
IP:             10.102.128.4
Port:           http    3080/TCP
Session Affinity:   ClientIP

# ipvsadm -ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -&amp;gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.102.128.4:3080 rr persistent 10800
&lt;/code&gt;&lt;/pre&gt;

&lt;!--

Iptables &amp; Ipset in IPVS Proxier

IPVS is for load balancing and it can&#39;t handle other workarounds in kube-proxy, e.g. packet filtering, hairpin-masquerade tricks, SNAT, etc.

IPVS proxier leverages iptables in the above scenarios. Specifically, ipvs proxier will fall back on iptables in the following 4 scenarios:

- kube-proxy start with --masquerade-all=true
- Specify cluster CIDR in kube-proxy startup
- Support Loadbalancer type service
- Support NodePort type service

However, we don&#39;t want to create too many iptables rules. So we adopt ipset for the sake of decreasing iptables rules. The following is the table of ipset sets that IPVS proxier maintains:

--&gt;

&lt;p&gt;IPVS 代理中的 Iptables 和 Ipset&lt;/p&gt;

&lt;p&gt;IPVS 用于负载均衡，它无法处理 kube-proxy 中的其他问题，例如 包过滤，数据包欺骗，SNAT 等&lt;/p&gt;

&lt;p&gt;IPVS proxier 在上述场景中利用 iptables。 具体来说，ipvs proxier 将在以下4种情况下依赖于 iptables：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;kube-proxy 以 &amp;ndash;masquerade-all = true 开头&lt;/li&gt;
&lt;li&gt;在 kube-proxy 启动中指定集群 CIDR&lt;/li&gt;
&lt;li&gt;支持 Loadbalancer 类型服务&lt;/li&gt;
&lt;li&gt;支持 NodePort 类型的服务&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;但是，我们不想创建太多的 iptables 规则。 所以我们采用 ipset 来减少 iptables 规则。 以下是 IPVS proxier 维护的 ipset 集表：&lt;/p&gt;

&lt;!--

  set name                          members                                     usage
  KUBE-CLUSTER-IP                   All Service IP + port                       masquerade for cases that masquerade-all=true or clusterCIDR specified
  KUBE-LOOP-BACK                    All Service IP + port + IP                  masquerade for resolving hairpin issue
  KUBE-EXTERNAL-IP                  Service External IP + port                  masquerade for packets to external IPs
  KUBE-LOAD-BALANCER                Load Balancer ingress IP + port             masquerade for packets to Load Balancer type service
  KUBE-LOAD-BALANCER-LOCAL          Load Balancer ingress IP + port with externalTrafficPolicy=local    accept packets to Load Balancer with externalTrafficPolicy=local
  KUBE-LOAD-BALANCER-FW             Load Balancer ingress IP + port with loadBalancerSourceRanges   Drop packets for Load Balancer type Service with loadBalancerSourceRanges specified
  KUBE-LOAD-BALANCER-SOURCE-CIDR    Load Balancer ingress IP + port + source CIDR   accept packets for Load Balancer type Service with loadBalancerSourceRanges specified
  KUBE-NODE-PORT-TCP                NodePort type Service TCP port              masquerade for packets to NodePort(TCP)
  KUBE-NODE-PORT-LOCAL-TCP          NodePort type Service TCP port with externalTrafficPolicy=local accept packets to NodePort Service with externalTrafficPolicy=local
  KUBE-NODE-PORT-UDP                NodePort type Service UDP port              masquerade for packets to NodePort(UDP)
  KUBE-NODE-PORT-LOCAL-UDP          NodePort type service UDP port with externalTrafficPolicy=local accept packets to NodePort Service with externalTrafficPolicy=local

--&gt;

&lt;p&gt;设置名称                              成员                                          用法
  KUBE-CLUSTER-IP                   所有服务 IP + 端口                                masquerade-all=true 或 clusterCIDR 指定的情况下进行伪装
  KUBE-LOOP-BACK                    所有服务 IP +端口+ IP                             解决数据包欺骗问题
  KUBE-EXTERNAL-IP                  服务外部 IP +端口                                 将数据包伪装成外部 IP
  KUBE-LOAD-BALANCER                负载均衡器入口 IP +端口                              将数据包伪装成 Load Balancer 类型的服务
  KUBE-LOAD-BALANCER-LOCAL          负载均衡器入口 IP +端口 以及 externalTrafficPolicy=local   接受数据包到 Load Balancer externalTrafficPolicy=local
  KUBE-LOAD-BALANCER-FW             负载均衡器入口 IP +端口 以及 loadBalancerSourceRanges  使用指定的 loadBalancerSourceRanges 丢弃 Load Balancer类型Service的数据包
  KUBE-LOAD-BALANCER-SOURCE-CIDR    负载均衡器入口 IP +端口 + 源 CIDR                     接受 Load Balancer 类型 Service 的数据包，并指定loadBalancerSourceRanges
  KUBE-NODE-PORT-TCP                NodePort 类型服务 TCP                           将数据包伪装成 NodePort（TCP）
  KUBE-NODE-PORT-LOCAL-TCP          NodePort 类型服务 TCP 端口，带有 externalTrafficPolicy=local 接受数据包到 NodePort 服务 使用 externalTrafficPolicy=local
  KUBE-NODE-PORT-UDP                NodePort 类型服务 UDP 端口                        将数据包伪装成 NodePort(UDP)
  KUBE-NODE-PORT-LOCAL-UDP          NodePort 类型服务 UDP 端口 使用 externalTrafficPolicy=local 接受数据包到NodePort服务 使用 externalTrafficPolicy=local&lt;/p&gt;

&lt;!--

In general, for IPVS proxier, the number of iptables rules is static, no matter how many Services/Pods we have.

--&gt;

&lt;p&gt;通常，对于 IPVS proxier，无论我们有多少 Service/ Pod，iptables 规则的数量都是静态的。&lt;/p&gt;

&lt;!--

Run kube-proxy in IPVS Mode

Currently, local-up scripts, GCE scripts, and kubeadm support switching IPVS proxy mode via exporting environment variables (KUBE_PROXY_MODE=ipvs) or specifying flag (--proxy-mode=ipvs). Before running IPVS proxier, please ensure IPVS required kernel modules are already installed.

    ip_vs
    ip_vs_rr
    ip_vs_wrr
    ip_vs_sh
    nf_conntrack_ipv4

Finally, for Kubernetes v1.10, feature gate SupportIPVSProxyMode is set to true by default. For Kubernetes v1.11, the feature gate is entirely removed. However, you need to enable --feature-gates=SupportIPVSProxyMode=true explicitly for Kubernetes before v1.10.

--&gt;

&lt;p&gt;在 IPVS 模式下运行 kube-proxy&lt;/p&gt;

&lt;p&gt;目前，本地脚本，GCE 脚本和 kubeadm 支持通过导出环境变量（KUBE_PROXY_MODE=ipvs）或指定标志（&amp;ndash;proxy-mode=ipvs）来切换 IPVS 代理模式。 在运行IPVS 代理之前，请确保已安装 IPVS 所需的内核模块。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ip_vs
ip_vs_rr
ip_vs_wrr
ip_vs_sh
nf_conntrack_ipv4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后，对于 Kubernetes v1.10，“SupportIPVSProxyMode” 默认设置为 “true”。 对于 Kubernetes v1.11 ，该选项已完全删除。 但是，您需要在v1.10之前为Kubernetes 明确启用 &amp;ndash;feature-gates = SupportIPVSProxyMode = true。&lt;/p&gt;

&lt;!--

Get Involved

The simplest way to get involved with Kubernetes is by joining one of the many Special Interest Groups (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly community meeting, and through the channels below.

Thank you for your continued feedback and support.

Post questions (or answer questions) on Stack Overflow

Join the community portal for advocates on K8sPort

Follow us on Twitter @Kubernetesio for latest updates

Chat with the community on Slack

Share your Kubernetes story

--&gt;

&lt;p&gt;参与其中&lt;/p&gt;

&lt;p&gt;参与 Kubernetes 的最简单方法是加入众多&lt;a href=&#34;https://github.com/kubernetes/community/blob/master/sig-list.md&#34; target=&#34;_blank&#34;&gt;特别兴趣小组&lt;/a&gt; (SIG）中与您的兴趣一致的小组。 你有什么想要向 Kubernetes 社区广播的吗？ 在我们的每周&lt;a href=&#34;https://github.com/kubernetes/community/blob/master/communication.md#weekly-meeting&#34; target=&#34;_blank&#34;&gt;社区会议&lt;/a&gt;或通过以下渠道分享您的声音。&lt;/p&gt;

&lt;p&gt;感谢您的持续反馈和支持。
在&lt;a href=&#34;http://stackoverflow.com/questions/tagged/kubernetes&#34; target=&#34;_blank&#34;&gt;Stack Overflow&lt;/a&gt;上发布问题（或回答问题）&lt;/p&gt;

&lt;p&gt;加入&lt;a href=&#34;http://k8sport.org/&#34; target=&#34;_blank&#34;&gt;K8sPort&lt;/a&gt;的倡导者社区门户网站&lt;/p&gt;

&lt;p&gt;在 Twitter 上关注我们 &lt;a href=&#34;https://twitter.com/kubernetesio&#34; target=&#34;_blank&#34;&gt;@Kubernetesio&lt;/a&gt;获取最新更新&lt;/p&gt;

&lt;p&gt;在&lt;a href=&#34;http://slack.k8s.io/&#34; target=&#34;_blank&#34;&gt;Slack&lt;/a&gt;上与社区聊天&lt;/p&gt;

&lt;p&gt;分享您的 Kubernetes &lt;a href=&#34;https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform&#34; target=&#34;_blank&#34;&gt;故事&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Airflow在Kubernetes中的使用（第一部分）：一种不同的操作器</title>
      <link>https://kubernetes.io/zh/blog/2018/06/28/airflow%E5%9C%A8kubernetes%E4%B8%AD%E7%9A%84%E4%BD%BF%E7%94%A8%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E4%B8%80%E7%A7%8D%E4%B8%8D%E5%90%8C%E7%9A%84%E6%93%8D%E4%BD%9C%E5%99%A8/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/zh/blog/2018/06/28/airflow%E5%9C%A8kubernetes%E4%B8%AD%E7%9A%84%E4%BD%BF%E7%94%A8%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E4%B8%80%E7%A7%8D%E4%B8%8D%E5%90%8C%E7%9A%84%E6%93%8D%E4%BD%9C%E5%99%A8/</guid>
      <description>
        
        
        

&lt;!--

Author: Daniel Imberman (Bloomberg LP)

--&gt;

&lt;p&gt;作者: Daniel Imberman (Bloomberg LP)&lt;/p&gt;

&lt;!--

## Introduction



As part of Bloomberg&#39;s continued commitment to developing the Kubernetes ecosystem, we are excited to announce the Kubernetes Airflow Operator; a mechanism for Apache Airflow, a popular workflow orchestration framework to natively launch arbitrary Kubernetes Pods using the Kubernetes API.

--&gt;

&lt;h2 id=&#34;介绍&#34;&gt;介绍&lt;/h2&gt;

&lt;p&gt;作为Bloomberg [继续致力于开发Kubernetes生态系统]的一部分（&lt;a href=&#34;https://www.techatbloomberg.com/blog/bloomberg-awarded-first-cncf-end-user-award-contributions-kubernetes/），我们很高兴能够宣布Kubernetes&#34; target=&#34;_blank&#34;&gt;https://www.techatbloomberg.com/blog/bloomberg-awarded-first-cncf-end-user-award-contributions-kubernetes/），我们很高兴能够宣布Kubernetes&lt;/a&gt; Airflow Operator的发布; &lt;a href=&#34;https://airflow.apache.org/&#34; target=&#34;_blank&#34;&gt;Apache Airflow&lt;/a&gt;的机制，一种流行的工作流程编排框架，使用Kubernetes API可以在本机启动任意的Kubernetes Pod。&lt;/p&gt;

&lt;!--

## What Is Airflow?



Apache Airflow is one realization of the DevOps philosophy of &#34;Configuration As Code.&#34; Airflow allows users to launch multi-step pipelines using a simple Python object DAG (Directed Acyclic Graph). You can define dependencies, programmatically construct complex workflows, and monitor scheduled jobs in an easy to read UI.







--&gt;

&lt;h2 id=&#34;什么是airflow&#34;&gt;什么是Airflow?&lt;/h2&gt;

&lt;p&gt;Apache Airflow是DevOps“Configuration As Code”理念的一种实现。 Airflow允许用户使用简单的Python对象DAG（有向无环图）启动多步骤流水线。 您可以在易于阅读的UI中定义依赖关系，以编程方式构建复杂的工作流，并监视调度的作业。&lt;/p&gt;

&lt;p&gt;&lt;img src =“/ images / blog / 2018-05-25-Airflow-Kubernetes-Operator / 2018-05-25-airflow_dags.png”width =“85％”alt =“Airflow DAGs”/&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src =“/ images / blog / 2018-05-25-Airflow-Kubernetes-Operator / 2018-05-25-airflow.png”width =“85％”alt =“Airflow UI”/&gt;&lt;/p&gt;

&lt;!--

## Why Airflow on Kubernetes?



Since its inception, Airflow&#39;s greatest strength has been its flexibility. Airflow offers a wide range of integrations for services ranging from Spark and HBase, to services on various cloud providers. Airflow also offers easy extensibility through its plug-in framework. However, one limitation of the project is that Airflow users are confined to the frameworks and clients that exist on the Airflow worker at the moment of execution. A single organization can have varied Airflow workflows ranging from data science pipelines to application deployments. This difference in use-case creates issues in dependency management as both teams might use vastly different libraries for their workflows.



To address this issue, we&#39;ve utilized Kubernetes to allow users to launch arbitrary Kubernetes pods and configurations. Airflow users can now have full power over their run-time environments, resources, and secrets, basically turning Airflow into an &#34;any job you want&#34; workflow orchestrator.

--&gt;

&lt;h2 id=&#34;为什么在kubernetes上使用airflow&#34;&gt;为什么在Kubernetes上使用Airflow?&lt;/h2&gt;

&lt;p&gt;自成立以来，Airflow的最大优势在于其灵活性。 Airflow提供广泛的服务集成，包括Spark和HBase，以及各种云提供商的服务。 Airflow还通过其插件框架提供轻松的可扩展性。但是，该项目的一个限制是Airflow用户仅限于执行时Airflow站点上存在的框架和客户端。单个组织可以拥有各种Airflow工作流程，范围从数据科学流到应用程序部署。用例中的这种差异会在依赖关系管理中产生问题，因为两个团队可能会在其工作流程使用截然不同的库。&lt;/p&gt;

&lt;p&gt;为了解决这个问题，我们使Kubernetes允许用户启动任意Kubernetes pod和配置。 Airflow用户现在可以在其运行时环境，资源和机密上拥有全部权限，基本上将Airflow转变为“您想要的任何工作”工作流程协调器。&lt;/p&gt;

&lt;!--

## The Kubernetes Operator



Before we move any further, we should clarify that an Operator in Airflow is a task definition. When a user creates a DAG, they would use an operator like the &#34;SparkSubmitOperator&#34; or the &#34;PythonOperator&#34; to submit/monitor a Spark job or a Python function respectively. Airflow comes with built-in operators for frameworks like Apache Spark, BigQuery, Hive, and EMR. It also offers a Plugins entrypoint that allows DevOps engineers to develop their own connectors.



Airflow users are always looking for ways to make deployments and ETL pipelines simpler to manage. Any opportunity to decouple pipeline steps, while increasing monitoring, can reduce future outages and fire-fights. The following is a list of benefits provided by the Airflow Kubernetes Operator:

--&gt;

&lt;h2 id=&#34;kubernetes运营商&#34;&gt;Kubernetes运营商&lt;/h2&gt;

&lt;p&gt;在进一步讨论之前，我们应该澄清Airflow中的&lt;a href=&#34;https://airflow.apache.org/concepts.html#operators&#34; target=&#34;_blank&#34;&gt;Operator&lt;/a&gt;是一个任务定义。 当用户创建DAG时，他们将使用像“SparkSubmitOperator”或“PythonOperator”这样的operator分别提交/监视Spark作业或Python函数。 Airflow附带了Apache Spark，BigQuery，Hive和EMR等框架的内置运算符。 它还提供了一个插件入口点，允许DevOps工程师开发自己的连接器。&lt;/p&gt;

&lt;p&gt;Airflow用户一直在寻找更易于管理部署和ETL流的方法。 在增加监控的同时，任何解耦流程的机会都可以减少未来的停机等问题。 以下是Airflow Kubernetes Operator提供的好处：&lt;/p&gt;

&lt;!--

 * Increased flexibility for deployments:

Airflow&#39;s plugin API has always offered a significant boon to engineers wishing to test new functionalities within their DAGs. On the downside, whenever a developer wanted to create a new operator, they had to develop an entirely new plugin. Now, any task that can be run within a Docker container is accessible through the exact same operator, with no extra Airflow code to maintain.

--&gt;

&lt;ul&gt;
&lt;li&gt;提高部署灵活性：&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Airflow的插件API一直为希望在其DAG中测试新功能的工程师提供了重要的福利。 不利的一面是，每当开发人员想要创建一个新的operator时，他们就必须开发一个全新的插件。 现在，任何可以在Docker容器中运行的任务都可以通过完全相同的运算符访问，而无需维护额外的Airflow代码。&lt;/p&gt;

&lt;!--

 * Flexibility of configurations and dependencies:

For operators that are run within static Airflow workers, dependency management can become quite difficult. If a developer wants to run one task that requires SciPy and another that requires NumPy, the developer would have to either maintain both dependencies within all Airflow workers or offload the task to an external machine (which can cause bugs if that external machine changes in an untracked manner). Custom Docker images allow users to ensure that the tasks environment, configuration, and dependencies are completely idempotent.

--&gt;

&lt;ul&gt;
&lt;li&gt;配置和依赖的灵活性：&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对于在静态Airflow工作程序中运行的operator，依赖关系管理可能变得非常困难。 如果开发人员想要运行一个需要&lt;a href=&#34;https://www.scipy.org&#34; target=&#34;_blank&#34;&gt;SciPy&lt;/a&gt;的任务和另一个需要&lt;a href=&#34;http://www.numpy.org&#34; target=&#34;_blank&#34;&gt;NumPy&lt;/a&gt;的任务，开发人员必须维护所有Airflow节点中的依赖关系或将任务卸载到其他计算机（如果外部计算机以未跟踪的方式更改，则可能导致错误）。 自定义Docker镜像允许用户确保任务环境，配置和依赖关系完全是幂等的。&lt;/p&gt;

&lt;!--

 * Usage of kubernetes secrets for added security:

Handling sensitive data is a core responsibility of any DevOps engineer. At every opportunity, Airflow users want to isolate any API keys, database passwords, and login credentials on a strict need-to-know basis. With the Kubernetes operator, users can utilize the Kubernetes Vault technology to store all sensitive data. This means that the Airflow workers will never have access to this information, and can simply request that pods be built with only the secrets they need.

--&gt;

&lt;ul&gt;
&lt;li&gt;使用kubernetes Secret以增加安全性：&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;处理敏感数据是任何开发工程师的核心职责。 Airflow用户总有机会在严格条款的基础上隔离任何API密钥，数据库密码和登录凭据。 使用Kubernetes运算符，用户可以利用Kubernetes Vault技术存储所有敏感数据。 这意味着Airflow工作人员将永远无法访问此信息，并且可以容易地请求仅使用他们需要的密码信息构建pod。&lt;/p&gt;

&lt;!--

# Architecture







The Kubernetes Operator uses the Kubernetes Python Client to generate a request that is processed by the APIServer (1). Kubernetes will then launch your pod with whatever specs you&#39;ve defined (2). Images will be loaded with all the necessary environment variables, secrets and dependencies, enacting a single command. Once the job is launched, the operator only needs to monitor the health of track logs (3). Users will have the choice of gathering logs locally to the scheduler or to any distributed logging service currently in their Kubernetes cluster.

--&gt;

&lt;p&gt;＃架构&lt;/p&gt;

&lt;p&gt;&lt;img src =“/ images / blog / 2018-05-25-Airflow-Kubernetes-Operator / 2018-05-25-airflow-architecture.png”width =“85％”alt =“Airflow Architecture”/&gt;&lt;/p&gt;

&lt;p&gt;Kubernetes Operator使用&lt;a href=&#34;https://github.com/kubernetes-client/Python&#34; target=&#34;_blank&#34;&gt;Kubernetes Python客户端&lt;/a&gt;生成由APIServer处理的请求（1）。 然后，Kubernetes将使用您定义的需求启动您的pod（2）。映像文件中将加载环境变量，Secret和依赖项，执行单个命令。 一旦启动作业，operator只需要监视跟踪日志的状况（3）。 用户可以选择将日志本地收集到调度程序或当前位于其Kubernetes集群中的任何分布式日志记录服务。&lt;/p&gt;

&lt;!--

# Using the Kubernetes Operator



## A Basic Example



The following DAG is probably the simplest example we could write to show how the Kubernetes Operator works. This DAG creates two pods on Kubernetes: a Linux distro with Python and a base Ubuntu distro without it. The Python pod will run the Python request correctly, while the one without Python will report a failure to the user. If the Operator is working correctly, the passing-task pod should complete, while the failing-task pod returns a failure to the Airflow webserver.

--&gt;

&lt;p&gt;＃使用Kubernetes Operator&lt;/p&gt;

&lt;p&gt;##一个基本的例子&lt;/p&gt;

&lt;p&gt;以下DAG可能是我们可以编写的最简单的示例，以显示Kubernetes Operator的工作原理。 这个DAG在Kubernetes上创建了两个pod：一个带有Python的Linux发行版和一个没有它的基本Ubuntu发行版。 Python pod将正确运行Python请求，而没有Python的那个将向用户报告失败。 如果Operator正常工作，则应该完成“passing-task”pod，而“falling-task”pod则向Airflow网络服务器返回失败。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-Python&#34; data-lang=&#34;Python&#34;&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#00f;font-weight:bold&#34;&gt;airflow&lt;/span&gt; &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;import&lt;/span&gt; DAG

&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#00f;font-weight:bold&#34;&gt;datetime&lt;/span&gt; &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;import&lt;/span&gt; datetime, timedelta

&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#00f;font-weight:bold&#34;&gt;airflow.contrib.operators.kubernetes_pod_operator&lt;/span&gt; &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;import&lt;/span&gt; KubernetesPodOperator

&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#00f;font-weight:bold&#34;&gt;airflow.operators.dummy_operator&lt;/span&gt; &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;import&lt;/span&gt; DummyOperator


default_args &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; {

    &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;owner&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;airflow&amp;#39;&lt;/span&gt;,

    &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;depends_on_past&amp;#39;&lt;/span&gt;: False,

    &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;start_date&amp;#39;&lt;/span&gt;: datetime&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;utcnow(),

    &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;email&amp;#39;&lt;/span&gt;: [&lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;airflow@example.com&amp;#39;&lt;/span&gt;],

    &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;email_on_failure&amp;#39;&lt;/span&gt;: False,

    &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;email_on_retry&amp;#39;&lt;/span&gt;: False,

    &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;retries&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;,

    &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;retry_delay&amp;#39;&lt;/span&gt;: timedelta(minutes&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;5&lt;/span&gt;)

}



dag &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; DAG(

    &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;kubernetes_sample&amp;#39;&lt;/span&gt;, default_args&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;default_args, schedule_interval&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;timedelta(minutes&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;10&lt;/span&gt;))

start &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; DummyOperator(task_id&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;run_this_first&amp;#39;&lt;/span&gt;, dag&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;dag)
passing &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; KubernetesPodOperator(namespace&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;default&amp;#39;&lt;/span&gt;,

                          image&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;Python:3.6&amp;#34;&lt;/span&gt;,

                          cmds&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;Python&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;-c&amp;#34;&lt;/span&gt;],

                          arguments&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;print(&amp;#39;hello world&amp;#39;)&amp;#34;&lt;/span&gt;],

                          labels&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;foo&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;bar&amp;#34;&lt;/span&gt;},

                          name&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;passing-test&amp;#34;&lt;/span&gt;,

                          task_id&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;passing-task&amp;#34;&lt;/span&gt;,

                          get_logs&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;True,

                          dag&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;dag

                          )

 failing &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; KubernetesPodOperator(namespace&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;default&amp;#39;&lt;/span&gt;,

                          image&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;ubuntu:1604&amp;#34;&lt;/span&gt;,

                          cmds&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;Python&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;-c&amp;#34;&lt;/span&gt;],

                          arguments&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;print(&amp;#39;hello world&amp;#39;)&amp;#34;&lt;/span&gt;],

                          labels&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;foo&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;bar&amp;#34;&lt;/span&gt;},

                          name&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;fail&amp;#34;&lt;/span&gt;,

                          task_id&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;failing-task&amp;#34;&lt;/span&gt;,

                          get_logs&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;True,

                          dag&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;dag

                          )

passing&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;set_upstream(start)

failing&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;set_upstream(start)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;!--

## But how does this relate to my workflow?



While this example only uses basic images, the magic of Docker is that this same DAG will work for any image/command pairing you want. The following is a recommended CI/CD pipeline to run production-ready code on an Airflow DAG.



### 1: PR in github

Use Travis or Jenkins to run unit and integration tests, bribe your favorite team-mate into PR&#39;ing your code, and merge to the master branch to trigger an automated CI build.



### 2: CI/CD via Jenkins -&gt; Docker Image



Generate your Docker images and bump release version within your Jenkins build.



### 3: Airflow launches task



Finally, update your DAGs to reflect the new release version and you should be ready to go!

--&gt;

&lt;p&gt;##但这与我的工作流程有什么关系？&lt;/p&gt;

&lt;p&gt;虽然这个例子只使用基本映像，但Docker的神奇之处在于，这个相同的DAG可以用于您想要的任何图像/命令配对。 以下是推荐的CI / CD管道，用于在Airflow DAG上运行生产就绪代码。&lt;/p&gt;

&lt;h3 id=&#34;1-github中的pr&#34;&gt;1：github中的PR&lt;/h3&gt;

&lt;p&gt;使用Travis或Jenkins运行单元和集成测试，请您的朋友PR您的代码，并合并到主分支以触发自动CI构建。&lt;/p&gt;

&lt;h3 id=&#34;2-ci-cd构建jenkins-docker-image&#34;&gt;2：CI / CD构建Jenkins - &amp;gt; Docker Image&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://getintodevops.com/blog/building-your-first-Docker-image-with-jenkins-2-guide-for-developers&#34; target=&#34;_blank&#34;&gt;在Jenkins构建中生成Docker镜像和缓冲版本&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&#34;3-airflow启动任务&#34;&gt;3：Airflow启动任务&lt;/h3&gt;

&lt;p&gt;最后，更新您的DAG以反映新版本，您应该准备好了！&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-Python&#34; data-lang=&#34;Python&#34;&gt;production_task &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; KubernetesPodOperator(namespace&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;default&amp;#39;&lt;/span&gt;,

                          &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# image=&amp;#34;my-production-job:release-1.0.1&amp;#34;, &amp;lt;-- old release&lt;/span&gt;

                          image&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;my-production-job:release-1.0.2&amp;#34;&lt;/span&gt;,

                          cmds&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;Python&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;-c&amp;#34;&lt;/span&gt;],

                          arguments&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;print(&amp;#39;hello world&amp;#39;)&amp;#34;&lt;/span&gt;],

                          name&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;fail&amp;#34;&lt;/span&gt;,

                          task_id&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;failing-task&amp;#34;&lt;/span&gt;,

                          get_logs&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;True,

                          dag&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;dag

                          )&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;!--

# Launching a test deployment



Since the Kubernetes Operator is not yet released, we haven&#39;t released an official helm chart or operator (however both are currently in progress). However, we are including instructions for a basic deployment below  and are actively looking for foolhardy beta testers to try this new feature. To try this system out please follow these steps:



## Step 1: Set your kubeconfig to point to a kubernetes cluster



## Step 2: Clone the Airflow Repo:



Run git clone https://github.com/apache/incubator-airflow.git to clone the official Airflow repo.



## Step 3: Run



To run this basic deployment, we are co-opting the integration testing script that we currently use for the Kubernetes Executor (which will be explained in the next article of this series). To launch this deployment, run these three commands:

--&gt;

&lt;p&gt;＃启动测试部署&lt;/p&gt;

&lt;p&gt;由于Kubernetes运营商尚未发布，我们尚未发布官方&lt;a href=&#34;https://helm.sh/&#34; target=&#34;_blank&#34;&gt;helm&lt;/a&gt; 图表或operator（但两者目前都在进行中）。 但是，我们在下面列出了基本部署的说明，并且正在积极寻找测试人员来尝试这一新功能。 要试用此系统，请按以下步骤操作：&lt;/p&gt;

&lt;p&gt;##步骤1：将kubeconfig设置为指向kubernetes集群&lt;/p&gt;

&lt;p&gt;##步骤2：clone Airflow 仓库：&lt;/p&gt;

&lt;p&gt;运行git clone https：// github.com / apache / incubator-airflow.git来clone官方Airflow仓库。&lt;/p&gt;

&lt;p&gt;##步骤3：运行&lt;/p&gt;

&lt;p&gt;为了运行这个基本Deployment，我们正在选择我们目前用于Kubernetes Executor的集成测试脚本（将在本系列的下一篇文章中对此进行解释）。 要启动此部署，请运行以下三个命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
sed -ie &amp;quot;s/KubernetesExecutor/LocalExecutor/g&amp;quot; scripts/ci/kubernetes/kube/configmaps.yaml

./scripts/ci/kubernetes/Docker/build.sh

./scripts/ci/kubernetes/kube/deploy.sh

&lt;/code&gt;&lt;/pre&gt;

&lt;!--

Before we move on, let&#39;s discuss what these commands are doing:



### sed -ie &#34;s/KubernetesExecutor/LocalExecutor/g&#34; scripts/ci/kubernetes/kube/configmaps.yaml



The Kubernetes Executor is another Airflow feature that allows for dynamic allocation  of tasks as idempotent pods. The reason we are switching this to the LocalExecutor is simply to introduce one feature at a time. You are more then welcome to skip this step if you would like to try the Kubernetes Executor, however we will go into more detail in a future article.



### ./scripts/ci/kubernetes/Docker/build.sh



This script will tar the Airflow master source code build a Docker container based on the Airflow distribution



### ./scripts/ci/kubernetes/kube/deploy.sh



Finally, we create a full Airflow deployment on your cluster. This includes Airflow configs, a postgres backend, the webserver + scheduler, and all necessary services between. One thing to note is that the role binding supplied is a cluster-admin, so if you do not have that level of permission on the cluster, you can modify this at scripts/ci/kubernetes/kube/airflow.yaml



## Step 4: Log into your webserver



Now that your Airflow instance is running let&#39;s take a look at the UI! The UI lives in port 8080 of the Airflow pod, so simply run

--&gt;

&lt;p&gt;在我们继续之前，让我们讨论这些命令正在做什么：&lt;/p&gt;

&lt;h3 id=&#34;sed-ie-s-kubernetesexecutor-localexecutor-g-scripts-ci-kubernetes-kube-configmaps-yaml&#34;&gt;sed -ie“s / KubernetesExecutor / LocalExecutor / g”scripts / ci / kubernetes / kube / configmaps.yaml&lt;/h3&gt;

&lt;p&gt;Kubernetes Executor是另一种Airflow功能，允许动态分配任务已解决幂等pod的问题。我们将其切换到LocalExecutor的原因只是一次引入一个功能。如果您想尝试Kubernetes Executor，欢迎您跳过此步骤，但我们将在以后的文章中详细介绍。&lt;/p&gt;

&lt;h3 id=&#34;scripts-ci-kubernetes-docker-build-sh&#34;&gt;./scripts/ci/kubernetes/Docker/build.sh&lt;/h3&gt;

&lt;p&gt;此脚本将对Airflow主分支代码进行打包，以根据Airflow的发行文件构建Docker容器&lt;/p&gt;

&lt;h3 id=&#34;scripts-ci-kubernetes-kube-deploy-sh&#34;&gt;./scripts/ci/kubernetes/kube/deploy.sh&lt;/h3&gt;

&lt;p&gt;最后，我们在您的群集上创建完整的Airflow部署。这包括Airflow配置，postgres后端，webserver +调度程序以及之间的所有必要服务。需要注意的一点是，提供的角色绑定是集群管理员，因此如果您没有该集群的权限级别，可以在scripts / ci / kubernetes / kube / airflow.yaml中进行修改。&lt;/p&gt;

&lt;p&gt;##步骤4：登录您的网络服务器&lt;/p&gt;

&lt;p&gt;现在您的Airflow实例正在运行，让我们来看看UI！用户界面位于Airflow pod的8080端口，因此只需运行即可&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
WEB=$(kubectl get pods -o go-template --template &#39;{{range .items}}{{.metadata.name}}{{&amp;quot;\n&amp;quot;}}{{end}}&#39; | grep &amp;quot;airflow&amp;quot; | head -1)

kubectl port-forward $WEB 8080:8080

&lt;/code&gt;&lt;/pre&gt;

&lt;!--

 Now the Airflow UI will exist on http://localhost:8080. To log in simply enter airflow/airflow and you should have full access to the Airflow web UI.



## Step 5: Upload a test document



To modify/add your own DAGs, you can use kubectl cp to upload local files into the DAG folder of the Airflow scheduler. Airflow will then read the new DAG and automatically upload it to its system. The following command will upload any local file into the correct directory:

--&gt;

&lt;p&gt;现在，Airflow UI将存在于&lt;a href=&#34;http://localhost:8080上。&#34; target=&#34;_blank&#34;&gt;http://localhost:8080上。&lt;/a&gt; 要登录，只需输入airflow /airflow，您就可以完全访问Airflow Web UI。&lt;/p&gt;

&lt;p&gt;##步骤5：上传测试文档&lt;/p&gt;

&lt;p&gt;要修改/添加自己的DAG，可以使用kubectl cp将本地文件上传到Airflow调度程序的DAG文件夹中。 然后，Airflow将读取新的DAG并自动将其上传到其系统。 以下命令将任何本地文件上载到正确的目录中：&lt;/p&gt;

&lt;p&gt;kubectl cp &lt;local file&gt; &lt;namespace&gt;/&lt;pod&gt;:/root/airflow/dags -c scheduler&lt;/p&gt;

&lt;!--

## Step 6: Enjoy!



# So when will I be able to use this?



 While this feature is still in the early stages, we hope to see it released for wide release in the next few months.



# Get Involved



This feature is just the beginning of multiple major efforts to improves Apache Airflow integration into Kubernetes. The Kubernetes Operator has been merged into the 1.10 release branch of Airflow (the executor in experimental mode), along with a fully k8s native scheduler called the Kubernetes Executor (article to come). These features are still in a stage where early adopters/contributers can have a huge influence on the future of these features.



For those interested in joining these efforts, I&#39;d recommend checkint out these steps:



 * Join the airflow-dev mailing list at dev@airflow.apache.org.

 * File an issue in Apache Airflow JIRA

 * Join our SIG-BigData meetings on Wednesdays at 10am PST.

 * Reach us on slack at #sig-big-data on kubernetes.slack.com



Special thanks to the Apache Airflow and Kubernetes communities, particularly Grant Nicholas, Ben Goldberg, Anirudh Ramanathan, Fokko Dreisprong, and Bolke de Bruin, for your awesome help on these features as well as our future efforts.

--&gt;

&lt;p&gt;##步骤6：使用它！&lt;/p&gt;

&lt;p&gt;#那么我什么时候可以使用它？&lt;/p&gt;

&lt;p&gt;虽然此功能仍处于早期阶段，但我们希望在未来几个月内发布该功能以进行广泛发布。&lt;/p&gt;

&lt;p&gt;#参与其中&lt;/p&gt;

&lt;p&gt;此功能只是将Apache Airflow集成到Kubernetes中的多项主要工作的开始。 Kubernetes Operator已合并到&lt;a href=&#34;https://github.com/apache/incubator-airflow/tree/v1-10-test&#34; target=&#34;_blank&#34;&gt;Airflow的1.10发布分支&lt;/a&gt;（实验模式中的执行模块），以及完整的k8s本地调度程序称为Kubernetes Executor（即将发布文章）。这些功能仍处于早期采用者/贡献者可能对这些功能的未来产生巨大影响的阶段。&lt;/p&gt;

&lt;p&gt;对于有兴趣加入这些工作的人，我建议按照以下步骤：&lt;/p&gt;

&lt;p&gt;*加入airflow-dev邮件列表dev@airflow.apache.org。&lt;/p&gt;

&lt;p&gt;*在[Apache Airflow JIRA]中提出问题（&lt;a href=&#34;https://issues.apache.org/jira/projects/AIRFLOW/issues/）&#34; target=&#34;_blank&#34;&gt;https://issues.apache.org/jira/projects/AIRFLOW/issues/）&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;*周三上午10点太平洋标准时间加入我们的SIG-BigData会议。&lt;/p&gt;

&lt;p&gt;*在kubernetes.slack.com上的＃sig-big-data找到我们。&lt;/p&gt;

&lt;p&gt;特别感谢Apache Airflow和Kubernetes社区，特别是Grant Nicholas，Ben Goldberg，Anirudh Ramanathan，Fokko Dreisprong和Bolke de Bruin，感谢您对这些功能的巨大帮助以及我们未来的努力。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes 内的动态 Ingress</title>
      <link>https://kubernetes.io/zh/blog/2018/06/07/dynamic-ingress-kubernetes/</link>
      <pubDate>Thu, 07 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/zh/blog/2018/06/07/dynamic-ingress-kubernetes/</guid>
      <description>
        
        
        

&lt;!--
title: Dynamic Ingress in Kubernetes
date:  2018-06-07
Author: Richard Li (Datawire)
--&gt;

&lt;p&gt;作者: Richard Li (Datawire)&lt;/p&gt;

&lt;!--
Kubernetes makes it easy to deploy applications that consist of many microservices, but one of the key challenges with this type of architecture is dynamically routing ingress traffic to each of these services.  One approach is Ambassador, a Kubernetes-native open source API Gateway built on the Envoy Proxy. Ambassador is designed for dynamic environment where services may come and go frequently.
Ambassador is configured using Kubernetes annotations. Annotations are used to configure specific mappings from a given Kubernetes service to a particular URL. A mapping can include a number of annotations for configuring a route. Examples include rate limiting, protocol, cross-origin request sharing, traffic shadowing, and routing rules.
--&gt;

&lt;p&gt;Kubernetes 可以轻松部署由许多微服务组成的应用程序，但这种架构的关键挑战之一是动态地将流量路由到这些服务中的每一个。
一种方法是使用 &lt;a href=&#34;https://www.getambassador.io&#34; target=&#34;_blank&#34;&gt;Ambassador&lt;/a&gt;，
一个基于 &lt;a href=&#34;https://www.envoyproxy.io&#34; target=&#34;_blank&#34;&gt;Envoy Proxy&lt;/a&gt; 构建的 Kubernetes 原生开源 API 网关。
Ambassador 专为动态环境而设计，这类环境中的服务可能被频繁添加或删除。&lt;/p&gt;

&lt;p&gt;Ambassador 使用 Kubernetes 注解进行配置。
注解用于配置从给定 Kubernetes 服务到特定 URL 的具体映射关系。
每个映射中可以包括多个注解，用于配置路由。
注解的例子有速率限制、协议、跨源请求共享（CORS）、流量影射和路由规则等。&lt;/p&gt;

&lt;!--
## A Basic Ambassador Example
Ambassador is typically installed as a Kubernetes deployment, and is also available as a Helm chart. To configure Ambassador, create a Kubernetes service with the Ambassador annotations. Here is an example that configures Ambassador to route requests to /httpbin/ to the public httpbin.org service:
--&gt;

&lt;h2 id=&#34;一个简单的-ambassador-示例&#34;&gt;一个简单的 Ambassador 示例&lt;/h2&gt;

&lt;p&gt;Ambassador 通常作为 Kubernetes Deployment 来安装，也可以作为 Helm Chart 使用。
配置 Ambassador 时，请使用 Ambassador 注解创建 Kubernetes 服务。
下面是一个例子，用来配置 Ambassador，将针对 /httpbin/ 的请求路由到公共的 httpbin.org 服务：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: httpbin
  annotations:
    getambassador.io/config: |
      ---
      apiVersion: ambassador/v0
      kind:  Mapping
      name:  httpbin_mapping
      prefix: /httpbin/
      service: httpbin.org:80
      host_rewrite: httpbin.org
spec:
  type: ClusterIP
  ports:
    - port: 80
&lt;/code&gt;&lt;/pre&gt;

&lt;!--
A mapping object is created with a prefix of /httpbin/ and a service name of httpbin.org. The host_rewrite annotation specifies that the HTTP host header should be set to httpbin.org.
--&gt;

&lt;p&gt;例子中创建了一个 Mapping 对象，其 prefix 设置为 /httpbin/，service 名称为 httpbin.org。
其中的 host_rewrite 注解指定 HTTP 的 host 头部字段应设置为 httpbin.org。&lt;/p&gt;

&lt;!--
## Kubeflow
Kubeflow provides a simple way to easily deploy machine learning infrastructure on Kubernetes. The Kubeflow team needed a proxy that provided a central point of authentication and routing to the wide range of services used in Kubeflow, many of which are ephemeral in nature.
&lt;center&gt;&lt;i&gt;Kubeflow architecture, pre-Ambassador&lt;/center&gt;&lt;/i&gt;
--&gt;

&lt;h2 id=&#34;kubeflow&#34;&gt;Kubeflow&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubeflow/kubeflow&#34; target=&#34;_blank&#34;&gt;Kubeflow&lt;/a&gt; 提供了一种简单的方法，用于在 Kubernetes 上轻松部署机器学习基础设施。
Kubeflow 团队需要一个代理，为 Kubeflow 中所使用的各种服务提供集中化的认证和路由能力；Kubeflow 中许多服务本质上都是生命期很短的。&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;i&gt;Kubeflow architecture, pre-Ambassador&lt;/center&gt;&lt;/i&gt;&lt;/p&gt;

&lt;!--
## Service configuration
With Ambassador, Kubeflow can use a distributed model for configuration. Instead of a central configuration file, Ambassador allows each service to configure its route in Ambassador via Kubernetes annotations. Here is a simplified example configuration:
--&gt;

&lt;h2 id=&#34;服务配置&#34;&gt;服务配置&lt;/h2&gt;

&lt;p&gt;有了 Ambassador，Kubeflow 可以使用分布式模型进行配置。
Ambassador 不使用集中的配置文件，而是允许每个服务通过 Kubernetes 注解在 Ambassador 中配置其路由。
下面是一个简化的配置示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;---
apiVersion: ambassador/v0
kind:  Mapping
name: tfserving-mapping-test-post
prefix: /models/test/
rewrite: /model/test/:predict
method: POST
service: test.kubeflow:8000
&lt;/code&gt;&lt;/pre&gt;

&lt;!--
In this example, the “test” service uses Ambassador annotations to dynamically configure a route to the service, triggered only when the HTTP method is a POST, and the annotation also specifies a rewrite rule.
--&gt;

&lt;p&gt;示例中，“test” 服务使用 Ambassador 注解来为服务动态配置路由。
所配置的路由仅在 HTTP 方法是 POST 时触发；注解中同时还给出了一条重写规则。&lt;/p&gt;

&lt;!--
With Ambassador, Kubeflow manages routing easily with Kubernetes annotations. Kubeflow configures a single ingress object that directs traffic to Ambassador, then creates services with Ambassador annotations as needed to direct traffic  to specific backends. For example, when deploying TensorFlow services,  Kubeflow creates and and annotates a K8s service so that the model will be served at https://&lt;ingress host&gt;/models/&lt;model name&gt;/. Kubeflow can also use the Envoy Proxy to do the actual L7 routing. Using Ambassador, Kubeflow takes advantage of additional routing configuration like URL rewriting and method-based routing.
If you’re interested in using Ambassador with Kubeflow, the standard Kubeflow install automatically installs and configures Ambassador.
If you’re interested in using Ambassador as an API Gateway or Kubernetes ingress solution for your non-Kubeflow services, check out the Getting Started with Ambassador guide.
## Kubeflow and Ambassador
--&gt;

&lt;h2 id=&#34;kubeflow-和-ambassador&#34;&gt;Kubeflow 和 Ambassador&lt;/h2&gt;

&lt;p&gt;通过 Ambassador，Kubeflow 可以使用 Kubernetes 注解轻松管理路由。
Kubeflow 配置同一个 Ingress 对象，将流量定向到 Ambassador，然后根据需要创建具有 Ambassador 注解的服务，以将流量定向到特定后端。
例如，在部署 TensorFlow 服务时，Kubeflow 会创建 Kubernetes 服务并为其添加注解，
以便用户能够在 &lt;code&gt;https://&amp;lt;ingress主机&amp;gt;/models/&amp;lt;模型名称&amp;gt;/&lt;/code&gt; 处访问到模型本身。
Kubeflow 还可以使用 Envoy Proxy 来进行实际的 L7 路由。
通过 Ambassador，Kubeflow 能够更充分地利用 URL 重写和基于方法的路由等额外的路由配置能力。&lt;/p&gt;

&lt;p&gt;如果您对在 Kubeflow 中使用 Ambassador 感兴趣，标准的 Kubeflow 安装会自动安装和配置 Ambassador。&lt;/p&gt;

&lt;p&gt;如果您有兴趣将 Ambassador 用作 API 网关或 Kubernetes 的 Ingress 解决方案，
请参阅 &lt;a href=&#34;https://www.getambassador.io/user-guide/getting-started&#34; target=&#34;_blank&#34;&gt;Ambassador 入门指南&lt;/a&gt;。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes 这四年</title>
      <link>https://kubernetes.io/zh/blog/2018/06/06/kubernetes-%E8%BF%99%E5%9B%9B%E5%B9%B4/</link>
      <pubDate>Wed, 06 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/zh/blog/2018/06/06/kubernetes-%E8%BF%99%E5%9B%9B%E5%B9%B4/</guid>
      <description>
        
        
        &lt;p&gt;&lt;!--
**Author**: Joe Beda (CTO and Founder, Heptio)
 On June 6, 2014 I checked in the [first commit](https://github.com/kubernetes/kubernetes/commit/2c4b3a562ce34cddc3f8218a2c4d11c7310e6d56) of what would become the public repository for Kubernetes. Many would assume that is where the story starts. It is the beginning of history, right? But that really doesn’t tell the whole story.
--&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;作者&lt;/strong&gt;：Joe Beda( Heptio 首席技术官兼创始人)
 2014 年 6 月 6 日，我检查了 Kubernetes 公共代码库的&lt;a href=&#34;https://github.com/kubernetes/kubernetes/commit/2c4b3a562ce34cddc3f8218a2c4d11c7310e6d56&#34; target=&#34;_blank&#34;&gt;第一次 commit&lt;/a&gt; 。许多人会认为这是故事开始的地方。这难道不是一切开始的地方吗？但这的确不能把整个过程说清楚。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2018-06-06-4-years-of-k8s/k8s-first-commit.png&#34; alt=&#34;k8s_first_commit&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;!--
The cast leading up to that commit was large and the success for Kubernetes since then is owed to an ever larger cast.
 Kubernetes was built on ideas that had been proven out at Google over the previous ten years with Borg. And Borg, itself, owed its existence to even earlier efforts at Google and beyond.
 Concretely, Kubernetes started as some prototypes from Brendan Burns combined with ongoing work from me and Craig McLuckie to better align the internal Google experience with the Google Cloud experience. Brendan, Craig, and I really wanted people to use this, so we made the case to build out this prototype as an open source project that would bring the best ideas from Borg out into the open.
 After we got the nod, it was time to actually build the system.  We took Brendan’s prototype (in Java), rewrote it in Go, and built just enough to get the core ideas across.  By this time the team had grown to include Ville Aikas, Tim Hockin, Brian Grant, Dawn Chen and Daniel Smith.  Once we had something working, someone had to sign up to clean things up to get it ready for public launch.  That ended up being me. Not knowing the significance at the time, I created a new repo, moved things over, and checked it in.  So while I have the first public commit to the repo, there was work underway well before that.
 The version of Kubernetes at that point was really just a shadow of what it was to become.  The core concepts were there but it was very raw.  For example, Pods were called Tasks.  That was changed a day before we went public.  All of this led up to the public announcement of Kubernetes on June 10th, 2014 in a keynote from Eric Brewer at the first DockerCon.  You can watch that video here:
--&gt;&lt;/p&gt;

&lt;p&gt;第一次 commit 涉及的人员众多，自那以后 Kubernetes 的成功归功于更大的开发者阵容。
 Kubernetes 建立在过去十年曾经在 Google 的 Borg 集群管理系统中验证过的思路之上。而 Borg 本身也是 Google 和其他公司早期努力的结果。
 具体而言，Kubernetes 最初是从 Brendan Burns 的一些原型开始，结合我和 Craig McLuckie 正在进行的工作，以更好地将 Google 内部实践与 Google Cloud 的经验相结合。 Brendan，Craig 和我真的希望人们使用它，所以我们建议将这个原型构建为一个开源项目，将 Borg 的最佳创意带给大家。
在我们所有人同意后，就开始着手构建这个系统了。我们采用了 Brendan 的原型（Java 语言），用 Go 语言重写了它，并且以上述核心思想去构建该系统。到这个时候，团队已经成长为包括 Ville Aikas，Tim Hockin，Brian Grant，Dawn Chen 和 Daniel Smith。一旦我们有了一些工作需求，有人必须承担一些脱敏的工作，以便为公开发布做好准备。这个角色最终由我承担。当时，我不知道这件事情的重要性，我创建了一个新的仓库，把代码搬过来，然后进行了检查。所以在我第一次提交 public commit 之前，就有工作已经启动了。
那时 Kubernetes 的版本只是现在版本的简单雏形。核心概念已经有了，但非常原始。例如，Pods 被称为 Tasks，这在我们推广前一天就被替换。2014年6月10日 Eric Brewe 在第一届 DockerCon 上的演讲中正式发布了 Kubernetes 。您可以在此处观看该视频：&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/YrxnVKZeqK8&#34; frameborder=&#34;0&#34; allow=&#34;autoplay; encrypted-media&#34; allowfullscreen&gt;&lt;/iframe&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;&lt;!--
 But, however raw, that modest start was enough to pique the interest of a community that started strong and has only gotten stronger.  Over the past four years Kubernetes has exceeded the expectations of all of us that were there early on. We owe the Kubernetes community a huge debt.  The success the project has seen  is based not just on code and technology but also the way that an amazing group of people have come together to create something special.  The best expression of this is the [set of Kubernetes values](https://github.com/kubernetes/steering/blob/master/values.md) that Sarah Novotny helped curate.
 Here is to another 4 years and beyond! 🎉🎉🎉
--&gt;&lt;/p&gt;

&lt;p&gt;但是，无论多么原始，这小小的一步足以激起一个开始强大而且变得更强大的社区的兴趣。在过去的四年里，Kubernetes 已经超出了我们所有人的期望。我们对 Kubernetes 社区的所有人员表示感谢。该项目所取得的成功不仅基于代码和技术，还基于一群出色的人聚集在一起所做的有意义的事情。Sarah Novotny 策划的一套 &lt;a href=&#34;https://github.com/kubernetes/steering/blob/master/values.md&#34; target=&#34;_blank&#34;&gt;Kubernetes 价值观&lt;/a&gt;是以上最好的表现形式。
 让我们一起期待下一个4年！🎉🎉🎉&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes 1 11：向 discuss kubernetes 问好</title>
      <link>https://kubernetes.io/zh/blog/2018/05/30/kubernetes-1-11%E5%90%91-discuss-kubernetes-%E9%97%AE%E5%A5%BD/</link>
      <pubDate>Wed, 30 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/zh/blog/2018/05/30/kubernetes-1-11%E5%90%91-discuss-kubernetes-%E9%97%AE%E5%A5%BD/</guid>
      <description>
        
        
        &lt;!--
---
title: &#39; Kubernetes 1 11：say-hello-to-discuss-kubernetes &#39;
cn-approvers:
- congfairy
layout: blog
date: 2018-05-30
---
--&gt;

&lt;!--   

Author: Jorge Castro (Heptio)

--&gt;

&lt;p&gt;作者: Jorge Castro (Heptio)&lt;/p&gt;

&lt;!-- 

Communication is key when it comes to engaging a community of over 35,000 people in a global and remote environment. Keeping track of everything in the Kubernetes community can be an overwhelming task. On one hand we have our official resources, like Stack Overflow, GitHub, and the mailing lists, and on the other we have more ephemeral resources like Slack, where you can hop in, chat with someone, and then go on your merry way. 

--&gt;

&lt;p&gt;就一个超过 35,000 人的全球性社区而言，参与其中时沟通是非常关键的。 跟踪 Kubernetes 社区中的所有内容可能是一项艰巨的任务。 一方面，我们有官方资源，如 Stack Overflow，GitHub 和邮件列表，另一方面，我们有更多瞬时性的资源，如 Slack，你可以加入进去、与某人聊天然后各走各路。&lt;/p&gt;

&lt;!--

Slack is great for casual and timely conversations and keeping up with other community members, but communication can&#39;t be easily referenced in the future. Plus it can be hard to raise your hand in a room filled with 35,000 participants and find a voice. Mailing lists are useful when trying to reach a specific group of people with a particular ask and want to keep track of responses on the thread, but can be daunting with a large amount of people. Stack Overflow and GitHub are ideal for collaborating on projects or questions that involve code and need to be searchable in the future, but certain topics like &#34;What&#39;s your favorite CI/CD tool&#34; or &#34;Kubectl tips and tricks&#34; are offtopic there.

While our current assortment of communication channels are valuable in their own rights, we found that there was still a gap between email and real time chat. Across the rest of the web, many other open source projects like Docker, Mozilla, Swift, Ghost, and Chef have had success building communities on top of Discourse, an open source discussion platform. So what if we could use this tool to bring our discussions together under a modern roof, with an open API, and perhaps not let so much of our information fade into the ether? There&#39;s only one way to find out: Welcome to discuss.kubernetes.io

--&gt;

&lt;p&gt;Slack 非常适合随意和及时的对话，并与其他社区成员保持联系，但未来很难轻易引用通信。此外，在35,000名参与者中提问并得到回答很难。邮件列表在有问题尝试联系特定人群并且想要跟踪大家的回应时非常有用，但是对于大量人员来说可能是麻烦的。 Stack Overflow 和 GitHub 非常适合在涉及代码的项目或问题上进行协作，并且如果在将来要进行搜索也很有用，但某些主题如“你最喜欢的 CI/CD 工具是什么”或“&lt;a href=&#34;http://discuss.kubernetes.io/t/kubectl-tips-and-tricks/192&#34; target=&#34;_blank&#34;&gt;Kubectl提示和技巧&lt;/a&gt;“在那里是没有意义的。&lt;/p&gt;

&lt;p&gt;虽然我们目前的各种沟通渠道对他们自己来说都很有价值，但我们发现电子邮件和实时聊天之间仍然存在差距。在网络的其他部分，许多其他开源项目，如 Docker、Mozilla、Swift、Ghost 和 Chef，已经成功地在&lt;a href=&#34;http://www.discourse.org/features&#34; target=&#34;_blank&#34;&gt;Discourse&lt;/a&gt;之上构建社区，一个开放的讨论平台。那么，如果我们可以使用这个工具将我们的讨论结合在一个平台下，使用开放的API，或许也不会让我们的大部分信息消失在网络中呢？只有一种方法可以找到：欢迎来到&lt;a href=&#34;http://discuss.kubernetes.io&#34; target=&#34;_blank&#34;&gt;discuss.kubernetes.io&lt;/a&gt;&lt;/p&gt;

&lt;!--

Right off the bat we have categories that users can browse. Checking and posting in these categories allow users to participate in things they might be interested in without having to commit to subscribing to a list. Granular notification controls allow the users to subscribe to just the category or tag they want, and allow for responding to topics via email. 

Ecosystem partners and developers now have a place where they can [announce projects](https://discuss.kubernetes.io/c/announcements) that they&#39;re working on to users without wondering if it would be offtopic on an official list. We can make this place be not just about core Kubernetes, but about the hundreds of wonderful tools our community is building. 


This new community forum gives people a place to go where they can discuss Kubernetes, and a sounding board for developers to make announcements of things happening around Kubernetes, all while being searchable and easily accessible to a wider audience. 

Hop in and take a look. We&#39;re just getting started, so you might want to begin by [introducing yourself](https://discuss.kubernetes.io/t/introduce-yourself-here/56) and then browsing around. Apps are also available for [Android](https://play.google.com/store/apps/details?id=com.discourse&amp;hl=en_US&amp;rdid=com.discourse&amp;pli=1)and [iOS](https://itunes.apple.com/us/app/discourse-app/id1173672076?mt=8).  
 

--&gt;

&lt;p&gt;马上，我们有用户可以浏览的类别。检查和发布这些类别允许用户参与他们可能感兴趣的事情，而无需订阅列表。精细的通知控件允许用户只订阅他们想要的类别或标签，并允许通过电子邮件回复主题。&lt;/p&gt;

&lt;p&gt;生态系统合作伙伴和开发人员现在有一个地方可以&lt;a href=&#34;http://discuss.kubernetes.io/c/announcements&#34; target=&#34;_blank&#34;&gt;宣布项目&lt;/a&gt;，他们正在为用户工作，而不会想知道它是否会在官方列表中脱离主题。我们可以让这个地方不仅仅是关于核心 Kubernetes，而是关于我们社区正在建设的数百个精彩工具。&lt;/p&gt;

&lt;p&gt;这个新的社区论坛为人们提供了一个可以讨论 Kubernetes 的地方，也是开发人员在 Kubernetes 周围发布事件的声音板，同时可以搜索并且更容易被更广泛的用户访问。&lt;/p&gt;

&lt;p&gt;进来看看。我们刚刚开始，所以，您可能希望从&lt;a href=&#34;http://discuss.kubernetes.io/t/introduce-yourself-here/56&#34; target=&#34;_blank&#34;&gt;自我介绍&lt;/a&gt;开始，到处浏览。也有 &lt;a href=&#34;http://play.google.com/store/apps/details?id=com.discourse&amp;amp;hl=en_US&amp;amp;rdid=com.discourse&amp;amp;pli=1&#34; target=&#34;_blank&#34;&gt;Android&lt;/a&gt; 和 &lt;a href=&#34;http://itunes.apple.com/us/app/discourse-app/id1173672076?mt=8&#34; target=&#34;_blank&#34;&gt;iOS&lt;/a&gt; 应用下载。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: 在 Kubernetes 上开发</title>
      <link>https://kubernetes.io/zh/blog/2018/05/01/developing-on-kubernetes/</link>
      <pubDate>Tue, 01 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/zh/blog/2018/05/01/developing-on-kubernetes/</guid>
      <description>
        
        
        

&lt;!--
---
title: Developing on Kubernetes
date: 2018-05-01
slug: developing-on-kubernetes
---
--&gt;

&lt;!--**Authors**:--&gt;

&lt;p&gt;&lt;strong&gt;作者&lt;/strong&gt;： &lt;a href=&#34;https://twitter.com/mhausenblas&#34; target=&#34;_blank&#34;&gt;Michael Hausenblas&lt;/a&gt; (Red Hat), &lt;a href=&#34;https://twitter.com/errordeveloper&#34; target=&#34;_blank&#34;&gt;Ilya Dmitrichenko&lt;/a&gt; (Weaveworks)&lt;/p&gt;

&lt;!-- 
How do you develop a Kubernetes app? That is, how do you write and test an app that is supposed to run on Kubernetes? This article focuses on the challenges, tools and methods you might want to be aware of to successfully write Kubernetes apps alone or in a team setting. 
--&gt;

&lt;p&gt;您将如何开发一个 Kubernates 应用？也就是说，您如何编写并测试一个要在 Kubernates 上运行的应用程序？本文将重点介绍在独自开发或者团队协作中，您可能希望了解到的为了成功编写 Kubernetes 应用程序而需面临的挑战，工具和方法。&lt;/p&gt;

&lt;!--
We’re assuming you are a developer, you have a favorite programming language, editor/IDE, and a testing framework available. The overarching goal is to introduce minimal changes to your current workflow when developing the app for Kubernetes. For example, if you’re a Node.js developer and are used to a hot-reload setup—that is, on save in your editor the running app gets automagically updated—then dealing with containers and container images, with container registries, Kubernetes deployments, triggers, and more can not only be overwhelming but really take all the fun out if it.
--&gt;

&lt;p&gt;我们假定您是一位开发人员，有您钟爱的编程语言，编辑器/IDE（集成开发环境），以及可用的测试框架。在针对 Kubernates 开发应用时，最重要的目标是减少对当前工作流程的影响，改变越少越好，尽量做到最小。举个例子，如果您是 Node.js 开发人员，习惯于那种热重载的环境 - 也就是说您在编辑器里一做保存，正在运行的程序就会自动更新 - 那么跟容器、容器镜像或者镜像仓库打交道，又或是跟 Kubernetes 部署、triggers 以及更多头疼东西打交道，不仅会让人难以招架也真的会让开发过程完全失去乐趣。&lt;/p&gt;

&lt;!--
In the following, we’ll first discuss the overall development setup, then review tools of the trade, and last but not least do a hands-on walkthrough of three exemplary tools that allow for iterative, local app development against Kubernetes.
--&gt;

&lt;p&gt;在下文中，我们将首先讨论 Kubernetes 总体开发环境，然后回顾常用工具，最后进行三个示例性工具的实践演练。这些工具允许针对 Kubernetes 进行本地应用程序的开发和迭代。&lt;/p&gt;

&lt;!--
## Where to run your cluster?
--&gt;

&lt;h2 id=&#34;您的集群运行在哪里&#34;&gt;您的集群运行在哪里？&lt;/h2&gt;

&lt;!--
As a developer you want to think about where the Kubernetes cluster you’re developing against runs as well as where the development environment sits. Conceptually there are four development modes:
--&gt;

&lt;p&gt;作为开发人员，您既需要考虑所针对开发的 Kubernetes 集群运行在哪里，也需要思考开发环境如何配置。概念上，有四种开发模式：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2018-05-01-developing-on-kubernetes/dok-devmodes_preview.png&#34; alt=&#34;Dev Modes&#34; /&gt;&lt;/p&gt;

&lt;!--
A number of tools support pure offline development including Minikube, Docker for Mac/Windows, Minishift, and the ones we discuss in detail below. Sometimes, for example, in a microservices setup where certain microservices already run in the cluster, a proxied setup (forwarding traffic into and from the cluster) is preferable and Telepresence is an example tool in this category. The live mode essentially means you’re building and/or deploying against a remote cluster and, finally, the pure online mode means both your development environment and the cluster are remote, as this is the case with, for example, [Eclipse Che](https://www.eclipse.org/che/docs/kubernetes-single-user.html) or [Cloud 9](https://github.com/errordeveloper/k9c). Let’s now have a closer look at the basics of offline development: running Kubernetes locally.
--&gt;

&lt;p&gt;许多工具支持纯 offline 开发，包括 Minikube、Docker（Mac 版/Windows 版）、Minishift 以及下文中我们将详细讨论的几种。有时，比如说在一个微服务系统中，已经有若干微服务在运行，proxied 模式（通过转发把数据流传进传出集群）就非常合适，Telepresence 就是此类工具的一个实例。live 模式，本质上是您基于一个远程集群进行构建和部署。最后，纯 online 模式意味着您的开发环境和运行集群都是远程的，典型的例子是 &lt;a href=&#34;https://www.eclipse.org/che/docs/kubernetes-single-user.html&#34; target=&#34;_blank&#34;&gt;Eclipse Che&lt;/a&gt; 或者 &lt;a href=&#34;https://github.com/errordeveloper/k9c&#34; target=&#34;_blank&#34;&gt;Cloud 9&lt;/a&gt;。现在让我们仔细看看离线开发的基础：在本地运行 Kubernetes。&lt;/p&gt;

&lt;!--
[Minikube](/docs/getting-started-guides/minikube/) is a popular choice for those who prefer to run Kubernetes in a local VM. More recently Docker for [Mac](https://docs.docker.com/docker-for-mac/kubernetes/) and [Windows](https://docs.docker.com/docker-for-windows/kubernetes/) started shipping Kubernetes as an experimental package (in the “edge” channel). Some reasons why you may want to prefer using Minikube over the Docker desktop option are:
--&gt;

&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/getting-started-guides/minikube/&#34;&gt;Minikube&lt;/a&gt; 在更加喜欢于本地 VM 上运行 Kubernetes 的开发人员中，非常受欢迎。不久前，Docker 的 &lt;a href=&#34;https://docs.docker.com/docker-for-mac/kubernetes/&#34; target=&#34;_blank&#34;&gt;Mac&lt;/a&gt; 版和 &lt;a href=&#34;https://docs.docker.com/docker-for-windows/kubernetes/&#34; target=&#34;_blank&#34;&gt;Windows&lt;/a&gt; 版，都试验性地开始自带 Kubernetes（需要下载 “edge” 安装包）。在两者之间，以下原因也许会促使您选择 Minikube 而不是 Docker 桌面版：&lt;/p&gt;

&lt;!--
* You already have Minikube installed and running
* You prefer to wait until Docker ships a stable package
* You’re a Linux desktop user
* You are a Windows user who doesn’t have Windows 10 Pro with Hyper-V
--&gt;

&lt;ul&gt;
&lt;li&gt;您已经安装了 Minikube 并且它运行良好&lt;/li&gt;
&lt;li&gt;您想等到 Docker 出稳定版本&lt;/li&gt;
&lt;li&gt;您是 Linux 桌面用户&lt;/li&gt;
&lt;li&gt;您是 Windows 用户，但是没有配有 Hyper-V 的 Windows 10 Pro&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
Running a local cluster allows folks to work offline and that you don’t have to pay for using cloud resources. Cloud provider costs are often rather affordable and free tiers exists, however some folks prefer to avoid having to approve those costs with their manager as well as potentially incur unexpected costs, for example, when leaving cluster running over the weekend.
--&gt;

&lt;p&gt;运行一个本地集群，开发人员可以离线工作，不用支付云服务。云服务收费一般不会太高，并且免费的等级也有，但是一些开发人员不喜欢为了使用云服务而必须得到经理的批准，也不愿意支付意想不到的费用，比如说忘了下线而集群在周末也在运转。&lt;/p&gt;

&lt;!--
Some developers prefer to use a remote Kubernetes cluster, and this is usually to allow for larger compute and storage capacity and also enable collaborative workflows more easily. This means it’s easier for you to pull in a colleague to help with debugging or share access to an app in the team. Additionally, for some developers it can be critical to mirror production environment as closely as possible, especially when it comes down to external cloud services, say,  proprietary databases, object stores, message queues, external load balancer, or mail delivery systems.
--&gt;

&lt;p&gt;有些开发人员却更喜欢远程的 Kubernetes 集群，这样他们通常可以获得更大的计算能力和存储容量，也简化了协同工作流程。您可以更容易的拉上一个同事来帮您调试，或者在团队内共享一个应用的使用。再者，对某些开发人员来说，尽可能的让开发环境类似生产环境至关重要，尤其是您依赖外部厂商的云服务时，如：专有数据库、云对象存储、消息队列、外商的负载均衡器或者邮件投递系统。&lt;/p&gt;

&lt;!--
In summary, there are good reasons for you to develop against a local cluster as well as a remote one. It very much depends on in which phase you are: from early prototyping and/or developing alone to integrating a set of more stable microservices.
--&gt;

&lt;p&gt;总之，无论您选择本地或者远程集群，理由都足够多。这很大程度上取决于您所处的阶段：从早期的原型设计/单人开发到后期面对一批稳定微服务的集成。&lt;/p&gt;

&lt;!--
Now that you have a basic idea of the options around the runtime environment, let’s move on to how to iteratively develop and deploy your app.
--&gt;

&lt;p&gt;既然您已经了解到运行环境的基本选项，那么我们就接着讨论如何迭代式的开发并部署您的应用。&lt;/p&gt;

&lt;!--
## The tools of the trade
--&gt;

&lt;h2 id=&#34;常用工具&#34;&gt;常用工具&lt;/h2&gt;

&lt;!--
We are now going to review tooling allowing you to develop apps on Kubernetes with the focus on having minimal impact on your existing workflow. We strive to provide an unbiased description including implications of using each of the tools in general terms.
--&gt;

&lt;p&gt;我们现在回顾既可以允许您可以在 Kubernetes 上开发应用程序又尽可能最小地改变您现有的工作流程的一些工具。我们致力于提供一份不偏不倚的描述，也会提及使用某个工具将会意味着什么。&lt;/p&gt;

&lt;!--
Note that this is a tricky area since even for established technologies such as, for example, JSON vs YAML vs XML or REST vs gRPC vs SOAP a lot depends on your background, your preferences and organizational settings. It’s even harder to compare tooling in the Kubernetes ecosystem as things evolve very rapidly and new tools are announced almost on a weekly basis; during the preparation of this post alone, for example, [Gitkube](https://gitkube.sh/) and [Watchpod](https://github.com/MinikubeAddon/watchpod) came out. To cover these new tools as well as related, existing tooling such as [Weave Flux](https://github.com/weaveworks/flux) and OpenShift’s [S2I](https://docs.openshift.com/container-platform/3.9/creating_images/s2i.html) we are planning a follow-up blog post to the one you’re reading.
--&gt;

&lt;p&gt;请注意这很棘手，因为即使在成熟定型的技术中做选择，比如说在 JSON、YAML、XML、REST、gRPC 或者 SOAP 之间做选择，很大程度也取决于您的背景、喜好以及公司环境。在 Kubernetes 生态系统内比较各种工具就更加困难，因为技术发展太快，几乎每周都有新工具面市；举个例子，仅在准备这篇博客的期间，&lt;a href=&#34;https://gitkube.sh/&#34; target=&#34;_blank&#34;&gt;Gitkube&lt;/a&gt; 和 &lt;a href=&#34;https://github.com/MinikubeAddon/watchpod&#34; target=&#34;_blank&#34;&gt;Watchpod&lt;/a&gt; 相继出品。为了进一步覆盖到这些新的，以及一些相关的已推出的工具，例如 &lt;a href=&#34;https://github.com/weaveworks/flux&#34; target=&#34;_blank&#34;&gt;Weave Flux&lt;/a&gt; 和 OpenShift 的 &lt;a href=&#34;https://docs.openshift.com/container-platform/3.9/creating_images/s2i.html&#34; target=&#34;_blank&#34;&gt;S2I&lt;/a&gt;，我们计划再写一篇跟进的博客。&lt;/p&gt;

&lt;h3 id=&#34;draft&#34;&gt;Draft&lt;/h3&gt;

&lt;!--
[Draft](https://github.com/Azure/draft) aims to help you get started deploying any app to Kubernetes. It is capable of applying heuristics as to what programming language your app is written in and generates a Dockerfile along with a Helm chart. It then runs the build for you and deploys resulting image to the target cluster via the Helm chart. It also allows user to setup port forwarding to localhost very easily.
--&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/Azure/draft&#34; target=&#34;_blank&#34;&gt;Draft&lt;/a&gt; 旨在帮助您将任何应用程序部署到 Kubernetes。它能够检测到您的应用所使用的编程语言，并且生成一份 Dockerfile 和 Helm 图表。然后它替您启动构建并且依照 Helm 图表把所生产的镜像部署到目标集群。它也可以让您很容易地设置到 localhost 的端口映射。&lt;/p&gt;

&lt;!--
Implications:
--&gt;

&lt;p&gt;这意味着：&lt;/p&gt;

&lt;!--
* User can customise the chart and Dockerfile templates however they like, or even create a [custom pack](https://github.com/Azure/draft/blob/master/docs/reference/dep-003.md) (with Dockerfile, the chart and more) for future use
--&gt;

&lt;ul&gt;
&lt;li&gt;用户可以任意地自定义 Helm 图表和 Dockerfile 模版，或者甚至创建一个 &lt;a href=&#34;https://github.com/Azure/draft/blob/master/docs/reference/dep-003.md&#34; target=&#34;_blank&#34;&gt;custom pack&lt;/a&gt;（使用 Dockerfile、Helm 图表以及其他）以备后用&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
* It’s not very simple to guess how just any app is supposed to be built, in some cases user may need to tweak Dockerfile and the Helm chart that Draft generates
--&gt;

&lt;ul&gt;
&lt;li&gt;要想理解一个应用应该怎么构建并不容易，在某些情况下，用户也许需要修改 Draft 生成的 Dockerfile 和 Heml 图表&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
* With [Draft version 0.12.0](https://github.com/Azure/draft/releases/tag/v0.12.0) or older, every time user wants to test a change, they need to wait for Draft to copy the code to the cluster, then run the build, push the image and release updated chart; this can timely, but it results in an image being for every single change made by the user (whether it was committed to git or not)
--&gt;

&lt;ul&gt;
&lt;li&gt;如果使用 &lt;a href=&#34;https://github.com/Azure/draft/releases/tag/v0.12.0&#34; target=&#34;_blank&#34;&gt;Draft version 0.12.0&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt; 或者更老版本，每一次用户想要测试一个改动，他们需要等 Draft 把代码拷贝到集群，运行构建，推送镜像并且发布更新后的图表；这些步骤可能进行得很快，但是每一次用户的改动都会产生一个镜像（无论是否提交到 git ）&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
* As of Draft version 0.12.0, builds are executed locally
* User doesn’t have an option to choose something other than Helm for deployment
* It can watch local changes and trigger deployments, but this feature is not enabled by default
--&gt;

&lt;ul&gt;
&lt;li&gt;在 Draft 0.12.0版本，构建是本地进行的&lt;/li&gt;
&lt;li&gt;用户不能选择 Helm 以外的工具进行部署&lt;/li&gt;
&lt;li&gt;它可以监控本地的改动并且触发部署，但是这个功能默认是关闭的&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
* It allows developer to use either local or remote Kubernetes cluster
* Deploying to production is up to the user, Draft authors recommend their other project – Brigade
* Can be used instead of Skaffold, and along the side of Squash
--&gt;

&lt;ul&gt;
&lt;li&gt;它允许开发人员使用本地或者远程的 Kubernates 集群&lt;/li&gt;
&lt;li&gt;如何部署到生产环境取决于用户， Draft 的作者推荐了他们的另一个项目 - Brigade&lt;/li&gt;
&lt;li&gt;可以代替 Skaffold， 并且可以和 Squash 一起使用&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
More info:
--&gt;

&lt;p&gt;更多信息：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/blog/2017/05/draft-kubernetes-container-development&#34; target=&#34;_blank&#34;&gt;Draft: Kubernetes container development made easy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Azure/draft/blob/master/docs/getting-started.md&#34; target=&#34;_blank&#34;&gt;Getting Started Guide&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;【1】：此处疑为 0.11.0，因为 0.12.0 已经支持本地构建，见下一条&lt;/p&gt;

&lt;h3 id=&#34;skaffold&#34;&gt;Skaffold&lt;/h3&gt;

&lt;!--
[Skaffold](https://github.com/GoogleCloudPlatform/skaffold) is a tool that aims to provide portability for CI integrations with different build system, image registry and deployment tools. It is different from Draft, yet somewhat comparable. It has a basic capability for generating manifests, but it’s not a prominent feature. Skaffold is extendible and lets user pick tools for use in each of the steps in building and deploying their app.
--&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/GoogleCloudPlatform/skaffold&#34; target=&#34;_blank&#34;&gt;Skaffold&lt;/a&gt; 让 CI 集成具有可移植性的，它允许用户采用不同的构建系统，镜像仓库和部署工具。它不同于 Draft，同时也具有一定的可比性。它具有生成系统清单的基本能力，但那不是一个重要功能。Skaffold 易于扩展，允许用户在构建和部署应用的每一步选取相应的工具。&lt;/p&gt;

&lt;!--
Implications:
--&gt;

&lt;p&gt;这意味着：&lt;/p&gt;

&lt;!--
* Modular by design
* Works independently of CI vendor, user doesn’t need Docker or Kubernetes plugin
* Works without CI as such, i.e. from the developer’s laptop
* It can watch local changes and trigger deployments
--&gt;

&lt;ul&gt;
&lt;li&gt;模块化设计&lt;/li&gt;
&lt;li&gt;不依赖于 CI，用户不需要 Docker 或者 Kubernetes 插件&lt;/li&gt;
&lt;li&gt;没有 CI 也可以工作，也就是说，可以在开发人员的电脑上工作&lt;/li&gt;
&lt;li&gt;它可以监控本地的改动并且触发部署&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
* It allows developer to use either local or remote Kubernetes cluster
* It can be used to deploy to production, user can configure how exactly they prefer to do it and provide different kind of pipeline for each target environment
* Can be used instead of Draft, and along the side with most other tools
--&gt;

&lt;ul&gt;
&lt;li&gt;它允许开发人员使用本地或者远程的 Kubernetes 集群&lt;/li&gt;
&lt;li&gt;它可以用于部署生产环境，用户可以精确配置，也可以为每一套目标环境提供不同的生产线&lt;/li&gt;
&lt;li&gt;可以代替 Draft，并且和其他工具一起使用&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
More info:
--&gt;

&lt;p&gt;更多信息：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cloudplatform.googleblog.com/2018/03/introducing-Skaffold-Easy-and-repeatable-Kubernetes-development.html&#34; target=&#34;_blank&#34;&gt;Introducing Skaffold: Easy and repeatable Kubernetes development&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/GoogleCloudPlatform/skaffold#getting-started-with-local-tooling&#34; target=&#34;_blank&#34;&gt;Getting Started Guide&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;squash&#34;&gt;Squash&lt;/h3&gt;

&lt;!--
[Squash](https://github.com/solo-io/squash) consists of a debug server that is fully integrated with Kubernetes, and a IDE plugin. It allows you to insert breakpoints and do all the fun stuff you are used to doing when debugging an application using an IDE. It bridges IDE debugging experience with your Kubernetes cluster by allowing you to attach the debugger to a pod running in your Kubernetes cluster.
--&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/solo-io/squash&#34; target=&#34;_blank&#34;&gt;Squash&lt;/a&gt; 包含一个与 Kubernetes 全面集成的调试服务器，以及一个 IDE 插件。它允许您插入断点和所有的调试操作，就像您所习惯的使用 IDE 调试一个程序一般。它允许您将调试器应用到 Kubernetes 集群中运行的 pod 上，从而让您可以使用 IDE 调试 Kubernetes 集群。&lt;/p&gt;

&lt;!--
Implications:
--&gt;

&lt;p&gt;这意味着：&lt;/p&gt;

&lt;!--
* Can be used independently of other tools you chose
* Requires a privileged DaemonSet
* Integrates with popular IDEs
* Supports Go, Python, Node.js, Java and gdb
--&gt;

&lt;ul&gt;
&lt;li&gt;不依赖您选择的其它工具&lt;/li&gt;
&lt;li&gt;需要一组特权 DaemonSet&lt;/li&gt;
&lt;li&gt;可以和流行 IDE 集成&lt;/li&gt;
&lt;li&gt;支持 Go、Python、Node.js、Java 和 gdb&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
* User must ensure application binaries inside the container image are compiled with debug symbols
* Can be used in combination with any other tools described here
* It can be used with either local or remote Kubernetes cluster
--&gt;

&lt;ul&gt;
&lt;li&gt;用户必须确保容器中的应用程序使编译时使用了调试符号&lt;/li&gt;
&lt;li&gt;可与此处描述的任何其他工具结合使用&lt;/li&gt;
&lt;li&gt;它可以与本地或远程 Kubernetes 集群一起使用&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
More info:
--&gt;

&lt;p&gt;更多信息：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=5TrV3qzXlgI&#34; target=&#34;_blank&#34;&gt;Squash: A Debugger for Kubernetes Apps&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/solo-io/squash/blob/master/docs/getting-started.md&#34; target=&#34;_blank&#34;&gt;Getting Started Guide&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;telepresence&#34;&gt;Telepresence&lt;/h3&gt;

&lt;!--
[Telepresence](https://www.telepresence.io/) connects containers running on developer’s workstation with a remote Kubernetes cluster using a two-way proxy and emulates in-cluster environment as well as provides access to config maps and secrets. It aims to improve iteration time for container app development by eliminating the need for deploying app to the cluster and leverages local container to abstract network and filesystem interface in order to make it appear as if the app was running in the cluster.
--&gt;

&lt;p&gt;&lt;a href=&#34;https://www.telepresence.io/&#34; target=&#34;_blank&#34;&gt;Telepresence&lt;/a&gt; 使用双向代理将开发人员工作站上运行的容器与远程 Kubernetes 集群连接起来，并模拟集群内环境以及提供对配置映射和机密的访问。它消除了将应用部署到集群的需要，并利用本地容器抽象出网络和文件系统接口，以使其看起来应用好像就在集群中运行，从而改进容器应用程序开发的迭代时间。&lt;/p&gt;

&lt;!--
Implications:
--&gt;

&lt;p&gt;这意味着：&lt;/p&gt;

&lt;!--
* It can be used independently of other tools you chose
* Using together with Squash is possible, although Squash would have to be used for pods in the cluster, while conventional/local debugger would need to be used for debugging local container that’s connected to the cluster via Telepresence
* Telepresence imposes some network latency
--&gt;

&lt;ul&gt;
&lt;li&gt;它不依赖于其它您选取的工具&lt;/li&gt;
&lt;li&gt;可以同 Squash 一起使用，但是 Squash 必须用于调试集群中的 pods，而传统/本地调试器需要用于调试通过 Telepresence 连接到集群的本地容器&lt;/li&gt;
&lt;li&gt;Telepresence 会产生一些网络延迟&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
* It provides connectivity via a side-car process - sshuttle, which is based on SSH
* More intrusive dependency injection mode with LD_PRELOAD/DYLD_INSERT_LIBRARIES is also available
* It is most commonly used with a remote Kubernetes cluster, but can be used with a local one also
--&gt;

&lt;ul&gt;
&lt;li&gt;它通过辅助进程提供连接 -  sshuttle，基于SSH的一个工具&lt;/li&gt;
&lt;li&gt;还提供了使用 LD_PRELOAD/DYLD_INSERT_LIBRARIES 的更具侵入性的依赖注入模式&lt;/li&gt;
&lt;li&gt;它最常用于远程 Kubernetes 集群，但也可以与本地集群一起使用&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
More info:
--&gt;

&lt;p&gt;更多信息：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.telepresence.io/&#34; target=&#34;_blank&#34;&gt;Telepresence: fast, realistic local development for Kubernetes microservices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.telepresence.io/tutorials/docker&#34; target=&#34;_blank&#34;&gt;Getting Started Guide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.telepresence.io/discussion/how-it-works&#34; target=&#34;_blank&#34;&gt;How It Works&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;ksync&#34;&gt;Ksync&lt;/h3&gt;

&lt;!--
[Ksync](https://github.com/vapor-ware/ksync) synchronizes application code (and configuration) between your local machine and the container running in Kubernetes, akin to what [oc rsync](https://docs.openshift.com/container-platform/3.9/dev_guide/copy_files_to_container.html) does in OpenShift. It aims to improve iteration time for app development by eliminating build and deployment steps.
--&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/vapor-ware/ksync&#34; target=&#34;_blank&#34;&gt;Ksync&lt;/a&gt; 在本地计算机和运行在 Kubernetes 中的容器之间同步应用程序代码（和配置），类似于 &lt;a href=&#34;https://docs.openshift.com/container-platform/3.9/dev_guide/copy_files_to_container.html&#34; target=&#34;_blank&#34;&gt;oc rsync&lt;/a&gt; 在 OpenShift 中的角色。它旨在通过消除构建和部署步骤来缩短应用程序开发的迭代时间。&lt;/p&gt;

&lt;!--
Implications:
--&gt;

&lt;p&gt;这意味着：&lt;/p&gt;

&lt;!--
* It bypasses container image build and revision control
* Compiled language users have to run builds inside the pod (TBC)
* Two-way sync – remote files are copied to local directory
* Container is restarted each time remote filesystem is updated
* No security features – development only
--&gt;

&lt;ul&gt;
&lt;li&gt;它绕过容器图像构建和修订控制&lt;/li&gt;
&lt;li&gt;使用编译语言的用户必须在 pod（TBC）内运行构建&lt;/li&gt;
&lt;li&gt;双向同步 - 远程文件会复制到本地目录&lt;/li&gt;
&lt;li&gt;每次更新远程文件系统时都会重启容器&lt;/li&gt;
&lt;li&gt;无安全功能 - 仅限开发&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
* Utilizes [Syncthing](https://github.com/syncthing/syncthing), a Go library for peer-to-peer sync
* Requires a privileged DaemonSet running in the cluster
* Node has to use Docker with overlayfs2 – no other CRI implementations are supported at the time of writing
--&gt;

&lt;ul&gt;
&lt;li&gt;使用 &lt;a href=&#34;https://github.com/syncthing/syncthing&#34; target=&#34;_blank&#34;&gt;Syncthing&lt;/a&gt;，一个用于点对点同步的 Go 语言库&lt;/li&gt;
&lt;li&gt;需要一个在集群中运行的特权 DaemonSet&lt;/li&gt;
&lt;li&gt;Node 必须使用带有 overlayfs2 的 Docker  - 在写作本文时，尚不支持其他 CRI 实现&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
More info:
--&gt;

&lt;p&gt;更多信息：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/vapor-ware/ksync#getting-started&#34; target=&#34;_blank&#34;&gt;Getting Started Guide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/vapor-ware/ksync/blob/master/docs/architecture.md&#34; target=&#34;_blank&#34;&gt;How It Works&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.katacoda.com/vaporio/scenarios/ksync&#34; target=&#34;_blank&#34;&gt;Katacoda scenario to try out ksync in your browser&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.syncthing.net/specs/&#34; target=&#34;_blank&#34;&gt;Syncthing Specification&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
## Hands-on walkthroughs
--&gt;

&lt;h2 id=&#34;实践演练&#34;&gt;实践演练&lt;/h2&gt;

&lt;!--
The app we will be using for the hands-on walkthroughs of the tools in the following is a simple [stock market simulator](https://github.com/kubernauts/dok-example-us), consisting of two microservices:
--&gt;

&lt;p&gt;我们接下来用于练习使用工具的应用是一个简单的&lt;a href=&#34;https://github.com/kubernauts/dok-example-us&#34; target=&#34;_blank&#34;&gt;股市模拟器&lt;/a&gt;，包含两个微服务：&lt;/p&gt;

&lt;!--
* The `stock-gen` microservice is written in Go and generates stock data randomly and exposes it via HTTP endpoint `/stockdata`.
‎* A second microservice, `stock-con` is a Node.js app that consumes the stream of stock data from `stock-gen` and provides an aggregation in form of a moving average via the HTTP endpoint `/average/$SYMBOL` as well as a health-check endpoint at `/healthz`.
--&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;stock-gen&lt;/code&gt;（股市数据生成器）微服务是用 Go 编写的，随机生成股票数据并通过 HTTP 端点 &lt;code&gt;/ stockdata&lt;/code&gt; 公开&lt;/li&gt;
&lt;li&gt;第二个微服务，&lt;code&gt;stock-con&lt;/code&gt;（股市数据消费者）是一个 Node.js 应用程序，它使用来自 &lt;code&gt;stock-gen&lt;/code&gt; 的股票数据流，并通过 HTTP 端点 &lt;code&gt;/average/$SYMBOL&lt;/code&gt; 提供股价移动平均线，也提供一个健康检查端点 &lt;code&gt;/healthz&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
Overall, the default setup of the app looks as follows:
--&gt;

&lt;p&gt;总体上，此应用的默认配置如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2018-05-01-developing-on-kubernetes/dok-architecture_preview.png&#34; alt=&#34;Default Setup&#34; /&gt;&lt;/p&gt;

&lt;!--
In the following we’ll do a hands-on walkthrough for a representative selection of tools discussed above: ksync, Minikube with local build, as well as Skaffold. For each of the tools we do the following:
--&gt;

&lt;p&gt;在下文中，我们将选取以上讨论的代表性工具进行实践演练：ksync，具有本地构建的 Minikube 以及 Skaffold。对于每个工具，我们执行以下操作：&lt;/p&gt;

&lt;!--
* Set up the respective tool incl. preparations for the deployment and local consumption of the `stock-con` microservice.
* Perform a code update, that is, change the source code of the `/healthz` endpoint in the `stock-con` microservice and observe the updates.
--&gt;

&lt;ul&gt;
&lt;li&gt;设置相应的工具，包括部署准备和 &lt;code&gt;stock-con&lt;/code&gt; 微服务数据的本地读取&lt;/li&gt;
&lt;li&gt;执行代码更新，即更改 &lt;code&gt;stock-con&lt;/code&gt; 微服务的 &lt;code&gt;/healthz&lt;/code&gt; 端点的源代码并观察网页刷新&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
Note that for the target Kubernetes cluster we’ve been using Minikube locally, but you can also a remote cluster for ksync and Skaffold if you want to follow along.
--&gt;

&lt;p&gt;请注意，我们一直使用 Minikube 的本地 Kubernetes 集群，但是您也可以使用 ksync 和 Skaffold 的远程集群跟随练习。&lt;/p&gt;

&lt;!--
### Walkthrough: ksync
--&gt;

&lt;h3 id=&#34;实践演练-ksync&#34;&gt;实践演练：ksync&lt;/h3&gt;

&lt;!--
As a preparation, install [ksync](https://vapor-ware.github.io/ksync/#installation) and then carry out the following steps to prepare the development setup:
--&gt;

&lt;p&gt;作为准备，安装 &lt;a href=&#34;https://vapor-ware.github.io/ksync/#installation&#34; target=&#34;_blank&#34;&gt;ksync&lt;/a&gt;，然后执行以下步骤配置开发环境：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ mkdir -p $(pwd)/ksync
$ kubectl create namespace dok
$ ksync init -n dok
&lt;/code&gt;&lt;/pre&gt;

&lt;!--
With the basic setup completed we&#39;re ready to tell ksync’s local client to watch a certain Kubernetes namespace and then we create a spec to define what we want to sync (the directory `$(pwd)/ksync` locally with `/app` in the container). Note that target pod is specified via the selector parameter:
--&gt;

&lt;p&gt;完成基本设置后，我们可以告诉 ksync 的本地客户端监控 Kubernetes 的某个命名空间，然后我们创建一个规范来定义我们想要同步的文件夹（本地的 &lt;code&gt;$(pwd)/ksync&lt;/code&gt; 和容器中的 &lt;code&gt;/ app&lt;/code&gt; ）。请注意，目标 pod 是用 selector 参数指定：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ksync watch -n dok
$ ksync create -n dok --selector=app=stock-con $(pwd)/ksync /app
$ ksync get -n dok
&lt;/code&gt;&lt;/pre&gt;

&lt;!--
Now we deploy the stock generator and the stock consumer microservice:
--&gt;

&lt;p&gt;现在我们部署股价数据生成器和股价数据消费者微服务：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl -n=dok apply \
      -f https://raw.githubusercontent.com/kubernauts/dok-example-us/master/stock-gen/app.yaml
$ kubectl -n=dok apply \
      -f https://raw.githubusercontent.com/kubernauts/dok-example-us/master/stock-con/app.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;!--
Once both deployments are created and the pods are running, we forward the `stock-con` service for local consumption (in a separate terminal session):
--&gt;

&lt;p&gt;一旦两个部署建好并且 pod 开始运行，我们转发 &lt;code&gt;stock-con&lt;/code&gt; 服务以供本地读取（另开一个终端窗口）：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get -n dok po --selector=app=stock-con  \
                     -o=custom-columns=:metadata.name --no-headers |  \
                     xargs -IPOD kubectl -n dok port-forward POD 9898:9898
&lt;/code&gt;&lt;/pre&gt;

&lt;!--
With that we should be able to consume the `stock-con` service from our local machine; we do this by regularly checking the response of the `healthz` endpoint like so (in a separate terminal session):
--&gt;

&lt;p&gt;这样，通过定期查询 &lt;code&gt;healthz&lt;/code&gt; 端点，我们就应该能够从本地机器上读取 &lt;code&gt;stock-con&lt;/code&gt; 服务，查询命令如下（在一个单独的终端窗口）：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ watch curl localhost:9898/healthz
&lt;/code&gt;&lt;/pre&gt;

&lt;!--
Now change the code in the `ksync/stock-con`directory, for example update the [`/healthz` endpoint code in `service.js`](https://github.com/kubernauts/dok-example-us/blob/2334ee8fb11f8813370122bd46285cf45bdd4c48/stock-con/service.js#L52) by adding a field to the JSON response and observe how the pod gets updated and the response of the `curl localhost:9898/healthz` command changes. Overall you should have something like the following in the end:
--&gt;

&lt;p&gt;现在，改动 &lt;code&gt;ksync/stock-con&lt;/code&gt; 目录中的代码，例如改动 &lt;a href=&#34;https://github.com/kubernauts/dok-example-us/blob/2334ee8fb11f8813370122bd46285cf45bdd4c48/stock-con/service.js#L52&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;service.js&lt;/code&gt; 中定义的 &lt;code&gt;/healthz&lt;/code&gt; 端点代码&lt;/a&gt;，在其 JSON 形式的响应中新添一个字段并观察 pod 如何更新以及 &lt;code&gt;curl localhost：9898/healthz&lt;/code&gt; 命令的输出发生何种变化。总的来说，您最后应该看到类似的内容：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2018-05-01-developing-on-kubernetes/dok-ksync_preview.png&#34; alt=&#34;Preview&#34; /&gt;&lt;/p&gt;

&lt;!--
### Walkthrough: Minikube with local build
--&gt;

&lt;h3 id=&#34;实践演练-带本地构建的-minikube&#34;&gt;实践演练：带本地构建的 Minikube&lt;/h3&gt;

&lt;!--
For the following you will need to have Minikube up and running and we will leverage the Minikube-internal Docker daemon for building images, locally. As a preparation, do the following
--&gt;

&lt;p&gt;对于以下内容，您需要启动并运行 Minikube，我们将利用 Minikube 自带的 Docker daemon 在本地构建镜像。作为准备，请执行以下操作&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/kubernauts/dok-example-us.git &amp;amp;&amp;amp; cd dok-example-us
$ eval $(minikube docker-env)
$ kubectl create namespace dok
&lt;/code&gt;&lt;/pre&gt;

&lt;!--
Now we deploy the stock generator and the stock consumer microservice:
--&gt;

&lt;p&gt;现在我们部署股价数据生成器和股价数据消费者微服务：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl -n=dok apply -f stock-gen/app.yaml
$ kubectl -n=dok apply -f stock-con/app.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;!--
Once both deployments are created and the pods are running, we forward the `stock-con` service for local consumption (in a separate terminal session):
--&gt;

&lt;p&gt;一旦两个部署建好并且 pod 开始运行，我们转发 &lt;code&gt;stock-con&lt;/code&gt; 服务以供本地读取（另开一个终端窗口）：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get -n dok po --selector=app=stock-con  \
                     -o=custom-columns=:metadata.name --no-headers |  \
                     xargs -IPOD kubectl -n dok port-forward POD 9898:9898 &amp;amp;
$ watch curl localhost:9898/healthz
&lt;/code&gt;&lt;/pre&gt;

&lt;!--
Now change the code in the `stock-con`directory, for example, update the [`/healthz` endpoint code in `service.js`](https://github.com/kubernauts/dok-example-us/blob/2334ee8fb11f8813370122bd46285cf45bdd4c48/stock-con/service.js#L52) by adding a field to the JSON response. Once you’re done with your code update, the last step is to build a new container image and kick off a new deployment like shown below:
--&gt;

&lt;p&gt;现在，改一下 &lt;code&gt;ksync/stock-con&lt;/code&gt; 目录中的代码，例如修改 &lt;a href=&#34;https://github.com/kubernauts/dok-example-us/blob/2334ee8fb11f8813370122bd46285cf45bdd4c48/stock-con/service.js#L52&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;service.js&lt;/code&gt; 中定义的 &lt;code&gt;/healthz&lt;/code&gt; 端点代码&lt;/a&gt;，在其 JSON 形式的响应中添加一个字段。在您更新完代码后，最后一步是构建新的容器镜像并启动新部署，如下所示：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker build -t stock-con:dev -f Dockerfile .
$ kubectl -n dok set image deployment/stock-con *=stock-con:dev
&lt;/code&gt;&lt;/pre&gt;

&lt;!--
Overall you should have something like the following in the end:
--&gt;

&lt;p&gt;总的来说，您最后应该看到类似的内容：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2018-05-01-developing-on-kubernetes/dok-minikube-localdev_preview.png&#34; alt=&#34;Local Preview&#34; /&gt;&lt;/p&gt;

&lt;!--
### Walkthrough: Skaffold
--&gt;

&lt;h3 id=&#34;实践演练-skaffold&#34;&gt;实践演练：Skaffold&lt;/h3&gt;

&lt;!--
To perform this walkthrough you first need to install [Skaffold](https://github.com/GoogleContainerTools/skaffold#installation). Once that is done, you can do the following steps to prepare the development setup:
--&gt;

&lt;p&gt;要进行此演练，首先需要安装 &lt;a href=&#34;https://github.com/GoogleContainerTools/skaffold#installation&#34; target=&#34;_blank&#34;&gt;Skaffold&lt;/a&gt;。完成后，您可以执行以下步骤来配置开发环境：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/kubernauts/dok-example-us.git &amp;amp;&amp;amp; cd dok-example-us
$ kubectl create namespace dok
&lt;/code&gt;&lt;/pre&gt;

&lt;!--
Now we deploy the stock generator (but not the stock consumer microservice, that is done via Skaffold):
--&gt;

&lt;p&gt;现在我们部署股价数据生成器（但是暂不部署股价数据消费者，此服务将使用 Skaffold 完成）：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl -n=dok apply -f stock-gen/app.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;!--
Note that initially we experienced an authentication error when doing `skaffold dev` and needed to apply a fix as described in [Issue 322](https://github.com/GoogleContainerTools/skaffold/issues/322). Essentially it means changing the content of `~/.docker/config.json` to:
--&gt;

&lt;p&gt;请注意，最初我们在执行 &lt;code&gt;skaffold dev&lt;/code&gt; 时发生身份验证错误，为避免此错误需要安装&lt;a href=&#34;https://github.com/GoogleContainerTools/skaffold/issues/322&#34; target=&#34;_blank&#34;&gt;问题322&lt;/a&gt; 中所述的修复。本质上，需要将 &lt;code&gt;〜/.docker/config.json&lt;/code&gt; 的内容改为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
   &amp;quot;auths&amp;quot;: {}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;!--
Next, we had to patch `stock-con/app.yaml` slightly to make it work with Skaffold:
--&gt;

&lt;p&gt;接下来，我们需要略微改动 &lt;code&gt;stock-con/app.yaml&lt;/code&gt;，这样 Skaffold 才能正常使用此文件：&lt;/p&gt;

&lt;!--
Add a `namespace` field to both the `stock-con` deployment and the service with the value of `dok`.
Change the `image` field of the container spec to `quay.io/mhausenblas/stock-con` since Skaffold manages the container image tag on the fly.
--&gt;

&lt;p&gt;在 &lt;code&gt;stock-con&lt;/code&gt; 部署和服务中添加一个 &lt;code&gt;namespace&lt;/code&gt; 字段，其值为 &lt;code&gt;dok&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;将容器规范的 &lt;code&gt;image&lt;/code&gt; 字段更改为 &lt;code&gt;quay.io/mhausenblas/stock-con&lt;/code&gt;，因为 Skaffold 可以即时管理容器镜像标签。&lt;/p&gt;

&lt;!--
 The resulting `app.yaml` file stock-con looks as follows:
 --&gt;

&lt;p&gt;最终的 stock-con 的 &lt;code&gt;app.yaml&lt;/code&gt; 文件看起来如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1beta1
kind: Deployment
metadata:
  labels:
    app: stock-con
  name: stock-con
  namespace: dok
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: stock-con
    spec:
      containers:
      - name: stock-con
        image: quay.io/mhausenblas/stock-con
        env:
        - name: DOK_STOCKGEN_HOSTNAME
          value: stock-gen
        - name: DOK_STOCKGEN_PORT
          value: &amp;quot;9999&amp;quot;
        ports:
        - containerPort: 9898
          protocol: TCP
        livenessProbe:
          initialDelaySeconds: 2
          periodSeconds: 5
          httpGet:
            path: /healthz
            port: 9898
        readinessProbe:
          initialDelaySeconds: 2
          periodSeconds: 5
          httpGet:
            path: /healthz
            port: 9898
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: stock-con
  name: stock-con
  namespace: dok
spec:
  type: ClusterIP
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 9898
  selector:
    app: stock-con
&lt;/code&gt;&lt;/pre&gt;

&lt;!--
The final step before we can start development is to configure Skaffold. So, create a file `skaffold.yaml` in the `stock-con/` directory with the following content:
 --&gt;

&lt;p&gt;我们能够开始开发之前的最后一步是配置 Skaffold。因此，在 &lt;code&gt;stock-con/&lt;/code&gt; 目录中创建文件 &lt;code&gt;skaffold.yaml&lt;/code&gt;，其中包含以下内容：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: skaffold/v1alpha2
kind: Config
build:
  artifacts:
  - imageName: quay.io/mhausenblas/stock-con
    workspace: .
    docker: {}
  local: {}
deploy:
  kubectl:
    manifests:
      - app.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;!--
Now we’re ready to kick off the development. For that execute the following in the `stock-con/` directory:
 --&gt;

&lt;p&gt;现在我们准备好开始开发了。为此，在 &lt;code&gt;stock-con/&lt;/code&gt; 目录中执行以下命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ skaffold dev
&lt;/code&gt;&lt;/pre&gt;

&lt;!--
Above command triggers a build of the `stock-con` image and then a deployment. Once the pod of the `stock-con` deployment is running, we again forward the `stock-con` service for local consumption (in a separate terminal session) and check the response of the `healthz` endpoint:
 --&gt;

&lt;p&gt;上面的命令将触发 &lt;code&gt;stock-con&lt;/code&gt; 图像的构建和部署。一旦 &lt;code&gt;stock-con&lt;/code&gt; 部署的 pod 开始运行，我们再次转发 &lt;code&gt;stock-con&lt;/code&gt; 服务以供本地读取（在单独的终端窗口中）并检查 &lt;code&gt;healthz&lt;/code&gt; 端点的响应：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ kubectl get -n dok po --selector&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b8860b&#34;&gt;app&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;stock-con  &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;                     -o&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;custom-columns&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;:metadata.name --no-headers |  &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;                     xargs -IPOD kubectl -n dok port-forward POD &lt;span style=&#34;color:#666&#34;&gt;9898&lt;/span&gt;:9898 &amp;amp;
$ watch curl localhost:9898/healthz&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;!--
If you now change the code in the `stock-con`directory, for example, by updating the [`/healthz` endpoint code in `service.js`](https://github.com/kubernauts/dok-example-us/blob/2334ee8fb11f8813370122bd46285cf45bdd4c48/stock-con/service.js#L52) by adding a field to the JSON response, you should see Skaffold noticing the change and create a new image as well as deploy it. The resulting screen would look something like this:
 --&gt;

&lt;p&gt;现在，如果您修改一下 &lt;code&gt;stock-con&lt;/code&gt; 目录中的代码，例如 &lt;a href=&#34;https://github.com/kubernauts/dok-example-us/blob/2334ee8fb11f8813370122bd46285cf45bdd4c48/stock-con/service.js#L52&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;service.js&lt;/code&gt; 中定义的 &lt;code&gt;/healthz&lt;/code&gt; 端点代码&lt;/a&gt;，在其 JSON 形式的响应中添加一个字段，您应该看到 Skaffold 可以检测到代码改动并创建新图像以及部署它。您的屏幕看起来应该类似这样：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2018-05-01-developing-on-kubernetes/dok-skaffold_preview.png&#34; alt=&#34;Skaffold Preview&#34; /&gt;
&lt;!--
By now you should have a feeling how different tools enable you to develop apps on Kubernetes and if you’re interested to learn more about tools and or methods, check out the following resources:
 --&gt;&lt;/p&gt;

&lt;p&gt;至此，您应该对不同的工具如何帮您在 Kubernetes 上开发应用程序有了一定的概念，如果您有兴趣了解有关工具和/或方法的更多信息，请查看以下资源：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Blog post by Shahidh K Muhammed on &lt;a href=&#34;https://blog.hasura.io/draft-vs-gitkube-vs-helm-vs-ksonnet-vs-metaparticle-vs-skaffold-f5aa9561f948&#34; target=&#34;_blank&#34;&gt;Draft vs Gitkube vs Helm vs Ksonnet vs Metaparticle vs Skaffold&lt;/a&gt; (03/2018)&lt;/li&gt;
&lt;li&gt;Blog post by Gergely Nemeth on &lt;a href=&#34;https://nemethgergely.com/using-kubernetes-for-local-development/index.html&#34; target=&#34;_blank&#34;&gt;Using Kubernetes for Local Development&lt;/a&gt;, with a focus on Skaffold (03/2018)&lt;/li&gt;
&lt;li&gt;Blog post by Richard Li on &lt;a href=&#34;https://hackernoon.com/locally-developing-kubernetes-services-without-waiting-for-a-deploy-f63995de7b99&#34; target=&#34;_blank&#34;&gt;Locally developing Kubernetes services (without waiting for a deploy)&lt;/a&gt;, with a focus on Telepresence&lt;/li&gt;
&lt;li&gt;Blog post by Abhishek Tiwari on &lt;a href=&#34;https://abhishek-tiwari.com/local-development-environment-for-kubernetes-using-minikube/&#34; target=&#34;_blank&#34;&gt;Local Development Environment for Kubernetes using Minikube&lt;/a&gt; (09/2017)&lt;/li&gt;
&lt;li&gt;Blog post by Aymen El Amri on &lt;a href=&#34;https://medium.com/devopslinks/using-kubernetes-minikube-for-local-development-c37c6e56e3db&#34; target=&#34;_blank&#34;&gt;Using Kubernetes for Local Development — Minikube&lt;/a&gt; (08/2017)&lt;/li&gt;
&lt;li&gt;Blog post by Alexis Richardson on &lt;a href=&#34;https://www.weave.works/blog/gitops-operations-by-pull-request&#34; target=&#34;_blank&#34;&gt;​GitOps - Operations by Pull Request&lt;/a&gt; (08/2017)&lt;/li&gt;
&lt;li&gt;Slide deck &lt;a href=&#34;https://docs.google.com/presentation/d/1d3PigRVt_m5rO89Ob2XZ16bW8lRSkHHH5k816-oMzZo/&#34; target=&#34;_blank&#34;&gt;GitOps: Drive operations through git&lt;/a&gt;, with a focus on Gitkube by Tirumarai Selvan (03/2018)&lt;/li&gt;
&lt;li&gt;Slide deck &lt;a href=&#34;https://speakerdeck.com/mhausenblas/developing-apps-on-kubernetes&#34; target=&#34;_blank&#34;&gt;Developing apps on Kubernetes&lt;/a&gt;, a talk Michael Hausenblas gave at a CNCF Paris meetup  (04/2018)&lt;/li&gt;
&lt;li&gt;YouTube videos:

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=QW85Y0Ug3KY&#34; target=&#34;_blank&#34;&gt;TGI Kubernetes 029: Developing Apps with Ksync&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=McwwWhCXMxc&#34; target=&#34;_blank&#34;&gt;TGI Kubernetes 030: Exploring Skaffold&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=zezeBAJ_3w8&#34; target=&#34;_blank&#34;&gt;TGI Kubernetes 031: Connecting with Telepresence&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=8B1D7cTMPgA&#34; target=&#34;_blank&#34;&gt;TGI Kubernetes 033: Developing with Draft&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Raw responses to the &lt;a href=&#34;https://docs.google.com/spreadsheets/d/12ilRCly2eHKPuicv1P_BD6z__PXAqpiaR-tDYe2eudE/edit&#34; target=&#34;_blank&#34;&gt;Kubernetes Application Survey&lt;/a&gt; 2018 by SIG Apps&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
With that we wrap up this post on how to go about developing apps on Kubernetes, we hope you learned something and if you have feedback and/or want to point out a tool that you found useful, please let us know via Twitter: [Ilya](https://twitter.com/errordeveloper) and [Michael](https://twitter.com/mhausenblas).
--&gt;

&lt;p&gt;有了这些，我们这篇关于如何在 Kubernetes 上开发应用程序的博客就可以收尾了，希望您有所收获，如果您有反馈和/或想要指出您认为有用的工具，请通过 Twitter 告诉我们：&lt;a href=&#34;https://twitter.com/errordeveloper&#34; target=&#34;_blank&#34;&gt;Ilya&lt;/a&gt; 和 &lt;a href=&#34;https://twitter.com/mhausenblas&#34; target=&#34;_blank&#34;&gt;Michael&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog:  Kubernetes 中自动缩放 </title>
      <link>https://kubernetes.io/zh/blog/2017/11/17/autoscaling-in-kubernetes/</link>
      <pubDate>Fri, 17 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/zh/blog/2017/11/17/autoscaling-in-kubernetes/</guid>
      <description>
        
        
        &lt;!--
---
title: &#34; Autoscaling in Kubernetes &#34;
date: 2017-11-17
slug: autoscaling-in-kubernetes
url: /blog/2017/11/Autoscaling-In-Kubernetes
---
--&gt;

&lt;!--
Kubernetes allows developers to automatically adjust cluster sizes and the number of pod replicas based on current traffic and load. These adjustments reduce the amount of unused nodes, saving money and resources. In this talk, Marcin Wielgus of Google walks you through the current state of pod and node autoscaling in Kubernetes: .how it works, and how to use it, including best practices for deployments in production applications.
--&gt;

&lt;p&gt;Kubernetes 允许开发人员根据当前的流量和负载自动调整集群大小和 pod 副本的数量。这些调整减少了未使用节点的数量，节省了资金和资源。
在这次演讲中，谷歌的 Marcin Wielgus 将带领您了解 Kubernetes 中 pod 和 node 自动调焦的当前状态：它是如何工作的，以及如何使用它，包括在生产应用程序中部署的最佳实践。&lt;/p&gt;

&lt;!--
Enjoyed this talk? Join us for more exciting sessions on scaling and automating your Kubernetes clusters at KubeCon in Austin on December 6-8. [Register Now](https://www.eventbrite.com/e/kubecon-cloudnativecon-north-america-registration-37824050754?_ga=2.9666039.317115486.1510003873-1623727562.1496428006)
--&gt;

&lt;p&gt;喜欢这个演讲吗？ 12 月 6 日至 8 日，在 Austin 参加 KubeCon 关于扩展和自动化您的 Kubernetes 集群的更令人兴奋的会议。&lt;a href=&#34;https://www.eventbrite.com/e/kubecon-cloudnativecon-north-america-registration-37824050754?_ga=2.9666039.317115486.1510003873-1623727562.1496428006&#34; target=&#34;_blank&#34;&gt;现在注册&lt;/a&gt;。&lt;/p&gt;

&lt;!--
Be sure to check out [Automating and Testing Production Ready Kubernetes Clusters in the Public Cloud](http://sched.co/CU64) by Ron Lipke, Senior Developer, Platform as a Service, Gannet/USA Today Network.
--&gt;

&lt;p&gt;一定要查看由 Ron Lipke， Gannet/USA Today Network, 平台即服务高级开发人员，在&lt;a href=&#34;http://sched.co/CU64&#34; target=&#34;_blank&#34;&gt;公共云中自动化和测试产品就绪的 Kubernetes 集群&lt;/a&gt;。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog:  Kubernetes 1.8 的五天 </title>
      <link>https://kubernetes.io/zh/blog/2017/10/24/five-days-of-kubernetes-18/</link>
      <pubDate>Tue, 24 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/zh/blog/2017/10/24/five-days-of-kubernetes-18/</guid>
      <description>
        
        
        &lt;!--
---
title: &#34; Five Days of Kubernetes 1.8 &#34;
date: 2017-10-24
slug: five-days-of-kubernetes-18
url: /blog/2017/10/Five-Days-Of-Kubernetes-18
---
--&gt;

&lt;!--
Kubernetes 1.8 is live, made possible by hundreds of contributors pushing thousands of commits in this latest releases.
--&gt;

&lt;p&gt;Kubernetes 1.8 已经推出，数百名贡献者在这个最新版本中推出了成千上万的提交。&lt;/p&gt;

&lt;!--
The community has tallied more than 66,000 commits in the main repo and continues rapid growth outside of the main repo, which signals growing maturity and stability for the project. The community has logged more than 120,000 commits across all repos and 17,839 commits across all repos for v1.7.0 to v1.8.0 alone.
--&gt;

&lt;p&gt;社区已经有超过 66,000 个提交在主仓库，并在主仓库之外继续快速增长，这标志着该项目日益成熟和稳定。仅 v1.7.0 到 v1.8.0，社区就记录了所有仓库的超过 120,000 次提交和 17839 次提交。&lt;/p&gt;

&lt;!--
With the help of our growing community of 1,400 plus contributors, we issued more than 3,000 PRs and pushed more than 5,000 commits to deliver Kubernetes 1.8 with significant security and workload support updates. This all points to increased stability, a result of our project-wide focus on maturing [process](https://github.com/kubernetes/sig-release), formalizing [architecture](https://github.com/kubernetes/community/tree/master/sig-architecture), and strengthening Kubernetes’ [governance model](https://github.com/kubernetes/community/tree/master/community/elections/2017).
--&gt;

&lt;p&gt;在拥有 1400 多名贡献者，并且不断发展壮大的社区的帮助下，我们合并了 3000 多个 PR，并发布了 5000 多个提交，最后的 Kubernetes 1.8 在安全和工作负载方面添加了很多的更新。
这一切都表明稳定性的提高，这是我们整个项目关注成熟&lt;a href=&#34;https://github.com/kubernetes/sig-release&#34; target=&#34;_blank&#34;&gt;流程&lt;/a&gt;、形式化&lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-architecture&#34; target=&#34;_blank&#34;&gt;架构&lt;/a&gt;和加强 Kubernetes 的&lt;a href=&#34;https://github.com/kubernetes/community/tree/master/community/elections/2017&#34; target=&#34;_blank&#34;&gt;治理模型&lt;/a&gt;的结果。&lt;/p&gt;

&lt;!--
While many improvements have been contributed, we highlight key features in this series of in-depth&amp;nbsp;posts listed below. [Follow along](https://twitter.com/kubernetesio) and see what’s new and improved with storage, security and more.
--&gt;

&lt;p&gt;虽然有很多改进，但我们在下面列出的这一系列深度文章中突出了一些关键特性。&lt;a href=&#34;https://twitter.com/kubernetesio&#34; target=&#34;_blank&#34;&gt;跟随&lt;/a&gt;并了解存储，安全等方面的新功能和改进功能。&lt;/p&gt;

&lt;!--
**Day 1:** [5 Days of Kubernetes 1.8](https://kubernetes.io/blog/2017/10/five-days-of-kubernetes-18)
**Day 2:** [kubeadm v1.8 Introduces Easy Upgrades for Kubernetes Clusters](https://kubernetes.io/blog/2017/10/kubeadm-v18-released)
**Day 3:** [Kubernetes v1.8 Retrospective: It Takes a Village to Raise a Kubernetes](https://kubernetes.io/blog/2017/10/it-takes-village-to-raise-kubernetes)
**Day 4:** [Using RBAC, Generally Available in Kubernetes v1.8](https://kubernetes.io/blog/2017/10/using-rbac-generally-available-18)
**Day 5:** [Enforcing Network Policies in Kubernetes](https://kubernetes.io/blog/2017/10/enforcing-network-policies-in-kubernetes)
--&gt;

&lt;p&gt;&lt;strong&gt;第一天：&lt;/strong&gt; &lt;a href=&#34;https://kubernetes.io/blog/2017/10/five-days-of-kubernetes-18&#34; target=&#34;_blank&#34;&gt;Kubernetes 1.8 的五天&lt;/a&gt;
&lt;strong&gt;第二天：&lt;/strong&gt; &lt;a href=&#34;https://kubernetes.io/blog/2017/10/kubeadm-v18-released&#34; target=&#34;_blank&#34;&gt;kubeadm v1.8 为 Kubernetes 集群引入了简单的升级&lt;/a&gt;
&lt;strong&gt;第三天：&lt;/strong&gt; &lt;a href=&#34;https://kubernetes.io/blog/2017/10/it-takes-village-to-raise-kubernetes&#34; target=&#34;_blank&#34;&gt;Kubernetes v1.8 回顾：提升一个 Kubernetes 需要一个 Village&lt;/a&gt;
&lt;strong&gt;第四天：&lt;/strong&gt; &lt;a href=&#34;https://kubernetes.io/blog/2017/10/using-rbac-generally-available-18&#34; target=&#34;_blank&#34;&gt;使用 RBAC，一般在 Kubernetes v1.8 中提供&lt;/a&gt;
&lt;strong&gt;第五天：&lt;/strong&gt; &lt;a href=&#34;https://kubernetes.io/blog/2017/10/enforcing-network-policies-in-kubernetes&#34; target=&#34;_blank&#34;&gt;在 Kubernetes 执行网络策略&lt;/a&gt;&lt;/p&gt;

&lt;!--
**Connect**
--&gt;

&lt;p&gt;&lt;strong&gt;链接&lt;/strong&gt;&lt;/p&gt;

&lt;!--
- Post questions (or answer questions) on [Stack Overflow](http://stackoverflow.com/questions/tagged/kubernetes)
- Join the community portal for advocates on [K8sPort](http://k8sport.org/)
- Follow us on Twitter [@Kubernetesio](https://twitter.com/kubernetesio) for latest updates&amp;nbsp;
- Connect with the community on [Slack](http://slack.k8s.io/)
- Get involved with the Kubernetes project on [GitHub](https://github.com/kubernetes/kubernetes)
--&gt;

&lt;ul&gt;
&lt;li&gt;在 &lt;a href=&#34;http://stackoverflow.com/questions/tagged/kubernetes&#34; target=&#34;_blank&#34;&gt;Stack Overflow&lt;/a&gt; 上发布问题（或回答问题）&lt;/li&gt;
&lt;li&gt;加入 &lt;a href=&#34;http://k8sport.org/&#34; target=&#34;_blank&#34;&gt;K8sPort&lt;/a&gt; 布道师的社区门户网站&lt;/li&gt;
&lt;li&gt;在 Twitter &lt;a href=&#34;https://twitter.com/kubernetesio&#34; target=&#34;_blank&#34;&gt;@Kubernetesio&lt;/a&gt; 关注我们以获取最新更新&lt;/li&gt;
&lt;li&gt;与 &lt;a href=&#34;http://slack.k8s.io/&#34; target=&#34;_blank&#34;&gt;Slack&lt;/a&gt; 上的社区联系&lt;/li&gt;
&lt;li&gt;参与 &lt;a href=&#34;https://github.com/kubernetes/kubernetes&#34; target=&#34;_blank&#34;&gt;GitHub&lt;/a&gt; 上的 Kubernetes 项目&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog:  Kubernetes 生日快乐。哦，这是你要去的地方！ </title>
      <link>https://kubernetes.io/zh/blog/2016/07/21/oh-the-places-you-will-go/</link>
      <pubDate>Thu, 21 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/zh/blog/2016/07/21/oh-the-places-you-will-go/</guid>
      <description>
        
        
        &lt;!--
---
title: &#34; Happy Birthday Kubernetes. Oh, the places you’ll go! &#34;
date: 2016-07-21
slug: oh-the-places-you-will-go
url: /blog/2016/07/Oh-The-Places-You-Will-Go
---
--&gt;

&lt;!--
_Editor’s note, Today’s guest post is from an independent Kubernetes contributor, Justin Santa Barbara, sharing his reflection on growth of the project from inception to its future._

**Dear K8s,**

_It’s hard to believe you’re only one - you’ve grown up so fast. On the occasion of your first birthday, I thought I would write a little note about why I was so excited when you were born, why I feel fortunate to be part of the group that is raising you, and why I’m eager to watch you continue to grow up!_
--&gt;

&lt;p&gt;&lt;em&gt;编者按，今天的嘉宾帖子来自一位独立的 kubernetes 撰稿人 Justin Santa Barbara，分享了他对项目从一开始到未来发展的思考。&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;亲爱的 K8s,&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;很难相信你是唯一的一个 - 成长这么快的。在你一岁生日的时候，我想我可以写一个小纸条，告诉你为什么我在你出生的时候那么兴奋，为什么我觉得很幸运能成为抚养你长大的一员，为什么我渴望看到你继续成长！&lt;/em&gt;&lt;/p&gt;

&lt;!--
_--Justin_

You started with an excellent foundation - good declarative functionality, built around a solid API with a well defined schema and the machinery so that we could evolve going forwards. And sure enough, over your first year you grew so fast: autoscaling, HTTP load-balancing support (Ingress), support for persistent workloads including clustered databases (PetSets). You’ve made friends with more clouds (welcome Azure &amp; OpenStack to the family), and even started to span zones and clusters (Federation). And these are just some of the most visible changes - there’s so much happening inside that brain of yours!

I think it’s wonderful you’ve remained so open in all that you do - you seem to write down everything on GitHub - for better or worse. I think we’ve all learned a lot about that on the way, like the perils of having engineers make scaling statements that are then weighed against claims made without quite the same framework of precision and rigor. But I’m proud that you chose not to lower your standards, but rose to the challenge and just ran faster instead - it might not be the most realistic approach, but it is the only way to move mountains!
--&gt;

&lt;p&gt;&lt;em&gt;&amp;ndash;Justin&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;你从一个优秀的基础 - 良好的声明性功能开始，它是围绕一个具有良好定义的模式和机制的坚实的 API 构建的，这样我们就可以向前发展了。果然，在你的第一年里，你增长得如此之快：autoscaling、HTTP load-balancing support (Ingress)、support for persistent workloads including clustered databases (PetSets)。你已经和更多的云交了朋友(欢迎 azure 和 openstack 加入家庭)，甚至开始跨越区域和集群(Federation)。这些只是一些最明显的变化 - 在你的大脑里发生了太多的变化！&lt;/p&gt;

&lt;p&gt;我觉得你一直保持开放的态度真是太好了 - 你好像把所有的东西都写在 github 上 - 不管是好是坏。我想我们在这方面都学到了很多，比如让工程师做缩放声明的风险，然后在没有完全相同的精确性和严谨性框架的情况下，将这些声明与索赔进行权衡。但我很自豪你选择了不降低你的标准，而是上升到挑战，只是跑得更快 - 这可能不是最现实的办法，但这是唯一的方式能移动山！&lt;/p&gt;

&lt;!--
And yet, somehow, you’ve managed to avoid a lot of the common dead-ends that other open source software has fallen into, particularly as those projects got bigger and the developers end up working on it more than they use it directly. How did you do that? There’s a probably-apocryphal story of an employee at IBM that makes a huge mistake, and is summoned to meet with the big boss, expecting to be fired, only to be told “We just spent several million dollars training you. Why would we want to fire you?”. Despite all the investment google is pouring into you (along with Redhat and others), I sometimes wonder if the mistakes we are avoiding could be worth even more. There is a very open development process, yet there’s also an “oracle” that will sometimes course-correct by telling us what happens two years down the road if we make a particular design decision. This is a parent you should probably listen to!
--&gt;

&lt;p&gt;然而，不知何故，你已经设法避免了许多其他开源软件陷入的共同死胡同，特别是当那些项目越来越大，开发人员最终要做的比直接使用它更多的时候。你是怎么做到的？有一个很可能是虚构的故事，讲的是 IBM 的一名员工犯了一个巨大的错误，被传唤去见大老板，希望被解雇，却被告知“我们刚刚花了几百万美元培训你。我们为什么要解雇你？“。尽管谷歌对你进行了大量的投资(包括 redhat 和其他公司)，但我有时想知道，我们正在避免的错误是否更有价值。有一个非常开放的开发过程，但也有一个“oracle”，它有时会通过告诉我们两年后如果我们做一个特定的设计决策会发生什么来纠正错误。这是你应该听的父母！&lt;/p&gt;

&lt;!--
And so although you’re only a year old, you really have an [old soul](http://queue.acm.org/detail.cfm?id=2898444). I’m just one of the [many people raising you](https://kubernetes.io/blog/2016/07/happy-k8sbday-1), but it’s a wonderful learning experience for me to be able to work with the people that have built these incredible systems and have all this domain knowledge. Yet because we started from scratch (rather than taking the existing Borg code) we’re at the same level and can still have genuine discussions about how to raise you. Well, at least as close to the same level as we could ever be, but it’s to their credit that they are all far too nice ever to mention it!

If I would pick just two of the wise decisions those brilliant people made:
--&gt;

&lt;p&gt;所以，尽管你只有一岁，你真的有一个&lt;a href=&#34;http://queue.acm.org/detail.cfm?ID=2898444&#34; target=&#34;_blank&#34;&gt;旧灵魂&lt;/a&gt;。我只是&lt;a href=&#34;https://kubernetes.io/blog/2016/07/happy-k8sbday-1&#34; target=&#34;_blank&#34;&gt;很多人抚养你&lt;/a&gt;中的一员，但对我来说，能够与那些建立了这些令人难以置信的系统并拥有所有这些领域知识的人一起工作是一次极好的学习经历。然而，因为我们是白手起家(而不是采用现有的 Borg 代码)，我们处于同一水平，仍然可以就如何培养你进行真正的讨论。好吧，至少和我们的水平一样接近，但值得称赞的是，他们都太好了，从来没提过！&lt;/p&gt;

&lt;p&gt;如果我选择两个聪明人做出的明智决定：&lt;/p&gt;

&lt;!--
- Labels &amp; selectors give us declarative “pointers”, so we can say “why” we want things, rather than listing the things directly. It’s the secret to how you can scale to [great heights](https://kubernetes.io/blog/2016/07/thousand-instances-of-cassandra-using-kubernetes-pet-set); not by naming each step, but saying “a thousand more steps just like that first one”.
- Controllers are state-synchronizers: we specify the goals, and your controllers will indefatigably work to bring the system to that state. They work through that strongly-typed API foundation, and are used throughout the code, so Kubernetes is more of a set of a hundred small programs than one big one. It’s not enough to scale to thousands of nodes technically; the project also has to scale to thousands of developers and features; and controllers help us get there.
--&gt;

&lt;ul&gt;
&lt;li&gt;标签和选择器给我们声明性的“pointers”，所以我们可以说“为什么”我们想要东西，而不是直接列出东西。这是如何扩展到[伟大高度]的秘密(&lt;a href=&#34;https://kubernetes.io/blog/2016/07/thousand-instances-of-cassandra-using-kubernetes-pet-set)；不是命名每一步，而是说“像第一步一样多走一千步”。&#34; target=&#34;_blank&#34;&gt;https://kubernetes.io/blog/2016/07/thousand-instances-of-cassandra-using-kubernetes-pet-set)；不是命名每一步，而是说“像第一步一样多走一千步”。&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;控制器是状态同步器：我们指定目标，您的控制器将不遗余力地工作，使系统达到该状态。它们工作在强类型 API 基础上，并且贯穿整个代码，因此 Kubernetes 比一个大的程序多一百个小程序。仅仅从技术上扩展到数千个节点是不够的；这个项目还必须扩展到数千个开发人员和特性；控制器帮助我们达到目的。&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
And so on we will go! We’ll be replacing those controllers and building on more, and the API-foundation lets us build anything we can express in that way - with most things just a label or annotation away! But your thoughts will not be defined by language: with third party resources you can express anything you choose. Now we can build Kubernetes without building in Kubernetes, creating things that feel as much a part of Kubernetes as anything else. Many of the recent additions, like ingress, DNS integration, autoscaling and network policies were done or could be done in this way. Eventually it will be hard to imagine you before these things, but tomorrow’s standard functionality can start today, with no obstacles or gatekeeper, maybe even for an audience of one.

So I’m looking forward to seeing more and more growth happen further and further from the core of Kubernetes. We had to work our way through those phases; starting with things that needed to happen in the kernel of Kubernetes - like replacing replication controllers with deployments. Now we’re starting to build things that don’t require core changes. But we’re still still talking about infrastructure separately from applications. It’s what comes next that gets really interesting: when we start building applications that rely on the Kubernetes APIs. We’ve always had the Cassandra example that uses the Kubernetes API to self-assemble, but we haven’t really even started to explore this more widely yet. In the same way that the S3 APIs changed how we build things that remember, I think the k8s APIs are going to change how we build things that think.
--&gt;

&lt;p&gt;等等我们就走！我们将取代那些控制器，建立更多，API 基金会让我们构建任何我们可以用这种方式表达的东西 - 大多数东西只是标签或注释远离！但你的思想不会由语言来定义：有了第三方资源，你可以表达任何你选择的东西。现在我们可以不用在 Kubernetes 建造Kubernetes 了，创造出与其他任何东西一样感觉是 Kubernetes 的一部分的东西。最近添加的许多功能，如ingress、DNS integration、autoscaling and network policies ，都已经完成或可以通过这种方式完成。最终，在这些事情发生之前很难想象你会是怎样的一个人，但是明天的标准功能可以从今天开始，没有任何障碍或看门人，甚至对一个听众来说也是这样。&lt;/p&gt;

&lt;p&gt;所以我期待着看到越来越多的增长发生在离 Kubernetes 核心越来越远的地方。我们必须通过这些阶段来工作；从需要在 kubernetes 内核中发生的事情开始——比如用部署替换复制控制器。现在我们开始构建不需要核心更改的东西。但我们仍然在讨论基础设施和应用程序。接下来真正有趣的是：当我们开始构建依赖于 kubernetes api 的应用程序时。我们一直有使用 kubernetes api 进行自组装的 cassandra 示例，但我们还没有真正开始更广泛地探讨这个问题。正如 S3 APIs 改变了我们构建记忆事物的方式一样，我认为 k8s APIs 也将改变我们构建思考事物的方式。&lt;/p&gt;

&lt;!--
So I’m looking forward to your second birthday: I can try to predict what you’ll look like then, but I know you’ll surpass even the most audacious things I can imagine. Oh, the places you’ll go!


_-- Justin Santa Barbara, Independent Kubernetes Contributor_
--&gt;

&lt;p&gt;所以我很期待你的二岁生日：我可以试着预测你那时的样子，但我知道你会超越我所能想象的最大胆的东西。哦，这是你要去的地方！&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&amp;ndash; Justin Santa Barbara, 独立的 Kubernetes 贡献者&lt;/em&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog:  Dashboard - Kubernetes 的全功能 Web 界面 </title>
      <link>https://kubernetes.io/zh/blog/2016/07/15/dashboard-web-interface-for-kubernetes/</link>
      <pubDate>Fri, 15 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/zh/blog/2016/07/15/dashboard-web-interface-for-kubernetes/</guid>
      <description>
        
        
        &lt;!--
---
title: &#34; Dashboard - Full Featured Web Interface for Kubernetes &#34;
date: 2016-07-15
slug: dashboard-web-interface-for-kubernetes
url: /blog/2016/07/Dashboard-Web-Interface-For-Kubernetes
---
--&gt;

&lt;!--
_Editor’s note: this post is part of a [series of in-depth articles](https://kubernetes.io/blog/2016/07/five-days-of-kubernetes-1-3) on what&#39;s new in Kubernetes 1.3_

[Kubernetes Dashboard](http://github.com/kubernetes/dashboard) is a project that aims to bring a general purpose monitoring and operational web interface to the Kubernetes world.&amp;nbsp;Three months ago we [released](https://kubernetes.io/blog/2016/04/building-awesome-user-interfaces-for-kubernetes) the first production ready version, and since then the dashboard has made massive improvements. In a single UI, you’re able to perform majority of possible interactions with your Kubernetes clusters without ever leaving your browser. This blog post breaks down new features introduced in the latest release and outlines the roadmap for the future.&amp;nbsp;
--&gt;

&lt;p&gt;&lt;em&gt;编者按：这篇文章是&lt;a href=&#34;https://kubernetes.io/blog/2016/07/five-days-of-kubernetes-1-3&#34; target=&#34;_blank&#34;&gt;一系列深入的文章&lt;/a&gt; 中关于Kubernetes 1.3的新内容的一部分&lt;/em&gt;
&lt;a href=&#34;http://github.com/kubernetes/dashboard&#34; target=&#34;_blank&#34;&gt;Kubernetes Dashboard&lt;/a&gt;是一个旨在为 Kubernetes 世界带来通用监控和操作 Web 界面的项目。三个月前，我们&lt;a href=&#34;https://kubernetes.io/blog/2016/04/building-awesome-user-interfaces-for-kubernetes&#34; target=&#34;_blank&#34;&gt;发布&lt;/a&gt;第一个面向生产的版本，从那时起 dashboard 已经做了大量的改进。在一个 UI 中，您可以在不离开浏览器的情况下，与 Kubernetes 集群执行大多数可能的交互。这篇博客文章分解了最新版本中引入的新功能，并概述了未来的路线图。&lt;/p&gt;

&lt;!--
**Full-Featured Dashboard**

Thanks to a large number of contributions from the community and project members, we were able to deliver many new features for [Kubernetes 1.3 release](https://kubernetes.io/blog/2016/07/kubernetes-1.3-bridging-cloud-native-and-enterprise-workloads). We have been carefully listening to all the great feedback we have received from our users (see the [summary infographics](http://static.lwy.io/img/kubernetes_dashboard_infographic.png)) and addressed the highest priority requests and pain points.
--&gt;

&lt;p&gt;&lt;strong&gt;全功能的 Dashboard&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;由于社区和项目成员的大量贡献，我们能够为&lt;a href=&#34;https://kubernetes.io/blog/2016/07/kubernetes-1-3-bridging-cloud-native-and-enterprise-workloads/&#34; target=&#34;_blank&#34;&gt;Kubernetes 1.3发行版&lt;/a&gt;提供许多新功能。我们一直在认真听取用户的反馈(参见&lt;a href=&#34;http://static.lwy.io/img/kubernetes_dashboard_infographic.png&#34; target=&#34;_blank&#34;&gt;摘要信息图表&lt;/a&gt;)，并解决了最高优先级的请求和难点。
&amp;ndash;&amp;gt;&lt;/p&gt;

&lt;!--
The Dashboard UI now handles all workload resources. This means that no matter what workload type you run, it is visible in the web interface and you can do operational changes on it. For example, you can modify your stateful MySQL installation with [Pet Sets](/docs/user-guide/petset/), do a rolling update of your web server with Deployments or install cluster monitoring with DaemonSets.&amp;nbsp;



 [![](https://lh3.googleusercontent.com/p9bMGxPx4jE6_Z2KB-MktmyuAxyFst-bEk29M_Bn0Bj5ul7uzinH6u5WjHsMmqhGvBwlABZt06dwQ5qkBZiLq_EM1oddCmpwChvXDNXZypaS5l8uzkKuZj3PBUmzTQT4dgDxSXgz) ](https://lh3.googleusercontent.com/p9bMGxPx4jE6_Z2KB-MktmyuAxyFst-bEk29M_Bn0Bj5ul7uzinH6u5WjHsMmqhGvBwlABZt06dwQ5qkBZiLq_EM1oddCmpwChvXDNXZypaS5l8uzkKuZj3PBUmzTQT4dgDxSXgz)
--&gt;

&lt;p&gt;Dashboard UI 现在处理所有工作负载资源。这意味着无论您运行什么工作负载类型，它都在 web 界面中可见，并且您可以对其进行操作更改。例如，可以使用&lt;a href=&#34;https://kubernetes.io/docs/user-guide/petset/&#34;&gt;Pet Sets&lt;/a&gt;修改有状态的 mysql 安装，使用部署对 web 服务器进行滚动更新，或使用守护程序安装群集监视。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://lh3.googleusercontent.com/p9bMGxPx4jE6_Z2KB-MktmyuAxyFst-bEk29M_Bn0Bj5ul7uzinH6u5WjHsMmqhGvBwlABZt06dwQ5qkBZiLq_EM1oddCmpwChvXDNXZypaS5l8uzkKuZj3PBUmzTQT4dgDxSXgz&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://lh3.googleusercontent.com/p9bMGxPx4jE6_Z2KB-MktmyuAxyFst-bEk29M_Bn0Bj5ul7uzinH6u5WjHsMmqhGvBwlABZt06dwQ5qkBZiLq_EM1oddCmpwChvXDNXZypaS5l8uzkKuZj3PBUmzTQT4dgDxSXgz&#34; alt=&#34;&#34; /&gt; &lt;/a&gt;&lt;/p&gt;

&lt;!--
In addition to viewing resources, you can create, edit, update, and delete them. This feature enables many use cases. For example, you can kill a failed Pod, do a rolling update on a Deployment, or just organize your resources. You can also export and import YAML configuration files of your cloud apps and store them in a version control system.



 ![](https://lh6.googleusercontent.com/zz-qjNcGgvWXrK1LIipUdIdPyeWJ1EyPVJxRnSvI6pMcLBkxDxpQt-ObsIiZsS_X0RjVBWtXYO5TCvhsymb__CGXFzKuPUnUrB4HKnAMsxtYdWLwMmHEb8c9P9Chzlo5ePHRKf5O)
--&gt;

&lt;p&gt;除了查看资源外，还可以创建、编辑、更新和删除资源。这个特性支持许多用例。例如，您可以杀死一个失败的 pod，对部署进行滚动更新，或者只组织资源。您还可以导出和导入云应用程序的 yaml 配置文件，并将它们存储在版本控制系统中。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh6.googleusercontent.com/zz-qjNcGgvWXrK1LIipUdIdPyeWJ1EyPVJxRnSvI6pMcLBkxDxpQt-ObsIiZsS_X0RjVBWtXYO5TCvhsymb__CGXFzKuPUnUrB4HKnAMsxtYdWLwMmHEb8c9P9Chzlo5ePHRKf5O&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;!--
The release includes a beta view of cluster nodes for administration and operational use cases. The UI lists all nodes in the cluster to allow for overview analysis and quick screening for problematic nodes. The details view shows all information about the node and links to pods running on it.



 ![](https://lh6.googleusercontent.com/3CSTUy-8Tz-yAL9tCqxNUqMcWJYKK0dwk7kidE9zy-L-sXFiD4A4Y2LKEqbJKgI6Fl6xbzYxsziI8dULVXPJbu6eU0ci7hNtqi3tTuhdbVD6CG3EXw151fvt2MQuqumHRbab6g-_)
--&gt;

&lt;p&gt;这个版本包括一个用于管理和操作用例的集群节点的 beta 视图。UI 列出集群中的所有节点，以便进行总体分析和快速筛选有问题的节点。details 视图显示有关该节点的所有信息以及指向在其上运行的 pod 的链接。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh6.googleusercontent.com/3CSTUy-8Tz-yAL9tCqxNUqMcWJYKK0dwk7kidE9zy-L-sXFiD4A4Y2LKEqbJKgI6Fl6xbzYxsziI8dULVXPJbu6eU0ci7hNtqi3tTuhdbVD6CG3EXw151fvt2MQuqumHRbab6g-_&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;!--
There are also many smaller scope new features that the we shipped with the release, namely: support for namespaced resources, internationalization, performance improvements, and many bug fixes (find out more in the [release notes](https://github.com/kubernetes/dashboard/releases/tag/v1.1.0)). All these improvements result in a better and simpler user experience of the product.
--&gt;

&lt;p&gt;我们随发行版提供的还有许多小范围的新功能，即：支持命名空间资源、国际化、性能改进和许多错误修复(请参阅&lt;a href=&#34;https://github.com/kubernetes/dashboard/releases/tag/v1.1.0&#34; target=&#34;_blank&#34;&gt;发行说明&lt;/a&gt;中的更多内容)。所有这些改进都会带来更好、更简单的产品用户体验。&lt;/p&gt;

&lt;!--
**Future Work**



The team has ambitious plans for the future spanning across multiple use cases. We are also open to all feature requests, which you can post on our [issue tracker](https://github.com/kubernetes/dashboard/issues).
--&gt;

&lt;p&gt;&lt;strong&gt;Future Work&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;该团队对跨越多个用例的未来有着雄心勃勃的计划。我们还对所有功能请求开放，您可以在我们的&lt;a href=&#34;https://github.com/kubernetes/dashboard/issues&#34; target=&#34;_blank&#34;&gt;问题跟踪程序&lt;/a&gt;上发布这些请求。&lt;/p&gt;

&lt;!--
Here is a list of our focus areas for the following months:

- [Handle more Kubernetes resources](https://github.com/kubernetes/dashboard/issues/961) - To show all resources that a cluster user may potentially interact with. Once done, Dashboard can act as a complete replacement for CLI.&amp;nbsp;
- [Monitoring and troubleshooting](https://github.com/kubernetes/dashboard/issues/962) - To add resource usage statistics/graphs to the objects shown in Dashboard. This focus area will allow for actionable debugging and troubleshooting of cloud applications.
- [Security, auth and logging in](https://github.com/kubernetes/dashboard/issues/964) - Make Dashboard accessible from networks external to a Cluster and work with custom authentication systems.
--&gt;

&lt;p&gt;以下是我们接下来几个月的重点领域：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/dashboard/issues/961&#34; target=&#34;_blank&#34;&gt;Handle more Kubernetes resources&lt;/a&gt; - 显示群集用户可能与之交互的所有资源。一旦完成，dashboard 就可以完全替代cli。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/dashboard/issues/962&#34; target=&#34;_blank&#34;&gt;Monitoring and troubleshooting&lt;/a&gt; - 将资源使用统计信息/图表添加到 Dashboard 中显示的对象。这个重点领域将允许对云应用程序进行可操作的调试和故障排除。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/dashboard/issues/964&#34; target=&#34;_blank&#34;&gt;Security, auth and logging in&lt;/a&gt; - 使仪表板可从群集外部的网络访问，并使用自定义身份验证系统。&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
**Connect With Us**



We would love to talk with you and hear your feedback!

- Email us at the [SIG-UI mailing list](https://groups.google.com/forum/#!forum/kubernetes-sig-ui)
- Chat with us on the Kubernetes Slack&amp;nbsp;[#SIG-UI channel](https://kubernetes.slack.com/messages/sig-ui/)
- Join our meetings: 4PM CEST. See the [SIG-UI calendar](https://calendar.google.com/calendar/embed?src=google.com_52lm43hc2kur57dgkibltqc6kc%40group.calendar.google.com&amp;ctz=Europe/Warsaw) for details.





_-- Piotr Bryk, Software Engineer, Google_
--&gt;

&lt;p&gt;&lt;strong&gt;联系我们&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;我们很乐意与您交谈并听取您的反馈！&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;请在&lt;a href=&#34;https://groups.google.com/forum/向我们发送电子邮件！论坛/kubernetes sig ui&#34; target=&#34;_blank&#34;&gt;SIG-UI邮件列表&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;在 kubernetes slack 上与我们聊天。&lt;a href=&#34;https://kubernetes.slack.com/messages/sig-ui/&#34; target=&#34;_blank&#34;&gt;#SIG-UI channel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;参加我们的会议：东部时间下午4点。请参阅&lt;a href=&#34;https://calendar.google.com/calendar/embed?src=google.com_52lm43hc2kur57dgkibltqc6kc%40group.calendar.google.com&amp;amp;ctz=Europe/Warsaw&#34; target=&#34;_blank&#34;&gt;SIG-UI日历&lt;/a&gt;了解详细信息。&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog:  Citrix &#43; Kubernetes = 全垒打 </title>
      <link>https://kubernetes.io/zh/blog/2016/07/14/citrix-netscaler-and-kubernetes/</link>
      <pubDate>Thu, 14 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/zh/blog/2016/07/14/citrix-netscaler-and-kubernetes/</guid>
      <description>
        
        
        &lt;!--
---
title: &#34; Citrix + Kubernetes = A Home Run &#34;
date: 2016-07-14
slug: citrix-netscaler-and-kubernetes
url: /blog/2016/07/Citrix-Netscaler-And-Kubernetes
---
--&gt;

&lt;!--
_Editor’s note: today’s guest post is by Mikko Disini, a Director of Product Management at Citrix Systems, sharing their collaboration experience on a Kubernetes integration.&amp;nbsp;_
--&gt;

&lt;p&gt;编者按：今天的客座文章来自 Citrix Systems 的产品管理总监 Mikko Disini，他分享了他们在 Kubernetes 集成上的合作经验。&amp;nbsp;_&lt;/p&gt;

&lt;!--
Technical collaboration is like sports. If you work together as a team, you can go down the homestretch and pull through for a win. That’s our experience with the Google Cloud Platform team.
--&gt;

&lt;p&gt;技术合作就像体育运动。如果你能像一个团队一样合作，你就能在最后关头取得胜利。这就是我们对谷歌云平台团队的经验。&lt;/p&gt;

&lt;!--
Recently, we approached Google Cloud Platform (GCP) to collaborate on behalf of Citrix customers and the broader enterprise market looking to migrate workloads.&amp;nbsp;This migration required including the [NetScaler Docker load balancer](https://www.citrix.com/blogs/2016/06/20/the-best-docker-load-balancer-at-dockercon-in-seattle-this-week/), CPX, into Kubernetes nodes and resolving any issues with getting traffic into the CPX proxies. &amp;nbsp;
--&gt;

&lt;p&gt;最近，我们与 Google 云平台（GCP）联系，代表 Citrix 客户以及更广泛的企业市场，希望就工作负载的迁移进行协作。此迁移需要将 [NetScaler Docker 负载均衡器]&lt;a href=&#34;https://www.citrix.com/blogs/2016/06/20/the-best-docker-load-balancer-at-dockercon-in-seattle-this-week/)&#34; target=&#34;_blank&#34;&gt;https://www.citrix.com/blogs/2016/06/20/the-best-docker-load-balancer-at-dockercon-in-seattle-this-week/)&lt;/a&gt; CPX 包含到 Kubernetes 节点中，并解决将流量引入 CPX 代理的任何问题。&lt;/p&gt;

&lt;!--
**Why NetScaler and Kubernetes?**
--&gt;

&lt;p&gt;&lt;strong&gt;为什么是 NetScaler 和 Kubernetes&lt;/strong&gt;&lt;/p&gt;

&lt;!--
1. Citrix customers want the same Layer 4 to Layer 7 capabilities from NetScaler that they have on-prem as they move to the cloud as they begin deploying their container and microservices architecture with Kubernetes&amp;nbsp;
2. Kubernetes provides a proven infrastructure for running containers and VMs with automated workload delivery
3. NetScaler CPX provides Layer 4 to Layer 7 services and highly efficient telemetry data to a logging and analytics platform, [NetScaler Management and Analytics System](https://www.citrix.com/blogs/2016/05/24/introducing-the-next-generation-netscaler-management-and-analytics-system/)
--&gt;

&lt;ol&gt;
&lt;li&gt;Citrix 的客户希望他们开始使用 Kubernetes 部署他们的容器和微服务体系结构时，能够像当初迁移到云计算时一样，享有 NetScaler 所提供的第 4 层到第 7 层能力&amp;nbsp;&lt;/li&gt;
&lt;li&gt;Kubernetes 提供了一套经过验证的基础设施，可用来运行容器和虚拟机，并自动交付工作负载；&lt;/li&gt;
&lt;li&gt;NetScaler CPX 提供第 4 层到第 7 层的服务，并为日志和分析平台 &lt;a href=&#34;https://www.citrix.com/blogs/2016/05/24/introducing-the-next-generation-netscaler-management-and-analytics-system/&#34; target=&#34;_blank&#34;&gt;NetScaler 管理和分析系统&lt;/a&gt; 提供高效的度量数据。&lt;/li&gt;
&lt;/ol&gt;

&lt;!--
I wish all our experiences working together with a technical partner were as good as working with GCP. We had a list of issues to enable our use cases and were able to collaborate swiftly on a solution. To resolve these, GCP team offered in depth technical assistance, working with Citrix such that NetScaler CPX can spin up and take over as a client-side proxy running on each host.&amp;nbsp;
--&gt;

&lt;p&gt;我希望我们所有与技术合作伙伴一起工作的经验都能像与 GCP 一起工作一样好。我们有一个列表，包含支持我们的用例所需要解决的问题。我们能够快速协作形成解决方案。为了解决这些问题，GCP 团队提供了深入的技术支持，与 Citrix 合作，从而使得 NetScaler CPX 能够在每台主机上作为客户端代理启动运行。&lt;/p&gt;

&lt;!--
Next, NetScaler CPX needed to be inserted in the data path of GCP ingress load balancer so that NetScaler CPX can spread traffic to front end web servers. The NetScaler team made modifications so that NetScaler CPX listens to API server events and configures itself to create a VIP, IP table rules and server rules to take ingress traffic and load balance across front end applications. Google Cloud Platform team provided feedback and assistance to verify modifications made to overcome the technical hurdles. Done!
--&gt;

&lt;p&gt;接下来，需要在 GCP 入口负载均衡器的数据路径中插入 NetScaler CPX，使 NetScaler CPX 能够将流量分散到前端 web 服务器。NetScaler 团队进行了修改，以便 NetScaler CPX 监听 API 服务器事件，并配置自己来创建 VIP、IP 表规则和服务器规则，以便跨前端应用程序接收流量和负载均衡。谷歌云平台团队提供反馈和帮助，验证为克服技术障碍所做的修改。完成了!&lt;/p&gt;

&lt;!--
NetScaler CPX use case is supported in [Kubernetes 1.3](https://kubernetes.io/blog/2016/07/kubernetes-1-3-bridging-cloud-native-and-enterprise-workloads/). Citrix customers and the broader enterprise market will have the opportunity to leverage NetScaler with Kubernetes, thereby lowering the friction to move workloads to the cloud.&amp;nbsp;
--&gt;

&lt;p&gt;NetScaler CPX 用例在 &lt;a href=&#34;https://kubernetes.io/blog/2016/07/kubernetes-1-3-bridging-cloud-native-and-enterprise-workloads/&#34; target=&#34;_blank&#34;&gt;Kubernetes 1.3&lt;/a&gt; 中提供支持。Citrix 的客户和更广泛的企业市场将有机会基于 Kubernetes 享用 NetScaler 服务，从而降低将工作负载转移到云平台的阻力。&amp;nbsp;&lt;/p&gt;

&lt;!--
You can learn more about&amp;nbsp;NetScaler CPX [here](https://www.citrix.com/networking/microservices.html).
--&gt;

&lt;p&gt;您可以在&lt;a href=&#34;https://www.citrix.com/networking/microservices.html&#34; target=&#34;_blank&#34;&gt;此处&lt;/a&gt;了解有关 NetScaler CPX 的更多信息。&lt;/p&gt;

&lt;!--
_&amp;nbsp;-- Mikko Disini, Director of Product Management - NetScaler, Citrix Systems_
--&gt;

&lt;p&gt;_&amp;nbsp;&amp;ndash; Mikko Disini，Citrix Systems NetScaler 产品管理总监&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog:  KubeCon EU 2016：伦敦 Kubernetes 社区 </title>
      <link>https://kubernetes.io/zh/blog/2016/02/24/kubecon-eu-2016-kubernetes-community-in/</link>
      <pubDate>Wed, 24 Feb 2016 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/zh/blog/2016/02/24/kubecon-eu-2016-kubernetes-community-in/</guid>
      <description>
        
        
        &lt;!--
---
title: &#34; KubeCon EU 2016: Kubernetes Community in London &#34;
date: 2016-02-24
slug: kubecon-eu-2016-kubernetes-community-in
url: /blog/2016/02/Kubecon-Eu-2016-Kubernetes-Community-In
---
--&gt;

&lt;!--
KubeCon EU 2016 is the inaugural [European Kubernetes](http://kubernetes.io/) community conference that follows on the American launch in November 2015. KubeCon is fully dedicated to education and community engagement for[Kubernetes](http://kubernetes.io/) enthusiasts, production users and the surrounding ecosystem.
--&gt;

&lt;p&gt;KubeCon EU 2016 是首届&lt;a href=&#34;http://kubernetes.io/&#34; target=&#34;_blank&#34;&gt;欧洲 Kubernetes&lt;/a&gt; 社区会议，紧随 2015 年 11 月召开的北美会议。KubeCon 致力于为 &lt;a href=&#34;http://kubernetes.io/&#34; target=&#34;_blank&#34;&gt;Kubernetes&lt;/a&gt; 爱好者、产品用户和周围的生态系统提供教育和社区参与。&lt;/p&gt;

&lt;!--
Come join us in London and hang out with hundreds from the Kubernetes community and experience a wide variety of deep technical expert talks and use cases.
--&gt;

&lt;p&gt;快来加入我们在伦敦，与 Kubernetes 社区的数百人一起出去，体验各种深入的技术专家讲座和用例。&lt;/p&gt;

&lt;!--
Don’t miss these great speaker sessions at the conference:
--&gt;

&lt;p&gt;不要错过这些优质的演讲：&lt;/p&gt;

&lt;!--
* “Kubernetes Hardware Hacks: Exploring the Kubernetes API Through Knobs, Faders, and Sliders” by Ian Lewis and Brian Dorsey, Developer Advocate, Google -* [http://sched.co/6Bl3](http://sched.co/6Bl3)

* “rktnetes: what&#39;s new with container runtimes and Kubernetes” by Jonathan Boulle, Developer and Team Lead at CoreOS -* [http://sched.co/6BY7](http://sched.co/6BY7)

* “Kubernetes Documentation: Contributing, fixing issues, collecting bounties” by John Mulhausen, Lead Technical Writer, Google -* [http://sched.co/6BUP](http://sched.co/6BUP)&amp;nbsp;
* “[What is OpenStack&#39;s role in a Kubernetes world?](https://kubeconeurope2016.sched.org/event/6BYC/what-is-openstacks-role-in-a-kubernetes-world?iframe=yes&amp;w=i:0;&amp;sidebar=yes&amp;bg=no#?iframe=yes&amp;w=i:100;&amp;sidebar=yes&amp;bg=no)” By Thierry Carrez, Director of Engineering, OpenStack Foundation -* http://sched.co/6BYC
* “A Practical Guide to Container Scheduling” by Mandy Waite, Developer Advocate, Google -* [http://sched.co/6BZa](http://sched.co/6BZa)

* “[Kubernetes in Production in The New York Times newsroom](https://kubeconeurope2016.sched.org/event/67f2/kubernetes-in-production-in-the-new-york-times-newsroom?iframe=yes&amp;w=i:0;&amp;sidebar=yes&amp;bg=no#?iframe=yes&amp;w=i:100;&amp;sidebar=yes&amp;bg=no)” Eric Lewis, Web Developer, New York Times -* [http://sched.co/67f2](http://sched.co/67f2)
* “[Creating an Advanced Load Balancing Solution for Kubernetes with NGINX](https://kubeconeurope2016.sched.org/event/6Bc9/creating-an-advanced-load-balancing-solution-for-kubernetes-with-nginx?iframe=yes&amp;w=i:0;&amp;sidebar=yes&amp;bg=no#?iframe=yes&amp;w=i:100;&amp;sidebar=yes&amp;bg=no)” by Andrew Hutchings, Technical Product Manager, NGINX -* http://sched.co/6Bc9
* And many more http://kubeconeurope2016.sched.org/
--&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;“Kubernetes 硬件黑客：通过旋钮、推杆和滑块探索 Kubernetes API” 演讲者 Ian Lewis 和 Brian Dorsey，谷歌开发布道师* &lt;a href=&#34;http://sched.co/6Bl3&#34; target=&#34;_blank&#34;&gt;http://sched.co/6Bl3&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;“rktnetes: 容器运行时和 Kubernetes 的新功能” 演讲者 Jonathan Boulle, CoreOS 的主程 -* &lt;a href=&#34;http://sched.co/6BY7&#34; target=&#34;_blank&#34;&gt;http://sched.co/6BY7&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;“Kubernetes 文档：贡献、修复问题、收集奖金” 作者：John Mulhausen，首席技术作家，谷歌 -* &lt;a href=&#34;http://sched.co/6BUP&#34; target=&#34;_blank&#34;&gt;http://sched.co/6BUP&lt;/a&gt;&amp;nbsp;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;“&lt;a href=&#34;https://kubeconeurope2016.sched.org/event/6BYC/what-is-openstacks-role-in-a-kubernetes-world?iframe=yes&amp;amp;w=i:0;&amp;amp;sidebar=yes&amp;amp;bg=no#?iframe=yes&amp;amp;w=i:100;&amp;amp;sidebar=yes&amp;amp;bg=no&#34; target=&#34;_blank&#34;&gt;OpenStack 在 Kubernetes 的世界中扮演什么角色？&lt;/a&gt;” 作者：Thierry carez, OpenStack 基金会工程总监 -* &lt;a href=&#34;http://sched.co/6BYC&#34; target=&#34;_blank&#34;&gt;http://sched.co/6BYC&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;“容器调度的实用指南” 作者：Mandy Waite，开发者倡导者，谷歌 -* &lt;a href=&#34;http://sched.co/6BZa&#34; target=&#34;_blank&#34;&gt;http://sched.co/6BZa&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;“&lt;a href=&#34;https://kubeconeurope2016.sched.org/event/67f2/kubernetes-in-production-in-the-new-york-times-newsroom?iframe=yes&amp;amp;w=i:0;&amp;amp;sidebar=yes&amp;amp;bg=no#?iframe=yes&amp;amp;w=i:100;&amp;amp;sidebar=yes&amp;amp;bg=no&#34; target=&#34;_blank&#34;&gt;《纽约时报》编辑部正在制作 Kubernetes&lt;/a&gt;” Eric Lewis，《纽约时报》网站开发人员 -* &lt;a href=&#34;http://sched.co/67f2&#34; target=&#34;_blank&#34;&gt;http://sched.co/67f2&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;“&lt;a href=&#34;https://kubeconeurope2016.sched.org/event/6Bc9/creating-an-advanced-load-balancing-solution-for-kubernetes-with-nginx?iframe=yes&amp;amp;w=i:0;&amp;amp;sidebar=yes&amp;amp;bg=no#?iframe=yes&amp;amp;w=i:100;&amp;amp;sidebar=yes&amp;amp;bg=no&#34; target=&#34;_blank&#34;&gt;使用 NGINX 为 Kubernetes 创建一个高级负载均衡解决方案&lt;/a&gt;” 作者：Andrew Hutchings, NGINX 技术产品经理 -* &lt;a href=&#34;http://sched.co/6Bc9&#34; target=&#34;_blank&#34;&gt;http://sched.co/6Bc9&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;还有更多 &lt;a href=&#34;http://kubeconeurope2016.sched.org/&#34; target=&#34;_blank&#34;&gt;http://kubeconeurope2016.sched.org/&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
Get your KubeCon EU [tickets here](https://ti.to/kubecon/kubecon-eu-2016).
--&gt;

&lt;p&gt;&lt;a href=&#34;https://ti.to/kubecon/kubecon-eu-2016&#34; target=&#34;_blank&#34;&gt;在这里&lt;/a&gt;获取您的 KubeCon EU 门票。&lt;/p&gt;

&lt;!--
Venue Location: CodeNode * 10 South Pl, London, United Kingdom
Accommodations: [hotels](https://skillsmatter.com/contact-us#hotels)
Website: [kubecon.io](https://www.kubecon.io/)
Twitter: [@KubeConio](https://twitter.com/kubeconio) #KubeCon
Google is a proud Diamond sponsor of KubeCon EU 2016. Come to London next month, March 10th &amp; 11th, and visit booth #13 to learn all about Kubernetes, Google Container Engine (GKE) and Google Cloud Platform!
--&gt;

&lt;p&gt;会场地址：CodeNode * 英国伦敦南广场 10 号
酒店住宿：&lt;a href=&#34;https://skillsmatter.com/contact-us&#34; target=&#34;_blank&#34;&gt;酒店&lt;/a&gt;
网站：&lt;a href=&#34;https://www.kubecon.io/&#34; target=&#34;_blank&#34;&gt;kubecon.io&lt;/a&gt;
推特：&lt;a href=&#34;https://twitter.com/kubeconio&#34; target=&#34;_blank&#34;&gt;@KubeConio&lt;/a&gt;
谷歌是 KubeCon EU 2016 的钻石赞助商。下个月 3 月 10 - 11 号来伦敦，参观 13 号展位，了解 Kubernetes，Google Container Engine（GKE），Google Cloud Platform 的所有信息!&lt;/p&gt;

&lt;!--
_KubeCon is organized by KubeAcademy, LLC, a community-driven group of developers focused on the education of developers and the promotion of Kubernetes._

-* Sarah Novotny, Kubernetes Community Manager, Google
--&gt;

&lt;p&gt;_KubeCon 是由 KubeAcademy、LLC 组织的，这是一个由社区驱动的开发者团体，专注于开发人员的教育和 kubernet.com 的推广
-* Sarah Novotny, 谷歌的 Kubernetes 社区经理&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog:  Kubernetes 社区会议记录 - 20160204 </title>
      <link>https://kubernetes.io/zh/blog/2016/02/09/kubernetes-community-meeting-notes/</link>
      <pubDate>Tue, 09 Feb 2016 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/zh/blog/2016/02/09/kubernetes-community-meeting-notes/</guid>
      <description>
        
        
        

&lt;!--
---
title: &#34; Kubernetes community meeting notes - 20160204 &#34;
date: 2016-02-09
slug: kubernetes-community-meeting-notes
url: /blog/2016/02/Kubernetes-Community-Meeting-Notes
---
--&gt;

&lt;!--
####  February 4th - rkt demo (congratulations on the 1.0, CoreOS!), eBay puts k8s on Openstack and considers Openstack on k8s, SIGs, and flaky test surge makes progress.
--&gt;

&lt;h4 id=&#34;2月4日-rkt演示-祝贺-1-0-版本-coreos-ebay-将-k8s-放在-openstack-上并认为-openstack-在-k8s-sig-和片状测试激增方面取得了进展&#34;&gt;2月4日 - rkt演示（祝贺 1.0 版本， CoreOS！）， eBay 将 k8s 放在 Openstack 上并认为 Openstack 在 k8s， SIG 和片状测试激增方面取得了进展。&lt;/h4&gt;

&lt;!--
The Kubernetes contributing community meets most Thursdays at 10:00PT to discuss the project&#39;s status via a videoconference. Here are the notes from the latest meeting.
--&gt;

&lt;p&gt;Kubernetes 贡献社区在每周四 10:00 PT 开会,通过视频会议讨论项目状态。以下是最近一次会议的笔记。&lt;/p&gt;

&lt;!--
* Note taker: Rob Hirschfeld
* Demo (20 min): CoreOS rkt + Kubernetes [Shaya Potter]
    * expect to see integrations w/ rkt &amp; k8s in the coming months (&#34;rkt-netes&#34;). not integrated into the v1.2 release.
    * Shaya gave a demo (8 minutes into meeting for video reference)
        * CLI of rkt shown spinning up containers
        * [note: audio is garbled at points]
        * Discussion about integration w/ k8s &amp; rkt
        * rkt community sync next week: https://groups.google.com/forum/#!topic/rkt-dev/FlwZVIEJGbY
        * Dawn Chen:
            * The remaining issues of integrating rkt with kubernetes: 1) cadivsor 2) DNS 3) bugs related to logging
            * But need more work on e2e test suites
--&gt;

&lt;ul&gt;
&lt;li&gt;书记员：Rob Hirschfeld&lt;/li&gt;
&lt;li&gt;演示视频（20分钟）：CoreOS rkt + Kubernetes[Shaya Potter]

&lt;ul&gt;
&lt;li&gt;期待在未来几个月内看到与rkt和k8s的整合（“rkt-netes”）。 还没有集成到 v1.2版本中。&lt;/li&gt;
&lt;li&gt;Shaya 做了一个演示（8分钟的会议视频参考）

&lt;ul&gt;
&lt;li&gt;rkt的CLI显示了旋转容器&lt;/li&gt;
&lt;li&gt;[注意：音频在点数上是乱码]&lt;/li&gt;
&lt;li&gt;关于 k8s&amp;amp;rkt 整合的讨论&lt;/li&gt;
&lt;li&gt;下周 rkt 社区同步：&lt;a href=&#34;https://groups.google.com/forum/#!topic/rkt-dev/FlwZVIEJGbY&#34; target=&#34;_blank&#34;&gt;https://groups.google.com/forum/#!topic/rkt-dev/FlwZVIEJGbY&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Dawn Chen:

&lt;ul&gt;
&lt;li&gt;将 rkt 与 kubernetes 集成的其余问题：1）cadivsor 2） DNS 3）与日志记录相关的错误&lt;/li&gt;
&lt;li&gt;但是需要在 e2e 测试套件上做更多的工作
&amp;lt;!&amp;ndash;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Use Case (10 min): eBay k8s on OpenStack and OpenStack on k8s [Ashwin Raveendran]

&lt;ul&gt;
&lt;li&gt;eBay is currently running Kubernetes on OpenStack&lt;/li&gt;
&lt;li&gt;Goal for eBay is to manage the OpenStack control plane w/ k8s.  Goal would be to achieve upgrades&lt;/li&gt;
&lt;li&gt;OpenStack Kolla creates containers for the control plane.  Uses Ansible+Docker for management of the containers.&lt;/li&gt;
&lt;li&gt;Working on k8s control plan management - Saltstack is proving to be a management challenge at the scale they want to operate.  Looking for automated management of the k8s control plane.
&amp;ndash;&amp;gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;用例（10分钟）：在 OpenStack 上的 eBay k8s 和 k8s 上的 OpenStack [Ashwin Raveendran]

&lt;ul&gt;
&lt;li&gt;eBay 目前正在 OpenStack 上运行 Kubernetes&lt;/li&gt;
&lt;li&gt;eBay 的目标是管理带有 k8s 的 OpenStack 控制平面。目标是实现升级。&lt;/li&gt;
&lt;li&gt;OpenStack Kolla 为控制平面创建容器。使用 Ansible+Docker 来管理容器。&lt;/li&gt;
&lt;li&gt;致力于 k8s 控制计划管理 - Saltstack 被证明是他们想运营的规模的管理挑战。寻找 k8s 控制平面的自动化管理。
&amp;lt;!&amp;ndash;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;SIG Report&lt;/li&gt;
&lt;li&gt;Testing update [Jeff, Joe, and Erick]

&lt;ul&gt;
&lt;li&gt;Working to make the workflow about contributing to K8s easier to understanding

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/kubernetes/pull/19714&#34; target=&#34;_blank&#34;&gt;pull/19714&lt;/a&gt; has flow chart of the bot flow to help users understand&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Need a consistent way to run tests w/ hacking config scripts (you have to fake a Jenkins process right now)&lt;/li&gt;
&lt;li&gt;Want to create necessary infrastructure to make test setup less flaky&lt;/li&gt;
&lt;li&gt;want to decouple test start (single or full) from Jenkins&lt;/li&gt;
&lt;li&gt;goal is to get to point where you have 1 script to run that can be pointed to any cluster&lt;/li&gt;
&lt;li&gt;demo included Google internal views - working to try get that external.&lt;/li&gt;
&lt;li&gt;want to be able to collect test run results&lt;/li&gt;
&lt;li&gt;Bob Wise calls for testing infrastructure to be a blocker on v1.3&lt;/li&gt;
&lt;li&gt;Long discussion about testing practices…

&lt;ul&gt;
&lt;li&gt;consensus that we want to have tests work over multiple platforms.&lt;/li&gt;
&lt;li&gt;would be helpful to have a comprehensive state dump for test reports&lt;/li&gt;
&lt;li&gt;&amp;ldquo;phone-home&amp;rdquo; to collect stack traces - should be available
&amp;ndash;&amp;gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;SIG 报告&lt;/li&gt;
&lt;li&gt;测试更新 [Jeff, Joe, 和 Erick]

&lt;ul&gt;
&lt;li&gt;努力使有助于 K8s 的工作流程更容易理解

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/kubernetes/pull/19714&#34; target=&#34;_blank&#34;&gt;pull/19714&lt;/a&gt;有 bot 流程图来帮助用户理解&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;需要一种一致的方法来运行测试 w/hacking 配置脚本（你现在必须伪造一个 Jenkins 进程）&lt;/li&gt;
&lt;li&gt;想要创建必要的基础设施，使测试设置不那么薄弱&lt;/li&gt;
&lt;li&gt;想要将测试开始（单次或完整）与 Jenkins分离&lt;/li&gt;
&lt;li&gt;目标是指出你有一个可以指向任何集群的脚本&lt;/li&gt;
&lt;li&gt;演示包括 Google 内部视图 - 努力尝试获取外部视图。&lt;/li&gt;
&lt;li&gt;希望能够收集测试运行结果&lt;/li&gt;
&lt;li&gt;Bob Wise 不赞同在 v1.3 版本进行测试方面的基础设施建设。&lt;/li&gt;
&lt;li&gt;关于测试实践的长期讨论…

&lt;ul&gt;
&lt;li&gt;我们希望在多个平台上进行测试的共识。&lt;/li&gt;
&lt;li&gt;为测试报告提供一个全面转储会很有帮助&lt;/li&gt;
&lt;li&gt;可以使用&amp;rdquo;phone-home&amp;rdquo;收集异常&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
* 1.2 Release Watch
* CoC [Sarah]
* GSoC [Sarah]
--&gt;

&lt;ul&gt;
&lt;li&gt;1.2发布观察&lt;/li&gt;
&lt;li&gt;CoC [Sarah]&lt;/li&gt;
&lt;li&gt;GSoC [Sarah]&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
To get involved in the Kubernetes community consider joining our [Slack channel][2], taking a look at the [Kubernetes project][3] on GitHub, or join the [Kubernetes-dev Google group][4]. If you&#39;re really excited, you can do all of the above and join us for the next community conversation -- February 11th, 2016. Please add yourself or a topic you want to know about to the [agenda][5] and get a calendar invitation by joining [this group][6].
--&gt;

&lt;p&gt;要参与 Kubernetes 社区，请考虑加入我们的&lt;a href=&#34;http://slack.k8s.io/&#34; target=&#34;_blank&#34;&gt;Slack 频道&lt;/a&gt;，查看 GitHub上的 &lt;a href=&#34;https://github.com/kubernetes/&#34; target=&#34;_blank&#34;&gt;Kubernetes 项目&lt;/a&gt;，或加入&lt;a href=&#34;https://groups.google.com/forum/#!forum/kubernetes-dev&#34; target=&#34;_blank&#34;&gt;Kubernetes-dev Google 小组&lt;/a&gt;。如果你真的很兴奋，你可以完成上述所有工作并加入我们的下一次社区对话-2016年2月11日。请将您自己或您想要了解的主题添加到&lt;a href=&#34;https://docs.google.com/document/d/1VQDIAB0OqiSjIHI8AWMvSdceWhnz56jNpZrLs6o7NJY/edit#&#34; target=&#34;_blank&#34;&gt;议程&lt;/a&gt;并通过加入&lt;a href=&#34;https://groups.google.com/forum/#!forum/kubernetes-community-video-chat&#34; target=&#34;_blank&#34;&gt;此组&lt;/a&gt;来获取日历邀请。&lt;/p&gt;

&lt;p&gt;&amp;ldquo;&lt;a href=&#34;https://youtu.be/IScpP8Cj0hw?list=PL69nYSiGNLP1pkHsbPjzAewvMgGUpkCnJ&amp;quot;&#34; target=&#34;_blank&#34;&gt;https://youtu.be/IScpP8Cj0hw?list=PL69nYSiGNLP1pkHsbPjzAewvMgGUpkCnJ&amp;quot;&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog:  为什么 Kubernetes 不用 libnetwork </title>
      <link>https://kubernetes.io/zh/blog/2016/01/14/why-kubernetes-doesnt-use-libnetwork/</link>
      <pubDate>Thu, 14 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/zh/blog/2016/01/14/why-kubernetes-doesnt-use-libnetwork/</guid>
      <description>
        
        
        &lt;!-- ---
title: &#34; Why Kubernetes doesn’t use libnetwork &#34;
date: 2016-01-14
slug: why-kubernetes-doesnt-use-libnetwork
url: /blog/2016/01/Why-Kubernetes-Doesnt-Use-Libnetwork
--- --&gt;

&lt;!-- Kubernetes has had a very basic form of network plugins since before version 1.0 was released — around the same time as Docker&#39;s [libnetwork](https://github.com/docker/libnetwork) and Container Network Model ([CNM](https://github.com/docker/libnetwork/blob/master/docs/design.md)) was introduced. Unlike libnetwork, the Kubernetes plugin system still retains its &#34;alpha&#34; designation. Now that Docker&#39;s network plugin support is released and supported, an obvious question we get is why Kubernetes has not adopted it yet. After all, vendors will almost certainly be writing plugins for Docker — we would all be better off using the same drivers, right?   --&gt;

&lt;p&gt;在 1.0 版本发布之前，Kubernetes 已经有了一个非常基础的网络插件形式-大约在引入 Docker’s &lt;a href=&#34;https://github.com/docker/libnetwork&#34; target=&#34;_blank&#34;&gt;libnetwork&lt;/a&gt; 和 Container Network Model (&lt;a href=&#34;https://github.com/docker/libnetwork/blob/master/docs/design.md&#34; target=&#34;_blank&#34;&gt;CNM&lt;/a&gt;) 的时候。与 libnetwork 不同，Kubernetes 插件系统仍然保留它的 &amp;lsquo;alpha&amp;rsquo; 名称。现在 Docker 的网络插件支持已经发布并得到支持，我们发现一个明显的问题是 Kubernetes 尚未采用它。毕竟，供应商几乎肯定会为 Docker 编写插件-我们最好还是用相同的驱动程序，对吧？&lt;/p&gt;

&lt;!-- Before going further, it&#39;s important to remember that Kubernetes is a system that supports multiple container runtimes, of which Docker is just one. Configuring networking is a facet of each runtime, so when people ask &#34;will Kubernetes support CNM?&#34; what they really mean is &#34;will kubernetes support CNM drivers with the Docker runtime?&#34; It would be great if we could achieve common network support across runtimes, but that’s not an explicit goal.   --&gt;

&lt;p&gt;在进一步说明之前，重要的是记住 Kubernetes 是一个支持多种容器运行时的系统， Docker 只是其中之一。配置网络只是每一个运行时的一个方面，所以当人们问起“ Kubernetes 会支持CNM吗？”，他们真正的意思是“ Kubernetes 会支持 Docker 运行时的 CNM 驱动吗？”如果我们能够跨运行时实现通用的网络支持会很棒，但这不是一个明确的目标。&lt;/p&gt;

&lt;!-- Indeed, Kubernetes has not adopted CNM/libnetwork for the Docker runtime. In fact, we’ve been investigating the alternative Container Network Interface ([CNI](https://github.com/appc/cni/blob/master/SPEC.md)) model put forth by CoreOS and part of the App Container ([appc](https://github.com/appc)) specification. Why? There are a number of reasons, both technical and non-technical.   --&gt;

&lt;p&gt;实际上， Kubernetes 还没有为 Docker 运行时采用 CNM/libnetwork 。事实上，我们一直在研究 CoreOS 提出的替代 Container Network Interface (&lt;a href=&#34;https://github.com/appc/cni/blob/master/SPEC.md&#34; target=&#34;_blank&#34;&gt;CNI&lt;/a&gt;) 模型以及 App Container (&lt;a href=&#34;https://github.com/appc&#34; target=&#34;_blank&#34;&gt;appc&lt;/a&gt;) 规范的一部分。为什么我们要这么做？有很多技术和非技术的原因。&lt;/p&gt;

&lt;!-- First and foremost, there are some fundamental assumptions in the design of Docker&#39;s network drivers that cause problems for us.   --&gt;

&lt;p&gt;首先，Docker 的网络驱动程序设计中存在一些基本假设，这些假设会给我们带来问题。&lt;/p&gt;

&lt;!-- Docker has a concept of &#34;local&#34; and &#34;global&#34; drivers. Local drivers (such as &#34;bridge&#34;) are machine-centric and don’t do any cross-node coordination. Global drivers (such as &#34;overlay&#34;) rely on [libkv](https://github.com/docker/libkv) (a key-value store abstraction) to coordinate across machines. This key-value store is a another plugin interface, and is very low-level (keys and values, no semantic meaning). To run something like Docker&#39;s overlay driver in a Kubernetes cluster, we would either need cluster admins to run a whole different instance of [consul](https://github.com/hashicorp/consul), [etcd](https://github.com/coreos/etcd) or [zookeeper](https://zookeeper.apache.org/) (see [multi-host networking](https://docs.docker.com/engine/userguide/networking/get-started-overlay/)), or else we would have to provide our own libkv implementation that was backed by Kubernetes.  --&gt;

&lt;p&gt;Docker 有一个“本地”和“全局”驱动程序的概念。本地驱动程序（例如 &amp;ldquo;bridge&amp;rdquo; ）以机器为中心，不进行任何跨节点协调。全局驱动程序（例如 &amp;ldquo;overlay&amp;rdquo; ）依赖于 &lt;a href=&#34;https://github.com/docker/libkv&#34; target=&#34;_blank&#34;&gt;libkv&lt;/a&gt; （一个键值存储抽象库）来协调跨机器。这个键值存储是另一个插件接口，并且是非常低级的（键和值，没有其他含义）。 要在 Kubernetes 集群中运行类似 Docker&amp;rsquo;s overlay 驱动程序，我们要么需要集群管理员来运行 &lt;a href=&#34;https://github.com/hashicorp/consul&#34; target=&#34;_blank&#34;&gt;consul&lt;/a&gt;, &lt;a href=&#34;https://github.com/coreos/etcd&#34; target=&#34;_blank&#34;&gt;etcd&lt;/a&gt; 或 &lt;a href=&#34;https://zookeeper.apache.org/&#34; target=&#34;_blank&#34;&gt;zookeeper&lt;/a&gt; 的整个不同实例 (see &lt;a href=&#34;https://docs.docker.com/engine/userguide/networking/get-started-overlay/&#34; target=&#34;_blank&#34;&gt;multi-host networking&lt;/a&gt;) 否则我们必须提供我们自己的 libkv 实现，那被 Kubernetes 支持。&lt;/p&gt;

&lt;!-- The latter sounds attractive, and we tried to implement it, but the libkv interface is very low-level, and the schema is defined internally to Docker. We would have to either directly expose our underlying key-value store or else offer key-value semantics (on top of our structured API which is itself implemented on a key-value system). Neither of those are very attractive for performance, scalability and security reasons. The net result is that the whole system would significantly be more complicated, when the goal of using Docker networking is to simplify things.   --&gt;

&lt;p&gt;后者听起来很有吸引力，并且我们尝试实现它，但 libkv 接口是非常低级的，并且架构在内部定义为 Docker 。我们必须直接暴露我们的底层键值存储，或者提供键值语义（在我们的结构化API之上，它本身是在键值系统上实现的）。对于性能，可伸缩性和安全性原因，这些都不是很有吸引力。最终结果是，当使用 Docker 网络的目标是简化事情时，整个系统将显得更加复杂。&lt;/p&gt;

&lt;!-- For users that are willing and able to run the requisite infrastructure to satisfy Docker global drivers and to configure Docker themselves, Docker networking should &#34;just work.&#34; Kubernetes will not get in the way of such a setup, and no matter what direction the project goes, that option should be available. For default installations, though, the practical conclusion is that this is an undue burden on users and we therefore cannot use Docker&#39;s global drivers (including &#34;overlay&#34;), which eliminates a lot of the value of using Docker&#39;s plugins at all.   --&gt;

&lt;p&gt;对于愿意并且能够运行必需的基础架构以满足 Docker 全局驱动程序并自己配置 Docker 的用户， Docker 网络应该“正常工作。” Kubernetes 不会妨碍这样的设置，无论项目的方向如何，该选项都应该可用。但是对于默认安装，实际的结论是这对用户来说是一个不应有的负担，因此我们不能使用 Docker 的全局驱动程序（包括 &amp;ldquo;overlay&amp;rdquo; ），这消除了使用 Docker 插件的很多价值。&lt;/p&gt;

&lt;!-- Docker&#39;s networking model makes a lot of assumptions that aren’t valid for Kubernetes. In docker versions 1.8 and 1.9, it includes a fundamentally flawed implementation of &#34;discovery&#34; that results in corrupted `/etc/hosts` files in containers ([docker #17190](https://github.com/docker/docker/issues/17190)) — and this cannot be easily turned off. In version 1.10 Docker is planning to [bundle a new DNS server](https://github.com/docker/docker/issues/17195), and it’s unclear whether this will be able to be turned off. Container-level naming is not the right abstraction for Kubernetes — we already have our own concepts of service naming, discovery, and binding, and we already have our own DNS schema and server (based on the well-established [SkyDNS](https://github.com/skynetservices/skydns)). The bundled solutions are not sufficient for our needs but are not disableable.   --&gt;

&lt;p&gt;Docker 的网络模型做出了许多对 Kubernetes 无效的假设。在 docker 1.8 和 1.9 版本中，它包含一个从根本上有缺陷的“发现”实现，导致容器中的 &lt;code&gt;/etc/hosts&lt;/code&gt; 文件损坏 (&lt;a href=&#34;https://github.com/docker/docker/issues/17190&#34; target=&#34;_blank&#34;&gt;docker #17190&lt;/a&gt;) - 并且这不容易被关闭。在 1.10 版本中，Docker 计划 &lt;a href=&#34;https://github.com/docker/docker/issues/17195&#34; target=&#34;_blank&#34;&gt;捆绑一个新的DNS服务器&lt;/a&gt;，目前还不清楚是否可以关闭它。容器级命名不是 Kubernetes 的正确抽象 - 我们已经有了自己的服务命名，发现和绑定概念，并且我们已经有了自己的 DNS 模式和服务器（基于完善的 &lt;a href=&#34;https://github.com/skynetservices/skydns&#34; target=&#34;_blank&#34;&gt;SkyDNS&lt;/a&gt; ）。捆绑的解决方案不足以满足我们的需求，但不能禁用。&lt;/p&gt;

&lt;!-- Orthogonal to the local/global split, Docker has both in-process and out-of-process (&#34;remote&#34;) plugins. We investigated whether we could bypass libnetwork (and thereby skip the issues above) and drive Docker remote plugins directly. Unfortunately, this would mean that we could not use any of the Docker in-process plugins, &#34;bridge&#34; and &#34;overlay&#34; in particular, which again eliminates much of the utility of libnetwork.   --&gt;

&lt;p&gt;与本地/全局拆分正交， Docker 具有进程内和进程外（ &amp;ldquo;remote&amp;rdquo; ）插件。我们调查了是否可以绕过 libnetwork （从而跳过上面的问题）并直接驱动 Docker remote 插件。不幸的是，这意味着我们无法使用任何 Docker 进程中的插件，特别是 &amp;ldquo;bridge&amp;rdquo; 和 &amp;ldquo;overlay&amp;rdquo;，这再次消除了 libnetwork 的大部分功能。&lt;/p&gt;

&lt;!-- On the other hand, CNI is more philosophically aligned with Kubernetes. It&#39;s far simpler than CNM, doesn&#39;t require daemons, and is at least plausibly cross-platform (CoreOS’s [rkt](https://coreos.com/rkt/docs/) container runtime supports it). Being cross-platform means that there is a chance to enable network configurations which will work the same across runtimes (e.g. Docker, Rocket, Hyper). It follows the UNIX philosophy of doing one thing well.   --&gt;

&lt;p&gt;另一方面， CNI 在哲学上与 Kubernetes 更加一致。它比 CNM 简单得多，不需要守护进程，并且至少是合理的跨平台（ CoreOS 的 &lt;a href=&#34;https://coreos.com/rkt/docs/&#34; target=&#34;_blank&#34;&gt;rkt&lt;/a&gt; 容器运行时支持它）。跨平台意味着有机会启用跨运行时（例如 Docker ， Rocket ， Hyper ）运行相同的网络配置。 它遵循 UNIX 的理念，即做好一件事。&lt;/p&gt;

&lt;!-- Additionally, it&#39;s trivial to wrap a CNI plugin and produce a more customized CNI plugin — it can be done with a simple shell script. CNM is much more complex in this regard. This makes CNI an attractive option for rapid development and iteration. Early prototypes have proven that it&#39;s possible to eject almost 100% of the currently hard-coded network logic in kubelet into a plugin.   --&gt;

&lt;p&gt;此外，包装 CNI 插件并生成更加个性化的 CNI 插件是微不足道的 - 它可以通过简单的 shell 脚本完成。 CNM 在这方面要复杂得多。这使得 CNI 对于快速开发和迭代是有吸引力的选择。早期的原型已经证明，可以将 kubelet 中几乎 100％ 的当前硬编码网络逻辑弹出到插件中。&lt;/p&gt;

&lt;!-- We investigated [writing a &#34;bridge&#34; CNM driver](https://groups.google.com/forum/#!topic/kubernetes-sig-network/5MWRPxsURUw) for Docker that ran CNI drivers. This turned out to be very complicated. First, the CNM and CNI models are very different, so none of the &#34;methods&#34; lined up. We still have the global vs. local and key-value issues discussed above. Assuming this driver would declare itself local, we have to get info about logical networks from Kubernetes.   --&gt;

&lt;p&gt;我们调查了为 Docker &lt;a href=&#34;https://groups.google.com/forum/#!topic/kubernetes-sig-network/5MWRPxsURUw&#34; target=&#34;_blank&#34;&gt;编写 &amp;ldquo;bridge&amp;rdquo; CNM驱动程序&lt;/a&gt; 并运行 CNI 驱动程序。事实证明这非常复杂。首先， CNM 和 CNI 模型非常不同，因此没有一种“方法”协调一致。 我们仍然有上面讨论的全球与本地和键值问题。假设这个驱动程序会声明自己是本地的，我们必须从 Kubernetes 获取有关逻辑网络的信息。&lt;/p&gt;

&lt;!-- Unfortunately, Docker drivers are hard to map to other control planes like Kubernetes. Specifically, drivers are not told the name of the network to which a container is being attached — just an ID that Docker allocates internally. This makes it hard for a driver to map back to any concept of network that exists in another system.   --&gt;

&lt;p&gt;不幸的是， Docker 驱动程序很难映射到像 Kubernetes 这样的其他控制平面。具体来说，驱动程序不会被告知连接容器的网络名称 - 只是 Docker 内部分配的 ID 。这使得驱动程序很难映射回另一个系统中存在的任何网络概念。&lt;/p&gt;

&lt;!-- This and other issues have been brought up to Docker developers by network vendors, and are usually closed as &#34;working as intended&#34; ([libnetwork #139](https://github.com/docker/libnetwork/issues/139), [libnetwork #486](https://github.com/docker/libnetwork/issues/486), [libnetwork #514](https://github.com/docker/libnetwork/pull/514), [libnetwork #865](https://github.com/docker/libnetwork/issues/865), [docker #18864](https://github.com/docker/docker/issues/18864)), even though they make non-Docker third-party systems more difficult to integrate with. Throughout this investigation Docker has made it clear that they’re not very open to ideas that deviate from their current course or that delegate control. This is very worrisome to us, since Kubernetes complements Docker and adds so much functionality, but exists outside of Docker itself.   --&gt;

&lt;p&gt;这个问题和其他问题已由网络供应商提出给 Docker 开发人员，并且通常关闭为“按预期工作”，(&lt;a href=&#34;https://github.com/docker/libnetwork/issues/139&#34; target=&#34;_blank&#34;&gt;libnetwork #139&lt;/a&gt;, &lt;a href=&#34;https://github.com/docker/libnetwork/issues/486&#34; target=&#34;_blank&#34;&gt;libnetwork #486&lt;/a&gt;, &lt;a href=&#34;https://github.com/docker/libnetwork/pull/514&#34; target=&#34;_blank&#34;&gt;libnetwork #514&lt;/a&gt;, &lt;a href=&#34;https://github.com/docker/libnetwork/issues/865&#34; target=&#34;_blank&#34;&gt;libnetwork #865&lt;/a&gt;, &lt;a href=&#34;https://github.com/docker/docker/issues/18864&#34; target=&#34;_blank&#34;&gt;docker #18864&lt;/a&gt;)，即使它们使非 Docker 第三方系统更难以与之集成。在整个调查过程中， Docker 明确表示他们对偏离当前路线或委托控制的想法不太欢迎。这对我们来说非常令人担忧，因为 Kubernetes 补充了 Docker 并增加了很多功能，但它存在于 Docker 之外。&lt;/p&gt;

&lt;!-- For all of these reasons we have chosen to invest in CNI as the Kubernetes plugin model. There will be some unfortunate side-effects of this. Most of them are relatively minor (for example, `docker inspect` will not show an IP address), but some are significant. In particular, containers started by `docker run` might not be able to communicate with containers started by Kubernetes, and network integrators will have to provide CNI drivers if they want to fully integrate with Kubernetes. On the other hand, Kubernetes will get simpler and more flexible, and a lot of the ugliness of early bootstrapping (such as configuring Docker to use our bridge) will go away.   --&gt;

&lt;p&gt;出于所有这些原因，我们选择投资 CNI 作为 Kubernetes 插件模型。这会有一些不幸的副作用。它们中的大多数都相对较小（例如， &lt;code&gt;docker inspect&lt;/code&gt; 不会显示 IP 地址），但有些是重要的。特别是，由 &lt;code&gt;docker run&lt;/code&gt; 启动的容器可能无法与 Kubernetes 启动的容器通信，如果网络集成商想要与 Kubernetes 完全集成，则必须提供 CNI 驱动程序。另一方面， Kubernetes 将变得更简单，更灵活，早期引入的许多丑陋的（例如配置 Docker 使用我们的网桥）将会消失。&lt;/p&gt;

&lt;!-- As we proceed down this path, we’ll certainly keep our eyes and ears open for better ways to integrate and simplify. If you have thoughts on how we can do that, we really would like to hear them — find us on [slack](http://slack.k8s.io/) or on our [network SIG mailing-list](https://groups.google.com/forum/#!forum/kubernetes-sig-network).   --&gt;

&lt;p&gt;当我们沿着这条道路前进时，我们肯定会保持眼睛和耳朵的开放，以便更好地整合和简化。如果您对我们如何做到这一点有所想法，我们真的希望听到它们 - 在 &lt;a href=&#34;http://slack.k8s.io/&#34; target=&#34;_blank&#34;&gt;slack&lt;/a&gt; 或者 &lt;a href=&#34;https://groups.google.com/forum/#!forum/kubernetes-sig-network&#34; target=&#34;_blank&#34;&gt;network SIG mailing-list&lt;/a&gt; 找到我们。&lt;/p&gt;

&lt;p&gt;Tim Hockin, Software Engineer, Google&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog:  Simple leader election with Kubernetes and Docker </title>
      <link>https://kubernetes.io/zh/blog/2016/01/11/simple-leader-election-with-kubernetes/</link>
      <pubDate>Mon, 11 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/zh/blog/2016/01/11/simple-leader-election-with-kubernetes/</guid>
      <description>
        
        
        

&lt;p&gt;title: &amp;ldquo;Kubernetes 和 Docker 简单的 leader election&amp;rdquo;
date: 2016-01-11
slug: simple-leader-election-with-kubernetes
url: /blog/2016/01/Simple-Leader-Election-With-Kubernetes&lt;/p&gt;

&lt;!--
Kubernetes simplifies the deployment and operational management of services running on clusters. However, it also simplifies the development of these services. In this post we&#39;ll see how you can use Kubernetes to easily perform leader election in your distributed application. Distributed applications usually replicate the tasks of a service for reliability and scalability, but often it is necessary to designate one of the replicas as the leader who is responsible for coordination among all of the replicas.
--&gt;

&lt;p&gt;Kubernetes 简化了集群上运行的服务的部署和操作管理。但是，它也简化了这些服务的发展。在本文中，我们将看到如何使用 Kubernetes 在分布式应用程序中轻松地执行 leader election。分布式应用程序通常为了可靠性和可伸缩性而复制服务的任务，但通常需要指定其中一个副本作为负责所有副本之间协调的负责人。&lt;/p&gt;

&lt;!--
Typically in leader election, a set of candidates for becoming leader is identified. These candidates all race to declare themselves the leader. One of the candidates wins and becomes the leader. Once the election is won, the leader continually &#34;heartbeats&#34; to renew their position as the leader, and the other candidates periodically make new attempts to become the leader. This ensures that a new leader is identified quickly, if the current leader fails for some reason.
--&gt;

&lt;p&gt;通常在 leader election 中，会确定一组成为领导者的候选人。这些候选人都竞相宣布自己为领袖。其中一位候选人获胜并成为领袖。一旦选举获胜，领导者就会不断地“信号”以表示他们作为领导者的地位，其他候选人也会定期地做出新的尝试来成为领导者。这样可以确保在当前领导因某种原因失败时，快速确定新领导。&lt;/p&gt;

&lt;!--
Implementing leader election usually requires either deploying software such as ZooKeeper, etcd or Consul and using it for consensus, or alternately, implementing a consensus algorithm on your own. We will see below that Kubernetes makes the process of using leader election in your application significantly easier.

####  Implementing leader election in Kubernetes
--&gt;

&lt;p&gt;实现 leader election 通常需要部署 ZooKeeper、etcd 或 Consul 等软件并将其用于协商一致，或者也可以自己实现协商一致算法。我们将在下面看到，Kubernetes 使在应用程序中使用 leader election 的过程大大简化。&lt;/p&gt;

&lt;p&gt;####在 Kubernetes 实施领导人选举&lt;/p&gt;

&lt;!--
The first requirement in leader election is the specification of the set of candidates for becoming the leader. Kubernetes already uses _Endpoints_ to represent a replicated set of pods that comprise a service, so we will re-use this same object. (aside: You might have thought that we would use _ReplicationControllers_, but they are tied to a specific binary, and generally you want to have a single leader even if you are in the process of performing a rolling update)

To perform leader election, we use two properties of all Kubernetes API objects:
--&gt;

&lt;p&gt;Leader election 的首要条件是确定候选人的人选。Kubernetes 已经使用 &lt;em&gt;Endpoints&lt;/em&gt; 来表示组成服务的一组复制 pod，因此我们将重用这个相同的对象。（旁白：您可能认为我们会使用 _ReplicationControllers_，但它们是绑定到特定的二进制文件，而且通常您希望只有一个领导者，即使您正在执行滚动更新）&lt;/p&gt;

&lt;p&gt;要执行 leader election，我们使用所有 Kubernetes api 对象的两个属性：&lt;/p&gt;

&lt;!--
* ResourceVersions - Every API object has a unique ResourceVersion, and you can use these versions to perform compare-and-swap on Kubernetes objects
* Annotations - Every API object can be annotated with arbitrary key/value pairs to be used by clients.

Given these primitives, the code to use master election is relatively straightforward, and you can find it [here][1]. Let&#39;s run it ourselves.
--&gt;

&lt;ul&gt;
&lt;li&gt;ResourceVersions - 每个 API 对象都有一个惟一的 ResourceVersion，您可以使用这些版本对 Kubernetes 对象执行比较和交换&lt;/li&gt;
&lt;li&gt;Annotations - 每个 API 对象都可以用客户端使用的任意键/值对进行注释。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;给定这些原语，使用 master election 的代码相对简单，您可以在这里找到&lt;a href=&#34;https://github.com/kubernetes/contrib/pull/353&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;。我们自己来做吧。&lt;/p&gt;

&lt;!--
```
$ kubectl run leader-elector --image=gcr.io/google_containers/leader-elector:0.4 --replicas=3 -- --election=example
```

This creates a leader election set with 3 replicas:

```
$ kubectl get pods
NAME                   READY     STATUS    RESTARTS   AGE
leader-elector-inmr1   1/1       Running   0          13s
leader-elector-qkq00   1/1       Running   0          13s
leader-elector-sgwcq   1/1       Running   0          13s
```
--&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl run leader-elector --image=gcr.io/google_containers/leader-elector:0.4 --replicas=3 -- --election=example
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这将创建一个包含3个副本的 leader election 集合：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pods
NAME                   READY     STATUS    RESTARTS   AGE
leader-elector-inmr1   1/1       Running   0          13s
leader-elector-qkq00   1/1       Running   0          13s
leader-elector-sgwcq   1/1       Running   0          13s
&lt;/code&gt;&lt;/pre&gt;

&lt;!--
To see which pod was chosen as the leader, you can access the logs of one of the pods, substituting one of your own pod&#39;s names in place of

```
${pod_name}, (e.g. leader-elector-inmr1 from the above)

$ kubectl logs -f ${name}
leader is (leader-pod-name)
```
… Alternately, you can inspect the endpoints object directly:
--&gt;

&lt;p&gt;要查看哪个pod被选为领导，您可以访问其中一个 pod 的日志，用您自己的一个 pod 的名称替换&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;${pod_name}, (e.g. leader-elector-inmr1 from the above)

$ kubectl logs -f ${name}
leader is (leader-pod-name)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;…或者，可以直接检查 endpoints 对象：&lt;/p&gt;

&lt;!--
_&#39;example&#39; is the name of the candidate set from the above kubectl run … command_
```
$ kubectl get endpoints example -o yaml
```
Now to validate that leader election actually works, in a different terminal, run:

```
$ kubectl delete pods (leader-pod-name)
```
--&gt;

&lt;p&gt;_&amp;lsquo;example&amp;rsquo; 是上面 kubectl run … 命令_中候选集的名称&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get endpoints example -o yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在，要验证 leader election 是否实际有效，请在另一个终端运行：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl delete pods (leader-pod-name)
&lt;/code&gt;&lt;/pre&gt;

&lt;!--
This will delete the existing leader. Because the set of pods is being managed by a replication controller, a new pod replaces the one that was deleted, ensuring that the size of the replicated set is still three. Via leader election one of these three pods is selected as the new leader, and you should see the leader failover to a different pod. Because pods in Kubernetes have a _grace period_ before termination, this may take 30-40 seconds.

The leader-election container provides a simple webserver that can serve on any address (e.g. http://localhost:4040). You can test this out by deleting the existing leader election group and creating a new one where you additionally pass in a --http=(host):(port) specification to the leader-elector image. This causes each member of the set to serve information about the leader via a webhook.
--&gt;

&lt;p&gt;这将删除现有领导。由于 pod 集由 replication controller 管理，因此新的 pod 将替换已删除的pod，确保复制集的大小仍为3。通过 leader election，这三个pod中的一个被选为新的领导者，您应该会看到领导者故障转移到另一个pod。因为 Kubernetes 的吊舱在终止前有一个 _grace period_，这可能需要30-40秒。&lt;/p&gt;

&lt;p&gt;Leader-election container 提供了一个简单的 web 服务器，可以服务于任何地址(e.g. &lt;a href=&#34;http://localhost:4040)。您可以通过删除现有的&#34; target=&#34;_blank&#34;&gt;http://localhost:4040)。您可以通过删除现有的&lt;/a&gt; leader election 组并创建一个新的 leader elector 组来测试这一点，在该组中，您还可以向 leader elector 映像传递&amp;ndash;http=(host):(port) 规范。这将导致集合中的每个成员通过 webhook 提供有关领导者的信息。&lt;/p&gt;

&lt;!--
```
# delete the old leader elector group
$ kubectl delete rc leader-elector

# create the new group, note the --http=localhost:4040 flag
$ kubectl run leader-elector --image=gcr.io/google_containers/leader-elector:0.4 --replicas=3 -- --election=example --http=0.0.0.0:4040

# create a proxy to your Kubernetes api server
$ kubectl proxy
```
--&gt;

&lt;pre&gt;&lt;code&gt;# delete the old leader elector group
$ kubectl delete rc leader-elector

# create the new group, note the --http=localhost:4040 flag
$ kubectl run leader-elector --image=gcr.io/google_containers/leader-elector:0.4 --replicas=3 -- --election=example --http=0.0.0.0:4040

# create a proxy to your Kubernetes api server
$ kubectl proxy
&lt;/code&gt;&lt;/pre&gt;

&lt;!--
You can then access:


http://localhost:8001/api/v1/proxy/namespaces/default/pods/(leader-pod-name):4040/


And you will see:

```
{&#34;name&#34;:&#34;(name-of-leader-here)&#34;}
```
####  Leader election with sidecars
--&gt;

&lt;p&gt;然后您可以访问：&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://localhost:8001/api/v1/proxy/namespaces/default/pods/(leader-pod-name):4040/&#34; target=&#34;_blank&#34;&gt;http://localhost:8001/api/v1/proxy/namespaces/default/pods/(leader-pod-name):4040/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;你会看到：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{&amp;quot;name&amp;quot;:&amp;quot;(name-of-leader-here)&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;有副手的-leader-election&#34;&gt;有副手的 leader election&lt;/h4&gt;

&lt;!--
Ok, that&#39;s great, you can do leader election and find out the leader over HTTP, but how can you use it from your own application? This is where the notion of sidecars come in. In Kubernetes, Pods are made up of one or more containers. Often times, this means that you add sidecar containers to your main application to make up a Pod. (for a much more detailed treatment of this subject see my earlier blog post).

The leader-election container can serve as a sidecar that you can use from your own application. Any container in the Pod that&#39;s interested in who the current master is can simply access http://localhost:4040 and they&#39;ll get back a simple JSON object that contains the name of the current master. Since all containers in a Pod share the same network namespace, there&#39;s no service discovery required!
--&gt;

&lt;p&gt;好吧，那太好了，你可以通过 HTTP 进行leader election 并找到 leader，但是你如何从自己的应用程序中使用它呢？这就是 sidecar 的由来。Kubernetes  中，Pods 由一个或多个容器组成。通常情况下，这意味着您将 sidecar containers 添加到主应用程序中以组成 pod。（关于这个主题的更详细的处理，请参阅我之前的博客文章）。
Leader-election container 可以作为一个 sidecar，您可以从自己的应用程序中使用。Pod 中任何对当前 master 感兴趣的容器都可以简单地访问&lt;a href=&#34;http://localhost:4040，它们将返回一个包含当前&#34; target=&#34;_blank&#34;&gt;http://localhost:4040，它们将返回一个包含当前&lt;/a&gt; master 名称的简单 json 对象。由于 pod中 的所有容器共享相同的网络命名空间，因此不需要服务发现！&lt;/p&gt;

&lt;!--
For example, here is a simple Node.js application that connects to the leader election sidecar and prints out whether or not it is currently the master. The leader election sidecar sets its identifier to `hostname` by default.

```
var http = require(&#39;http&#39;);
// This will hold info about the current master
var master = {};

  // The web handler for our nodejs application
  var handleRequest = function(request, response) {
    response.writeHead(200);
    response.end(&#34;Master is &#34; + master.name);
  };

  // A callback that is used for our outgoing client requests to the sidecar
  var cb = function(response) {
    var data = &#39;&#39;;
    response.on(&#39;data&#39;, function(piece) { data = data + piece; });
    response.on(&#39;end&#39;, function() { master = JSON.parse(data); });
  };

  // Make an async request to the sidecar at http://localhost:4040
  var updateMaster = function() {
    var req = http.get({host: &#39;localhost&#39;, path: &#39;/&#39;, port: 4040}, cb);
    req.on(&#39;error&#39;, function(e) { console.log(&#39;problem with request: &#39; + e.message); });
    req.end();
  };

  / / Set up regular updates
  updateMaster();
  setInterval(updateMaster, 5000);

  // set up the web server
  var www = http.createServer(handleRequest);
  www.listen(8080);
  ```
  Of course, you can use this sidecar from any language that you choose that supports HTTP and JSON.
--&gt;

&lt;p&gt;例如，这里有一个简单的 Node.js 应用程序，它连接到 leader election sidecar 并打印出它当前是否是 master。默认情况下，leader election sidecar 将其标识符设置为 &lt;code&gt;hostname&lt;/code&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var http = require(&#39;http&#39;);
// This will hold info about the current master
var master = {};

  // The web handler for our nodejs application
  var handleRequest = function(request, response) {
    response.writeHead(200);
    response.end(&amp;quot;Master is &amp;quot; + master.name);
  };

  // A callback that is used for our outgoing client requests to the sidecar
  var cb = function(response) {
    var data = &#39;&#39;;
    response.on(&#39;data&#39;, function(piece) { data = data + piece; });
    response.on(&#39;end&#39;, function() { master = JSON.parse(data); });
  };

  // Make an async request to the sidecar at http://localhost:4040
  var updateMaster = function() {
    var req = http.get({host: &#39;localhost&#39;, path: &#39;/&#39;, port: 4040}, cb);
    req.on(&#39;error&#39;, function(e) { console.log(&#39;problem with request: &#39; + e.message); });
    req.end();
  };

  / / Set up regular updates
  updateMaster();
  setInterval(updateMaster, 5000);

  // set up the web server
  var www = http.createServer(handleRequest);
  www.listen(8080);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然，您可以从任何支持 HTTP 和 JSON 的语言中使用这个 sidecar。&lt;/p&gt;

&lt;!--
#### Conclusion


  Hopefully I&#39;ve shown you how easy it is to build leader election for your distributed application using Kubernetes. In future installments we&#39;ll show you how Kubernetes is making building distributed systems even easier. In the meantime, head over to [Google Container Engine][2] or [kubernetes.io][3] to get started with Kubernetes.




--&gt;

&lt;h4 id=&#34;结论&#34;&gt;结论&lt;/h4&gt;

&lt;p&gt;希望我已经向您展示了使用 Kubernetes 为您的分布式应用程序构建 leader election 是多么容易。在以后的部分中，我们将向您展示 Kubernetes 如何使构建分布式系统变得更加容易。同时，转到&lt;a href=&#34;https://cloud.google.com/container-engine/&#34; target=&#34;_blank&#34;&gt;Google Container Engine&lt;/a&gt;或&lt;a href=&#34;http://kubernetes.io/&#34; target=&#34;_blank&#34;&gt;kubernetes.io&lt;/a&gt;开始使用Kubernetes。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog:  使用 Puppet 管理 Kubernetes Pods，Services 和 Replication Controllers </title>
      <link>https://kubernetes.io/zh/blog/2015/12/17/managing-kubernetes-pods-services-and-replication-controllers-with-puppet/</link>
      <pubDate>Thu, 17 Dec 2015 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/zh/blog/2015/12/17/managing-kubernetes-pods-services-and-replication-controllers-with-puppet/</guid>
      <description>
        
        
        

&lt;!--
---
title: &#34; Managing Kubernetes Pods, Services and Replication Controllers with Puppet &#34;
date: 2015-12-17
slug: managing-kubernetes-pods-services-and-replication-controllers-with-puppet
url: /blog/2015/12/Managing-Kubernetes-Pods-Services-And-Replication-Controllers-With-Puppet
---
--&gt;

&lt;!--
_Today’s guest post is written by Gareth Rushgrove, Senior Software Engineer at Puppet Labs, a leader in IT automation. Gareth tells us about a new Puppet module that helps manage resources in Kubernetes.&amp;nbsp;_

People familiar with [Puppet](https://github.com/puppetlabs/puppet)&amp;nbsp;might have used it for managing files, packages and users on host computers. But Puppet is first and foremost a configuration management tool, and config management is a much broader discipline than just managing host-level resources. A good definition of configuration management is that it aims to solve four related problems: identification, control, status accounting and verification and audit. These problems exist in the operation of any complex system, and with the new [Puppet Kubernetes module](https://forge.puppetlabs.com/garethr/kubernetes)&amp;nbsp;we’re starting to look at how we can solve those problems for Kubernetes.
--&gt;

&lt;p&gt;&lt;em&gt;今天的嘉宾帖子是由 IT 自动化领域的领导者 Puppet Labs 的高级软件工程师 Gareth Rushgrove 撰写的。Gareth告诉我们一个新的 Puppet 模块，它帮助管理 Kubernetes 中的资源。&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;熟悉[Puppet]的人(&lt;a href=&#34;https://github.com/puppetlabs/puppet)可能使用它来管理主机上的文件、包和用户。但是Puppet首先是一个配置管理工具，配置管理是一个比管理主机级资源更广泛的规程。配置管理的一个很好的定义是它旨在解决四个相关的问题：标识、控制、状态核算和验证审计。这些问题存在于任何复杂系统的操作中，并且有了新的[Puppet&#34; target=&#34;_blank&#34;&gt;https://github.com/puppetlabs/puppet)可能使用它来管理主机上的文件、包和用户。但是Puppet首先是一个配置管理工具，配置管理是一个比管理主机级资源更广泛的规程。配置管理的一个很好的定义是它旨在解决四个相关的问题：标识、控制、状态核算和验证审计。这些问题存在于任何复杂系统的操作中，并且有了新的[Puppet&lt;/a&gt; Kubernetes module](&lt;a href=&#34;https://forge.puppetlabs.com/garethr/kubernetes)，我们开始研究如何为&#34; target=&#34;_blank&#34;&gt;https://forge.puppetlabs.com/garethr/kubernetes)，我们开始研究如何为&lt;/a&gt; Kubernetes 解决这些问题。&lt;/p&gt;

&lt;!--
### The Puppet Kubernetes Module

The Puppet Kubernetes module currently assumes you already have a Kubernetes cluster [up and running](http://kubernetes.io/gettingstarted/).&amp;nbsp;Its focus is on managing the resources in Kubernetes, like Pods, Replication Controllers and Services, not (yet) on managing the underlying kubelet or etcd services. Here’s a quick snippet of code describing a Pod in Puppet’s DSL.
--&gt;

&lt;h3 id=&#34;puppet-kubernetes-模块&#34;&gt;Puppet Kubernetes 模块&lt;/h3&gt;

&lt;p&gt;Puppet kubernetes 模块目前假设您已经有一个 kubernetes 集群 [启动并运行]](&lt;a href=&#34;http://kubernetes.io/gettingstarted/)。它的重点是管理&#34; target=&#34;_blank&#34;&gt;http://kubernetes.io/gettingstarted/)。它的重点是管理&lt;/a&gt; Kubernetes中的资源，如 Pods、Replication Controllers 和 Services，而不是（现在）管理底层的 kubelet 或 etcd services。下面是描述 Puppet’s DSL 中一个 Pod 的简短代码片段。&lt;/p&gt;

&lt;!--
```
kubernetes_pod { &#39;sample-pod&#39;:
  ensure =&gt; present,
  metadata =&gt; {
    namespace =&gt; &#39;default&#39;,
  },
  spec =&gt; {
    containers =&gt; [{
      name =&gt; &#39;container-name&#39;,
      image =&gt; &#39;nginx&#39;,
    }]
  },
```
}
--&gt;

&lt;pre&gt;&lt;code&gt;kubernetes_pod { &#39;sample-pod&#39;:
  ensure =&amp;gt; present,
  metadata =&amp;gt; {
    namespace =&amp;gt; &#39;default&#39;,
  },
  spec =&amp;gt; {
    containers =&amp;gt; [{
      name =&amp;gt; &#39;container-name&#39;,
      image =&amp;gt; &#39;nginx&#39;,
    }]
  },
}
&lt;/code&gt;&lt;/pre&gt;

&lt;!--
If you’re familiar with the YAML file format, you’ll probably recognise the structure immediately. The interface is intentionally identical to aid conversion between different formats — in fact, the code powering this is autogenerated from the Kubernetes API Swagger definitions. Running the above code, assuming we save it as pod.pp, is as simple as:


```
puppet apply pod.pp
```
--&gt;

&lt;p&gt;如果您熟悉 YAML 文件格式，您可能会立即识别该结构。 该接口故意采取相同的格式以帮助在不同格式之间进行转换 — 事实上，为此提供支持的代码是从Kubernetes API Swagger自动生成的。 运行上面的代码，假设我们将其保存为 pod.pp，就像下面这样简单：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;puppet apply pod.pp
&lt;/code&gt;&lt;/pre&gt;

&lt;!--
Authentication uses the standard kubectl configuration file. You can find complete [installation instructions in the module&#39;s README](https://github.com/garethr/garethr-kubernetes/blob/master/README.md).

Kubernetes has several resources, from Pods and Services to Replication Controllers and Service Accounts. You can see an example of the module managing these resources in the [Kubernetes guestbook sample in Puppet](https://puppetlabs.com/blog/kubernetes-guestbook-example-puppet)&amp;nbsp;post. This demonstrates converting the canonical hello-world example to use Puppet code.  --&gt;

&lt;p&gt;身份验证使用标准的 kubectl 配置文件。您可以在模块的自述文件中找到完整的&lt;a href=&#34;https://github.com/garethr/garethr-kubernetes/blob/master/README.md&#34; target=&#34;_blank&#34;&gt;README&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;Kubernetes 有很多资源，来自 Pods、 Services、 Replication Controllers 和 Service Accounts。您可以在&lt;a href=&#34;https://puppetlabs.com/blog/kubernetes-guestbook-example-puppet&#34; target=&#34;_blank&#34;&gt;Puppet 中的 kubernetes 留言簿示例&lt;/a&gt;文章中看到管理这些资源的模块示例。这演示了如何将规范的 hello-world 示例转换为使用 Puppet代码。&lt;/p&gt;

&lt;!--
One of the main advantages of using Puppet for this, however, is that you can create your own higher-level and more business-specific interfaces to Kubernetes-managed applications. For instance, for the guestbook, you could create something like the following:


```
guestbook { &#39;myguestbook&#39;:
  redis_slave_replicas =&gt; 2,
  frontend_replicas =&gt; 3,
  redis_master_image =&gt; &#39;redis&#39;,
  redis_slave_image =&gt; &#39;gcr.io/google_samples/gb-redisslave:v1&#39;,
  frontend_image =&gt; &#39;gcr.io/google_samples/gb-frontend:v3&#39;,
}
```
--&gt;

&lt;p&gt;然而，使用 Puppet 的一个主要优点是，您可以创建自己的更高级别和更特定于业务的接口，以连接 kubernetes 管理的应用程序。例如，对于留言簿，可以创建如下内容：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;guestbook { &#39;myguestbook&#39;:
  redis_slave_replicas =&amp;gt; 2,
  frontend_replicas =&amp;gt; 3,
  redis_master_image =&amp;gt; &#39;redis&#39;,
  redis_slave_image =&amp;gt; &#39;gcr.io/google_samples/gb-redisslave:v1&#39;,
  frontend_image =&amp;gt; &#39;gcr.io/google_samples/gb-frontend:v3&#39;,
}
&lt;/code&gt;&lt;/pre&gt;

&lt;!--
You can read more about using Puppet’s defined types, and see lots more code examples, in the Puppet blog post, [Building Your Own Abstractions for Kubernetes in Puppet](https://puppetlabs.com/blog/building-your-own-abstractions-kubernetes-puppet).


### Conclusions

The advantages of using Puppet rather than just the standard YAML files and kubectl are:
--&gt;

&lt;p&gt;您可以在Puppet博客文章&lt;a href=&#34;https://puppetlabs.com/blog/building-your-own-abstractions-kubernetes-puppet&#34; target=&#34;_blank&#34;&gt;在 Puppet 中为 Kubernetes 构建自己的抽象&lt;/a&gt;中阅读更多关于使用 Puppet 定义的类型的信息，并看到更多的代码示例。&lt;/p&gt;

&lt;h3 id=&#34;结论&#34;&gt;结论&lt;/h3&gt;

&lt;p&gt;使用 Puppet 而不仅仅是使用标准的 YAML 文件和 kubectl 的优点是：&lt;/p&gt;

&lt;!--
- The ability to create your own abstractions to cut down on repetition and craft higher-level user interfaces, like the guestbook example above.&amp;nbsp;
- Use of Puppet’s development tools for validating code and for writing unit tests.&amp;nbsp;
- Integration with other tools such as Puppet Server, for ensuring that your model in code matches the state of your cluster, and with PuppetDB for storing reports and tracking changes.
- The ability to run the same code repeatedly against the Kubernetes API, to detect any changes or remediate configuration drift.&amp;nbsp;
--&gt;

&lt;ul&gt;
&lt;li&gt;能够创建自己的抽象，以减少重复和设计更高级别的用户界面，如上面的留言簿示例。&lt;/li&gt;
&lt;li&gt;使用 Puppet 的开发工具验证代码和编写单元测试。&lt;/li&gt;
&lt;li&gt;与 Puppet Server 等其他工具配合，以确保代码中的模型与集群的状态匹配，并与 PuppetDB 配合工作，以存储报告和跟踪更改。&lt;/li&gt;
&lt;li&gt;能够针对 Kubernetes API 重复运行相同的代码，以检测任何更改或修正配置。&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
It’s also worth noting that most large organisations will have very heterogenous environments, running a wide range of software and operating systems. Having a single toolchain that unifies those discrete systems can make adopting new technology like Kubernetes much easier.
--&gt;

&lt;p&gt;值得注意的是，大多数大型组织都将拥有非常异构的环境，运行各种各样的软件和操作系统。拥有统一这些离散系统的单一工具链可以使采用 Kubernetes 等新技术变得更加容易。&lt;/p&gt;

&lt;!--
It’s safe to say that Kubernetes provides an excellent set of primitives on which to build cloud-native systems. And with Puppet, you can address some of the operational and configuration management issues that come with running any complex system in production. [Let us know](mailto:gareth@puppetlabs.com)&amp;nbsp;what you think if you try the module out, and what else you’d like to see supported in the future.

&amp;nbsp;-&amp;nbsp;Gareth Rushgrove, Senior Software Engineer, Puppet Labs
--&gt;

&lt;p&gt;可以肯定地说，Kubernetes提供了一组优秀的组件来构建云原生系统。使用 Puppet，您可以解决在生产中运行任何复杂系统所带来的一些操作和配置管理问题。&lt;a href=&#34;mailto:gareth@puppetlabs.com&#34; target=&#34;_blank&#34;&gt;告诉我们&lt;/a&gt;如果您试用了该模块，您会有什么想法，以及您希望在将来看到哪些支持。&lt;/p&gt;

&lt;p&gt;Gareth Rushgrove，Puppet Labs 高级软件工程师&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: 宣布首个Kubernetes企业培训课程 </title>
      <link>https://kubernetes.io/zh/blog/2015/07/08/announcing-first-kubernetes-enterprise/</link>
      <pubDate>Wed, 08 Jul 2015 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/zh/blog/2015/07/08/announcing-first-kubernetes-enterprise/</guid>
      <description>
        
        
        &lt;!-- ---
title: &#34; Announcing the First Kubernetes Enterprise Training Course &#34;
date: 2015-07-08
slug: announcing-first-kubernetes-enterprise
url: /blog/2015/07/Announcing-First-Kubernetes-Enterprise
--- --&gt;

&lt;!-- At Google we rely on Linux application containers to run our core infrastructure. Everything from Search to Gmail runs in containers. &amp;nbsp;In fact, we like containers so much that even our Google Compute Engine VMs run in containers! &amp;nbsp;Because containers are critical to our business, we have been working with the community on many of the basic container technologies (from cgroups to Docker’s LibContainer) and even decided to build the next generation of Google’s container scheduling technology, Kubernetes, in the open. --&gt;

&lt;p&gt;在谷歌，我们依赖 Linux 容器应用程序去运行我们的核心基础架构。所有服务，从搜索引擎到Gmail服务，都运行在容器中。事实上，我们非常喜欢容器，甚至我们的谷歌云计算引擎虚拟机也运行在容器上！由于容器对于我们的业务至关重要，我们已经与社区合作开发许多基本的容器技术（从 cgroups 到 Docker 的 LibContainer）,甚至决定去构建谷歌的下一代开源容器调度技术，Kubernetes。&lt;/p&gt;

&lt;!-- One year into the Kubernetes project, and on the eve of our planned V1 release at OSCON, we are pleased to announce the first-ever formal Kubernetes enterprise-focused training session organized by a key Kubernetes contributor, Mesosphere. The inaugural session will be taught by Zed Shaw and Michael Hausenblas from Mesosphere, and will take place on July 20 at OSCON in Portland. [Pre-registration](https://mesosphere.com/training/kubernetes/) is free for early registrants, but space is limited so act soon! --&gt;

&lt;p&gt;在 Kubernetes 项目进行一年后，在 OSCON 上发布 V1 版本的前夕，我们很高兴的宣布Kubernetes 的主要贡献者 Mesosphere 组织了有史以来第一次正规的以企业为中心的 Kubernetes 培训会议。首届会议将于 6 月 20 日在波特兰的 OSCON 举办，由来自 Mesosphere 的 Zed Shaw 和 Michael Hausenblas 演讲。&lt;a href=&#34;https://mesosphere.com/training/kubernetes/&#34; target=&#34;_blank&#34;&gt;Pre-registration&lt;/a&gt; 对于优先注册者是免费的，但名额有限，立刻行动吧！&lt;/p&gt;

&lt;!-- This one-day course will cover the basics of building and deploying containerized applications using Kubernetes. It will walk attendees through the end-to-end process of creating a Kubernetes application architecture, building and configuring Docker images, and deploying them on a Kubernetes cluster. Users will also learn the fundamentals of deploying Kubernetes applications and services on our Google Container Engine and Mesosphere’s Datacenter Operating System. --&gt;

&lt;p&gt;这个为期一天的课程将包涵使用 Kubernetes 构建和部署容器化应用程序的基础知识。它将通过完整的流程引导与参会者创建一个 Kubernetes 的应用程序体系结构，创建和配置 Docker 镜像，并把它们部署到 Kubernetes 集群上。用户还将了解在我们的谷歌容器引擎和 Mesosphere 的数据中心操作系统上部署 Kubernetes 应用程序和服务的基础知识。&lt;/p&gt;

&lt;!-- The upcoming Kubernetes bootcamp will be a great way to learn how to apply Kubernetes to solve long-standing deployment and application management problems. &amp;nbsp;This is just the first of what we hope are many, and from a broad set of contributors. --&gt;

&lt;p&gt;即将推出的 Kubernetes bootcamp 将是学习如何应用 Kubernetes 解决长期部署和应用程序管理问题的一个好途径。相对于我们所预期的，来自于广泛社区的众多培训项目而言，这只是其中一个。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: 幻灯片：Kubernetes 集群管理，爱丁堡大学演讲</title>
      <link>https://kubernetes.io/zh/blog/2015/06/26/slides-cluster-management-with/</link>
      <pubDate>Fri, 26 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/zh/blog/2015/06/26/slides-cluster-management-with/</guid>
      <description>
        
        
        &lt;!--
---
title: &#34; Slides: Cluster Management with Kubernetes, talk given at the University of Edinburgh &#34;
date: 2015-06-26
slug: slides-cluster-management-with
url: /blog/2015/06/Slides-Cluster-Management-With
---
--&gt;

&lt;!--
On Friday 5 June 2015 I gave a talk called [Cluster Management with Kubernetes](https://docs.google.com/presentation/d/1H4ywDb4vAJeg8KEjpYfhNqFSig0Q8e_X5I36kM9S6q0/pub?start=false&amp;loop=false&amp;delayms=3000) to a general audience at the University of Edinburgh. The talk includes an example of a music store system with a Kibana front end UI and an Elasticsearch based back end which helps to make concrete concepts like pods, replication controllers and services.

[Cluster Management with Kubernetes](https://docs.google.com/presentation/d/1H4ywDb4vAJeg8KEjpYfhNqFSig0Q8e_X5I36kM9S6q0/pub?start=false&amp;loop=false&amp;delayms=3000).
--&gt;

&lt;p&gt;2015年6月5日星期五，我在爱丁堡大学给普通听众做了一个演讲，题目是&lt;a href=&#34;https://docs.google.com/presentation/d/1H4ywDb4vAJeg8KEjpYfhNqFSig0Q8e_X5I36kM9S6q0/pub?start=false&amp;amp;loop=false&amp;amp;delayms=3000&#34; target=&#34;_blank&#34;&gt;使用 Kubernetes 进行集群管理&lt;/a&gt;。这次演讲包括一个带有 Kibana 前端 UI 的音乐存储系统的例子，以及一个基于 Elasticsearch 的后端，该后端有助于生成具体的概念，如 pods、复制控制器和服务。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/1H4ywDb4vAJeg8KEjpYfhNqFSig0Q8e_X5I36kM9S6q0/pub?start=false&amp;amp;loop=false&amp;amp;delayms=3000&#34; target=&#34;_blank&#34;&gt;Kubernetes 集群管理&lt;/a&gt;。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog:  OpenStack 上的 Kubernetes </title>
      <link>https://kubernetes.io/zh/blog/2015/05/19/kubernetes-on-openstack/</link>
      <pubDate>Tue, 19 May 2015 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/zh/blog/2015/05/19/kubernetes-on-openstack/</guid>
      <description>
        
        
        

&lt;!--
---
title: &#34; Kubernetes on OpenStack &#34;
date: 2015-05-19
slug: kubernetes-on-openstack
url: /blog/2015/05/Kubernetes-On-Openstack
---
--&gt;

&lt;p&gt;&lt;a href=&#34;https://3.bp.blogspot.com/-EOrCHChZJZE/VVZzq43g6CI/AAAAAAAAF-E/JUilRHk369E/s1600/Untitled%2Bdrawing.jpg&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://3.bp.blogspot.com/-EOrCHChZJZE/VVZzq43g6CI/AAAAAAAAF-E/JUilRHk369E/s400/Untitled%2Bdrawing.jpg&#34; alt=&#34;&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;!--
Today, the [OpenStack foundation](https://www.openstack.org/foundation/) made it even easier for you deploy and manage clusters of Docker containers on OpenStack clouds by including Kubernetes in its [Community App Catalog](http://apps.openstack.org/). &amp;nbsp;At a keynote today at the OpenStack Summit in Vancouver, Mark Collier, COO of the OpenStack Foundation, and Craig Peters, &amp;nbsp;[Mirantis](https://www.mirantis.com/) product line manager, demonstrated the Community App Catalog workflow by launching a Kubernetes cluster in a matter of seconds by leveraging the compute, storage, networking and identity systems already present in an OpenStack cloud.
--&gt;

&lt;p&gt;今天，&lt;a href=&#34;https://www.openstack.org/foundation/&#34; target=&#34;_blank&#34;&gt;OpenStack 基金会&lt;/a&gt;通过在其&lt;a href=&#34;http://apps.openstack.org/&#34; target=&#34;_blank&#34;&gt;社区应用程序目录&lt;/a&gt;中包含 Kubernetes，使您更容易在 OpenStack 云上部署和管理 Docker 容器集群。
今天在温哥华 OpenStack 峰会上的主题演讲中，OpenStack 基金会的首席运营官：Mark Collier 和 &lt;a href=&#34;https://www.mirantis.com/&#34; target=&#34;_blank&#34;&gt;Mirantis&lt;/a&gt; 产品线经理 Craig Peters 通过利用 OpenStack 云中已经存在的计算、存储、网络和标识系统，在几秒钟内启动了 Kubernetes 集群，展示了社区应用程序目录的工作流。&lt;/p&gt;

&lt;!--
The entries in the catalog include not just the ability to [start a Kubernetes cluster](http://apps.openstack.org/#tab=murano-apps&amp;asset=Kubernetes%20Cluster), but also a range of applications deployed in Docker containers managed by Kubernetes. These applications include:
--&gt;

&lt;p&gt;目录中的条目不仅包括&lt;a href=&#34;http://apps.openstack.org/#tab=murano-apps&amp;amp;asset=Kubernetes%20Cluster&#34; target=&#34;_blank&#34;&gt;启动 Kubernetes 集群&lt;/a&gt;的功能，还包括部署在 Kubernetes 管理的 Docker 容器中的一系列应用程序。这些应用包括：&lt;/p&gt;

&lt;!--

-
Apache web server
-
Nginx web server
-
Crate - The Distributed Database for Docker
-
GlassFish - Java EE 7 Application Server
-
Tomcat - An open-source web server and servlet container
-
InfluxDB - An open-source, distributed, time series database
-
Grafana - Metrics dashboard for InfluxDB
-
Jenkins - An extensible open source continuous integration server
-
MariaDB database
-
MySql database
-
Redis - Key-value cache and store
-
PostgreSQL database
-
MongoDB NoSQL database
-
Zend Server - The Complete PHP Application Platform

--&gt;

&lt;p&gt;-&lt;/p&gt;

&lt;h2 id=&#34;apache-web-服务器&#34;&gt;Apache web 服务器&lt;/h2&gt;

&lt;h2 id=&#34;nginx-web-服务器&#34;&gt;Nginx web 服务器&lt;/h2&gt;

&lt;h2 id=&#34;crate-docker的分布式数据库&#34;&gt;Crate - Docker的分布式数据库&lt;/h2&gt;

&lt;h2 id=&#34;glassfish-java-ee-7-应用服务器&#34;&gt;GlassFish - Java EE 7 应用服务器&lt;/h2&gt;

&lt;h2 id=&#34;tomcat-一个开源的-web-服务器和-servlet-容器&#34;&gt;Tomcat - 一个开源的 web 服务器和 servlet 容器&lt;/h2&gt;

&lt;h2 id=&#34;influxdb-一个开源的-分布式的-时间序列数据库&#34;&gt;InfluxDB - 一个开源的、分布式的、时间序列数据库&lt;/h2&gt;

&lt;h2 id=&#34;grafana-influxdb-的度量仪表板&#34;&gt;Grafana -   InfluxDB 的度量仪表板&lt;/h2&gt;

&lt;h2 id=&#34;jenkins-一个可扩展的开放源码持续集成服务器&#34;&gt;Jenkins - 一个可扩展的开放源码持续集成服务器&lt;/h2&gt;

&lt;h2 id=&#34;mariadb-数据库&#34;&gt;MariaDB 数据库&lt;/h2&gt;

&lt;h2 id=&#34;mysql-数据库&#34;&gt;MySql 数据库&lt;/h2&gt;

&lt;h2 id=&#34;redis-键-值缓存和存储&#34;&gt;Redis - 键-值缓存和存储&lt;/h2&gt;

&lt;h2 id=&#34;postgresql-数据库&#34;&gt;PostgreSQL 数据库&lt;/h2&gt;

&lt;h2 id=&#34;mongodb-nosql-数据库&#34;&gt;MongoDB NoSQL 数据库&lt;/h2&gt;

&lt;p&gt;Zend 服务器 - 完整的 PHP 应用程序平台&lt;/p&gt;

&lt;!--
This list will grow, and is curated [here](https://opendev.org/x/k8s-docker-suite-app-murano/src/branch/master/Kubernetes). You can examine (and contribute to) the YAML file that tells Murano how to install and start the Kubernetes cluster [here](https://opendev.org/x/k8s-docker-suite-app-murano/src/branch/master/Kubernetes/KubernetesCluster/package/Classes/KubernetesCluster.yaml).
--&gt;

&lt;p&gt;此列表将会增长，并在&lt;a href=&#34;https://opendev.org/x/k8s-docker-suite-app-murano/src/branch/master/Kubernetes&#34; target=&#34;_blank&#34;&gt;此处&lt;/a&gt;进行策划。您可以检查（并参与）YAML 文件，该文件告诉 Murano 如何根据&lt;a href=&#34;https://opendev.org/x/k8s-docker-suite-app-murano/src/branch/master/Kubernetes/KubernetesCluster/package/Classes/KubernetesCluster.yaml&#34; target=&#34;_blank&#34;&gt;此处&lt;/a&gt;定义来安装和启动 &amp;hellip;apps/blob/master/Docker/Kubernetes/KubernetesCluster/package/Classes/KubernetesCluster.yaml)安装和启动 Kubernetes 集群。&lt;/p&gt;

&lt;!--
[The Kubernetes open source project](https://github.com/GoogleCloudPlatform/kubernetes) has continued to see fantastic community adoption and increasing momentum, with over 11,000 commits and 7,648 stars on GitHub. With supporters ranging from Red Hat and Intel to CoreOS and Box.net, it has come to represent a range of customer interests ranging from enterprise IT to cutting edge startups. We encourage you to give it a try, give us your feedback, and get involved in our growing community.
--&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/GoogleCloudPlatform/kubernetes&#34; target=&#34;_blank&#34;&gt;Kubernetes 开源项目&lt;/a&gt;继续受到社区的欢迎，并且势头越来越好，GitHub 上有超过 11000 个提交和 7648 颗星。从 Red Hat 和 Intel 到 CoreOS 和 Box.net，它已经代表了从企业 IT 到前沿创业企业的一系列客户。我们鼓励您尝试一下，给我们您的反馈，并参与到我们不断增长的社区中来。&lt;/p&gt;

&lt;!--

- Martin Buhr, Product Manager, Kubernetes Open Source Project

--&gt;

&lt;ul&gt;
&lt;li&gt;Martin Buhr, Kubernetes 开源项目产品经理&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog:  Kubernetes 社区每周聚会笔记- 2015年5月1日 </title>
      <link>https://kubernetes.io/zh/blog/2015/05/11/weekly-kubernetes-community-hangout/</link>
      <pubDate>Mon, 11 May 2015 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/zh/blog/2015/05/11/weekly-kubernetes-community-hangout/</guid>
      <description>
        
        
        &lt;!--
---
title: &#34; Weekly Kubernetes Community Hangout Notes - May 1 2015 &#34;
date: 2015-05-11
slug: weekly-kubernetes-community-hangout
url: /blog/2015/05/Weekly-Kubernetes-Community-Hangout
---
--&gt;

&lt;!--
Every week the Kubernetes contributing community meet virtually over Google Hangouts. We want anyone who&#39;s interested to know what&#39;s discussed in this forum.
--&gt;

&lt;p&gt;每个星期，Kubernetes 贡献者社区几乎都会在谷歌 Hangouts 上聚会。我们希望任何对此感兴趣的人都能了解这个论坛的讨论内容。&lt;/p&gt;

&lt;!--

* Simple rolling update - Brendan

    * Rolling update = nice example of why RCs and Pods are good.

    * ...pause… (Brendan needs demo recovery tips from Kelsey)

    * Rolling update has recovery: Cancel update and restart, update continues from where it stopped.

    * New controller  gets name of old controller, so appearance is pure update.

    * Can also name versions in update (won&#39;t do rename at the end).

--&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;简单的滚动更新 - Brendan&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;滚动更新 = RCs和Pods很好的例子。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&amp;hellip;pause… (Brendan 需要 Kelsey 的演示恢复技巧)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;滚动更新具有恢复功能:取消更新并重新启动，更新从停止的地方继续。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;新控制器获取旧控制器的名称，因此外观是纯粹的更新。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;还可以在 update 中命名版本(最后不会重命名)。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!--

* Rocket demo - CoreOS folks

    * 2 major differences between rocket &amp; docker: Rocket is daemonless &amp; pod-centric.

    * Rocket has AppContainer format as native, but also supports docker image format.

    * Can run AppContainer and docker containers in same pod.

    * Changes are close to merged.

--&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Rocket 演示 - CoreOS 的伙计们&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Rocket 和 docker 之间的主要区别: Rocket 是无守护进程和以 pod 为中心。。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Rocket 具有原生的 AppContainer 格式，但也支持 docker 镜像格式。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;可以在同一个 pod 中运行 AppContainer 和 docker 容器。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;变更接近于合并。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!--

* demo service accounts and secrets being added to pods - Jordan

    * Problem: It&#39;s hard to get a token to talk to the API.

    * New API object: &#34;ServiceAccount&#34;

    * ServiceAccount is namespaced, controller makes sure that at least 1 default service account exists in a namespace.

    * Typed secret &#34;ServiceAccountToken&#34;, controller makes sure there is at least 1 default token.

    * DEMO

    *     * Can create new service account with ServiceAccountToken. Controller will create token for it.

    * Can create a pod with service account, pods will have service account secret mounted at /var/run/secrets/kubernetes.io/…

--&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;演示 service accounts 和 secrets 被添加到 pod - Jordan&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;问题：很难获得与API通信的令牌。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;新的API对象：&amp;rdquo;ServiceAccount&amp;rdquo;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ServiceAccount 是命名空间，控制器确保命名空间中至少存在一个个默认 service account。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;键入 &amp;ldquo;ServiceAccountToken&amp;rdquo;，控制器确保至少有一个默认令牌。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;演示&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;ul&gt;
&lt;li&gt;可以使用 ServiceAccountToken 创建新的 service account。控制器将为它创建令牌。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;可以创建一个带有 service account 的 pod, pod 将在 /var/run/secrets/kubernets.io/…&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!--

* Kubelet running in a container - Paul

    * Kubelet successfully ran pod w/ mounted secret.

--&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Kubelet 在容器中运行 - Paul&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Kubelet 成功地运行了带有 secret 的 pod。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog:  Kubernetes 社区每周聚会笔记- 2015年4月24日 </title>
      <link>https://kubernetes.io/zh/blog/2015/04/30/weekly-kubernetes-community-hangout_29/</link>
      <pubDate>Thu, 30 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/zh/blog/2015/04/30/weekly-kubernetes-community-hangout_29/</guid>
      <description>
        
        
        &lt;!--
---
title: &#34; Weekly Kubernetes Community Hangout Notes - April 24 2015 &#34;
date: 2015-04-30
slug: weekly-kubernetes-community-hangout_29
url: /blog/2015/04/Weekly-Kubernetes-Community-Hangout_29
---

--&gt;

&lt;!--
Every week the Kubernetes contributing community meet virtually over Google Hangouts. We want anyone who&#39;s interested to know what&#39;s discussed in this forum.
--&gt;

&lt;p&gt;每个星期，Kubernetes 贡献者社区几乎都会在谷歌 Hangouts 上聚会。我们希望任何对此感兴趣的人都能了解这个论坛的讨论内容。&lt;/p&gt;

&lt;!--
Agenda:

* Flocker and Kubernetes integration demo

--&gt;

&lt;p&gt;日程安排：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Flocker 和 Kubernetes 集成演示&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
Notes:

* flocker and kubernetes integration demo
* * Flocker Q/A

    * Does the file still exists on node1 after migration?

    * Brendan: Any plan this to make it a volume? So we don&#39;t need powerstrip?

        * Luke:  Need to figure out interest to decide if we want to make it a first-class persistent disk provider in kube.

        * Brendan: Removing need for powerstrip would make it simple to use. Totally go for it.

        * Tim: Should take no more than 45 minutes to add it to kubernetes:)

--&gt;

&lt;p&gt;笔记：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;flocker 和 kubernetes 集成演示&lt;/li&gt;

&lt;li&gt;&lt;ul&gt;
&lt;li&gt;Flocker Q/A&lt;/li&gt;
&lt;/ul&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;迁移后文件是否仍存在于node1上？&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Brendan: 有没有计划把它做成一本书？我们不需要 powerstrip？&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Luke:  需要找出感兴趣的来决定我们是否想让它成为 kube 中的一个一流的持久性磁盘提供商。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Brendan: 删除对 powerstrip 的需求会使其易于使用。完全去做。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Tim: 将它添加到 kubernetes 应该不超过45分钟:)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!--

    * Derek: Contrast this with persistent volumes and claims?

        * Luke: Not much difference, except for the novel ZFS based backend. Makes workloads really portable.

        * Tim: very different than network-based volumes. Its interesting that it is the only offering that allows upgrading media.

        * Brendan: claims, how does it look for replicated claims? eg Cassandra wants to have replicated data underneath. It would be efficient to scale up and down. Create storage on the fly based on load dynamically. Its step beyond taking snapshots - programmatically creating replicas with preallocation.

        * Tim: helps with auto-provisioning.

--&gt;

&lt;pre&gt;&lt;code&gt;* Derek: 持久卷和请求相比呢?

    * Luke: 除了基于 ZFS 的新后端之外，差别不大。使工作负载真正可移植。

    * Tim: 与基于网络的卷非常不同。有趣的是，它是唯一允许升级媒体的产品。

    * Brendan: 请求，它如何查找重复请求？Cassandra 希望在底层复制数据。向上和向下扩缩是有效的。根据负载动态地创建存储。它的步骤不仅仅是快照——通过编程使用预分配创建副本。

    * Tim: 帮助自动配置。
&lt;/code&gt;&lt;/pre&gt;

&lt;!--

    * Brian: Does flocker requires any other component?

        * Kai: Flocker control service co-located with the master.  (dia on blog post). Powerstrip + Powerstrip Flocker. Very interested in mpersisting state in etcd. It keeps metadata about each volume.

        * Brendan: In future, flocker can be a plugin and we&#39;ll take care of persistence. Post v1.0.

        * Brian: Interested in adding generic plugin for services like flocker.

        * Luke: Zfs can become really valuable when scaling to lot of containers on a single node.

--&gt;

&lt;pre&gt;&lt;code&gt;* Brian: flocker 是否需要其他组件？

    * Kai: Flocker 控制服务与主服务器位于同一位置。(dia 在博客上)。Powerstrip + Powerstrip Flocker。对在 etcd 中持久化状态非常有趣。它保存关于每个卷的元数据。

    * Brendan: 在未来，flocker 可以是一个插件，我们将负责持久性。发布 v1.0。

    * Brian: 有兴趣为 flocker 等服务添加通用插件。

    * Luke: 当扩展到单个节点上的许多容器时，Zfs 会变得非常有价值。
&lt;/code&gt;&lt;/pre&gt;

&lt;!--

    * Alex: Can flocker service can be run as a pod?

        * Kai: Yes, only requirement is the flocker control service should be able to talk to zfs agent. zfs agent needs to be installed on the host and zfs binaries need to be accessible.

        * Brendan: In theory, all zfs bits can be put it into a container with devices.

        * Luke: Yes, still working through cross-container mounting issue.

        * Tim: pmorie is working through it to make kubelet work in a container. Possible re-use.

    * Kai: Cinder support is coming. Few days away.
* Bob: What&#39;s the process of pushing kube to GKE? Need more visibility for confidence.

--&gt;

&lt;pre&gt;&lt;code&gt;* Alex: flocker 服务可以作为 pod 运行吗？

    * Kai: 是的，唯一的要求是 flocker 控制服务应该能够与 zfs 代理对话。需要在主机上安装 zfs 代理，并且需要访问 zfs 二进制文件。

    * Brendan: 从理论上讲，所有 zfs 位都可以与设备一起放入容器中。

    * Luke: 是的，仍然在处理跨容器安装问题。

    * Tim: pmorie 正在通过它使 kubelet 在容器中工作。可能重复使用。

* Kai: Cinder 支持即将到来。几天之后。
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Bob: 向 GKE 推送 kube 的过程是怎样的？需要更多的可见度。&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog:  Kubernetes 社区每周聚会笔记- 2015年4月17日 </title>
      <link>https://kubernetes.io/zh/blog/2015/04/17/weekly-kubernetes-community-hangout_17/</link>
      <pubDate>Fri, 17 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/zh/blog/2015/04/17/weekly-kubernetes-community-hangout_17/</guid>
      <description>
        
        
        &lt;!--
---
title: &#34; Weekly Kubernetes Community Hangout Notes - April 17 2015 &#34;
date: 2015-04-17
slug: weekly-kubernetes-community-hangout_17
url: /blog/2015/04/Weekly-Kubernetes-Community-Hangout_17
---
--&gt;

&lt;!--
Every week the Kubernetes contributing community meet virtually over Google Hangouts. We want anyone who&#39;s interested to know what&#39;s discussed in this forum.
--&gt;

&lt;p&gt;每个星期，Kubernetes 贡献者社区几乎都会在谷歌 Hangouts 上聚会。我们希望任何对此感兴趣的人都能了解这个论坛的讨论内容。&lt;/p&gt;

&lt;!--
Agenda

* Mesos Integration
* High Availability (HA)
* Adding performance and profiling details to e2e to track regressions
* Versioned clients

--&gt;

&lt;p&gt;议程&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Mesos 集成&lt;/li&gt;
&lt;li&gt;高可用性（HA）&lt;/li&gt;
&lt;li&gt;向 e2e 添加性能和分析详细信息以跟踪回归&lt;/li&gt;
&lt;li&gt;客户端版本化&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
Notes
--&gt;

&lt;p&gt;笔记&lt;/p&gt;

&lt;!--

* Mesos integration

    * Mesos integration proposal:

    * No blockers to integration.

    * Documentation needs to be updated.

--&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Mesos 集成&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Mesos 集成提案：&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;没有阻塞集成的因素。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;文档需要更新。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!--

* HA

    * Proposal should land today.

    * Etcd cluster.

    * Load-balance apiserver.

    * Cold standby for controller manager and other master components.

--&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;HA&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;提案今天应该会提交。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Etcd 集群。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;apiserver 负载均衡。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;控制器管理器和其他主组件的冷备用。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!--

* Adding performance and profiling details to e2e to track regression

    * Want red light for performance regression

    * Need a public DB to post the data

        * See

    * Justin working on multi-platform e2e dashboard

--&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;向 e2e 添加性能和分析详细信息以跟踪回归&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;希望红色为性能回归&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;需要公共数据库才能发布数据&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;查看&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Justin 致力于多平台 e2e 仪表盘&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!--

* Versioned clients

    *

    *

    * Client library currently uses internal API objects.

    * Nobody reported that frequent changes to types.go have been painful, but we are worried about it.

    * Structured types are useful in the client. Versioned structs would be ok.

    * If start with json/yaml (kubectl), shouldn’t convert to structured types. Use swagger.

--&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;客户端版本化&lt;/p&gt;

&lt;p&gt;*&lt;/p&gt;

&lt;p&gt;*&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;客户端库当前使用内部 API 对象。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;尽管没有人反映频繁修改 &lt;code&gt;types.go&lt;/code&gt; 有多痛苦，但我们很为此担心。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;结构化类型在客户端中很有用。版本化的结构就可以了。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;如果从 json/yaml (kubectl) 开始，则不应转换为结构化类型。使用 swagger。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!--

* Security context

    *

    * Administrators can restrict who can run privileged containers or require specific unix uids

    * Kubelet will be able to get pull credentials from apiserver

    * Policy proposal coming in the next week or so
--&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Security context&lt;/p&gt;

&lt;p&gt;*&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;管理员可以限制谁可以运行特权容器或需要特定的 unix uid&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;kubelet 将能够从 apiserver 获取证书&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;政策提案将于下周左右出台&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!--

* Discussing upstreaming of users, etc. into Kubernetes, at least as optional
* 1.0 Roadmap

    * Focus is performance, stability, cluster upgrades

    * TJ has been making some edits to [roadmap.md][4] but hasn’t sent out a PR yet
* Kubernetes UI

    * Dependencies broken out into third-party

    * @lavalamp is reviewer

--&gt;

&lt;ul&gt;
&lt;li&gt;讨论用户的上游，等等进入Kubernetes，至少是可选的&lt;/li&gt;

&lt;li&gt;&lt;p&gt;1.0 路线图&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;重点是性能，稳定性，集群升级&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;TJ 一直在对&lt;a href=&#34;https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/roadmap.md&#34; target=&#34;_blank&#34;&gt;roadmap.md&lt;/a&gt;进行一些编辑，但尚未发布PR&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Kubernetes UI&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;依赖关系分解为第三方&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;@lavalamp 是评论家&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog:  Kubernetes Release: 0.15.0 </title>
      <link>https://kubernetes.io/zh/blog/2015/04/16/kubernetes-release-0150/</link>
      <pubDate>Thu, 16 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/zh/blog/2015/04/16/kubernetes-release-0150/</guid>
      <description>
        
        
        &lt;!--
Release Notes:
--&gt;

&lt;p&gt;Release 说明：&lt;/p&gt;

&lt;!--

* Enables v1beta3 API and sets it to the default API version ([#6098][1])
* Added multi-port Services ([#6182][2])
    * New Getting Started Guides
    * Multi-node local startup guide ([#6505][3])
    * Mesos on Google Cloud Platform ([#5442][4])
    * Ansible Setup instructions ([#6237][5])
* Added a controller framework ([#5270][6], [#5473][7])
* The Kubelet now listens on a secure HTTPS port ([#6380][8])
* Made kubectl errors more user-friendly ([#6338][9])
* The apiserver now supports client cert authentication ([#6190][10])
* The apiserver now limits the number of concurrent requests it processes ([#6207][11])
* Added rate limiting to pod deleting ([#6355][12])
* Implement Balanced Resource Allocation algorithm as a PriorityFunction in scheduler package ([#6150][13])
* Enabled log collection from master ([#6396][14])
* Added an api endpoint to pull logs from Pods ([#6497][15])
* Added latency metrics to scheduler ([#6368][16])
* Added latency metrics to REST client ([#6409][17])

--&gt;

&lt;ul&gt;
&lt;li&gt;启用 1beta3 API 并将其设置为默认 API 版本 (&lt;a href=&#34;https://github.com/GoogleCloudPlatform/kubernetes/pull/6098&#34; title=&#34;在 master 中默认启用 v1beta3 api 版本&#34; target=&#34;_blank&#34;&gt;#6098&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;增加了多端口服务(&lt;a href=&#34;https://github.com/GoogleCloudPlatform/kubernetes/pull/6182&#34; title=&#34;实现多端口服务&#34; target=&#34;_blank&#34;&gt;#6182&lt;/a&gt;)

&lt;ul&gt;
&lt;li&gt;新入门指南&lt;/li&gt;
&lt;li&gt;多节点本地启动指南 (&lt;a href=&#34;https://github.com/GoogleCloudPlatform/kubernetes/pull/6505&#34; title=&#34;Docker 多节点&#34; target=&#34;_blank&#34;&gt;#6505&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Google 云平台上的 Mesos (&lt;a href=&#34;https://github.com/GoogleCloudPlatform/kubernetes/pull/5442&#34; title=&#34;谷歌云平台上 Mesos 入门指南&#34; target=&#34;_blank&#34;&gt;#5442&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Ansible 安装说明 (&lt;a href=&#34;https://github.com/GoogleCloudPlatform/kubernetes/pull/6237&#34; title=&#34;示例 ansible 设置仓库&#34; target=&#34;_blank&#34;&gt;#6237&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;添加了一个控制器框架 (&lt;a href=&#34;https://github.com/GoogleCloudPlatform/kubernetes/pull/5270&#34; title=&#34;控制器框架&#34; target=&#34;_blank&#34;&gt;#5270&lt;/a&gt;, &lt;a href=&#34;https://github.com/GoogleCloudPlatform/kubernetes/pull/5473&#34; title=&#34;添加 DeltaFIFO（控制器框架块）&#34; target=&#34;_blank&#34;&gt;#5473&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Kubelet 现在监听一个安全的 HTTPS 端口 (&lt;a href=&#34;https://github.com/GoogleCloudPlatform/kubernetes/pull/6380&#34; title=&#34;将 kubelet 配置为使用 HTTPS (获得 2)&#34; target=&#34;_blank&#34;&gt;#6380&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;使 kubectl 错误更加友好 (&lt;a href=&#34;https://github.com/GoogleCloudPlatform/kubernetes/pull/6338&#34; title=&#34;返回用于配置验证的类型化错误，并简化错误&#34; target=&#34;_blank&#34;&gt;#6338&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;apiserver 现在支持客户端 cert 身份验证 (&lt;a href=&#34;https://github.com/GoogleCloudPlatform/kubernetes/pull/6190&#34; title=&#34;添加客户端证书认证&#34; target=&#34;_blank&#34;&gt;#6190&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;apiserver 现在限制了它处理的并发请求的数量 (&lt;a href=&#34;https://github.com/GoogleCloudPlatform/kubernetes/pull/6207&#34; title=&#34;为服务器处理的正在运行的请求数量添加一个限制。&#34; target=&#34;_blank&#34;&gt;#6207&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;添加速度限制删除 pod (&lt;a href=&#34;https://github.com/GoogleCloudPlatform/kubernetes/pull/6355&#34; title=&#34;添加速度限制删除 pod&#34; target=&#34;_blank&#34;&gt;#6355&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;将平衡资源分配算法作为优先级函数实现在调度程序包中 (&lt;a href=&#34;https://github.com/GoogleCloudPlatform/kubernetes/pull/6150&#34; title=&#34;将均衡资源分配算法作为优先级函数实现在调度程序包中。&#34; target=&#34;_blank&#34;&gt;#6150&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;从主服务器启用日志收集功能 (&lt;a href=&#34;https://github.com/GoogleCloudPlatform/kubernetes/pull/6396&#34; title=&#34;启用主服务器收集日志。&#34; target=&#34;_blank&#34;&gt;#6396&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;添加了一个 api 端口来从 Pod 中提取日志 (&lt;a href=&#34;https://github.com/GoogleCloudPlatform/kubernetes/pull/6497&#34; title=&#34;pod 子日志资源&#34; target=&#34;_blank&#34;&gt;#6497&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;为调度程序添加了延迟指标 (&lt;a href=&#34;https://github.com/GoogleCloudPlatform/kubernetes/pull/6368&#34; title=&#34;将基本延迟指标添加到调度程序。&#34; target=&#34;_blank&#34;&gt;#6368&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;为 REST 客户端添加了延迟指标 (&lt;a href=&#34;https://github.com/GoogleCloudPlatform/kubernetes/pull/6409&#34; title=&#34;向 REST 客户端添加延迟指标&#34; target=&#34;_blank&#34;&gt;#6409&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;!--

* etcd now runs in a pod on the master ([#6221][18])
* nginx now runs in a container on the master ([#6334][19])
* Began creating Docker images for master components ([#6326][20])
* Updated GCE provider to work with gcloud 0.9.54 ([#6270][21])
* Updated AWS provider to fix Region vs Zone semantics ([#6011][22])
* Record event when image GC fails ([#6091][23])
* Add a QPS limiter to the kubernetes client ([#6203][24])
* Decrease the time it takes to run make release ([#6196][25])
* New volume support
    * Added iscsi volume plugin ([#5506][26])
    * Added glusterfs volume plugin ([#6174][27])
    * AWS EBS volume support ([#5138][28])
* Updated to heapster version to v0.10.0 ([#6331][29])
* Updated to etcd 2.0.9 ([#6544][30])
* Updated to Kibana to v1.2 ([#6426][31])
* Bug Fixes
    * Kube-proxy now updates iptables rules if a service&#39;s public IPs change ([#6123][32])
    * Retry kube-addons creation if the initial creation fails ([#6200][33])
    * Make kube-proxy more resiliant to running out of file descriptors ([#6727][34])

--&gt;

&lt;ul&gt;
&lt;li&gt;etcd 现在在 master 上的一个 pod 中运行 (&lt;a href=&#34;https://github.com/GoogleCloudPlatform/kubernetes/pull/6221&#34; title=&#34;在 pod 中运行 etcd 2.0.5&#34; target=&#34;_blank&#34;&gt;#6221&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;nginx 现在在 master上的容器中运行 (&lt;a href=&#34;https://github.com/GoogleCloudPlatform/kubernetes/pull/6334&#34; title=&#34;添加一个 nginx docker 镜像用于主程序。&#34; target=&#34;_blank&#34;&gt;#6334&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;开始为主组件构建 Docker 镜像 (&lt;a href=&#34;https://github.com/GoogleCloudPlatform/kubernetes/pull/6326&#34; title=&#34;为主组件创建 Docker 镜像&#34; target=&#34;_blank&#34;&gt;#6326&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;更新了 GCE 程序以使用 gcloud 0.9.54 (&lt;a href=&#34;https://github.com/GoogleCloudPlatform/kubernetes/pull/6270&#34; title=&#34;gcloud 0.9.54 的更新&#34; target=&#34;_blank&#34;&gt;#6270&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;更新了 AWS 程序来修复区域与区域语义 (&lt;a href=&#34;https://github.com/GoogleCloudPlatform/kubernetes/pull/6011&#34; title=&#34;修复 AWS 区域 与 zone&#34; target=&#34;_blank&#34;&gt;#6011&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;记录镜像 GC 失败时的事件 (&lt;a href=&#34;https://github.com/GoogleCloudPlatform/kubernetes/pull/6091&#34; title=&#34;记录镜像 GC 失败时的事件。&#34; target=&#34;_blank&#34;&gt;#6091&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;为 kubernetes 客户端添加 QPS 限制器 (&lt;a href=&#34;https://github.com/GoogleCloudPlatform/kubernetes/pull/6203&#34; title=&#34;向 kubernetes 客户端添加 QPS 限制器。&#34; target=&#34;_blank&#34;&gt;#6203&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;减少运行 make release 所需的时间 (&lt;a href=&#34;https://github.com/GoogleCloudPlatform/kubernetes/pull/6196&#34; title=&#34;在 `make release` 的构建和打包阶段并行化架构&#34; target=&#34;_blank&#34;&gt;#6196&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;新卷的支持

&lt;ul&gt;
&lt;li&gt;添加 iscsi 卷插件 (&lt;a href=&#34;https://github.com/GoogleCloudPlatform/kubernetes/pull/5506&#34; title=&#34;添加 iscsi 卷插件&#34; target=&#34;_blank&#34;&gt;#5506&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;添加 glusterfs 卷插件 (&lt;a href=&#34;https://github.com/GoogleCloudPlatform/kubernetes/pull/6174&#34; title=&#34;实现 glusterfs 卷插件&#34; target=&#34;_blank&#34;&gt;#6174&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;AWS EBS 卷支持 (&lt;a href=&#34;https://github.com/GoogleCloudPlatform/kubernetes/pull/5138&#34; title=&#34;AWS EBS 卷支持&#34; target=&#34;_blank&#34;&gt;#5138&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;更新到 heapster 版本到 v0.10.0 (&lt;a href=&#34;https://github.com/GoogleCloudPlatform/kubernetes/pull/6331&#34; title=&#34;将 heapster 版本更新到 v0.10.0&#34; target=&#34;_blank&#34;&gt;#6331&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;更新到 etcd 2.0.9 (&lt;a href=&#34;https://github.com/GoogleCloudPlatform/kubernetes/pull/6544&#34; title=&#34;构建 etcd 镜像(版本 2.0.9)，并将 kubernetes 集群升级到新版本&#34; target=&#34;_blank&#34;&gt;#6544&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;更新到 Kibana 到 v1.2 (&lt;a href=&#34;https://github.com/GoogleCloudPlatform/kubernetes/pull/6426&#34; title=&#34;更新 Kibana 到 v1.2，它对 Elasticsearch 的位置进行了参数化&#34; target=&#34;_blank&#34;&gt;#6426&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;漏洞修复

&lt;ul&gt;
&lt;li&gt;如果服务的公共 IP 发生变化，Kube-proxy现在会更新iptables规则 (&lt;a href=&#34;https://github.com/GoogleCloudPlatform/kubernetes/pull/6123&#34; title=&#34;修复了 kube-proxy 中的一个错误，如果一个服务的公共 ip 发生变化，它不会更新 iptables 规则&#34; target=&#34;_blank&#34;&gt;#6123&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;如果初始创建失败，则重试 kube-addons 创建 (&lt;a href=&#34;https://github.com/GoogleCloudPlatform/kubernetes/pull/6200&#34; title=&#34;如果 kube-addons 创建失败，请重试 kube-addons 创建。&#34; target=&#34;_blank&#34;&gt;#6200&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;使 kube-proxy 对耗尽文件描述符更具弹性 (&lt;a href=&#34;https://github.com/GoogleCloudPlatform/kubernetes/pull/6727&#34; title=&#34;pkg/proxy: fd 用完后引起恐慌&#34; target=&#34;_blank&#34;&gt;#6727&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
To download, please visit https://github.com/GoogleCloudPlatform/kubernetes/releases/tag/v0.15.0
--&gt;

&lt;p&gt;要下载，请访问 &lt;a href=&#34;https://github.com/GoogleCloudPlatform/kubernetes/releases/tag/v0.15.0&#34; target=&#34;_blank&#34;&gt;https://github.com/GoogleCloudPlatform/kubernetes/releases/tag/v0.15.0&lt;/a&gt;&lt;/p&gt;

&lt;!--





















--&gt;

&lt;!--













--&gt;

      </description>
    </item>
    
    <item>
      <title>Blog:  Kubernetes 社区每周聚会笔记 - 2015年3月27日 </title>
      <link>https://kubernetes.io/zh/blog/2015/03/28/weekly-kubernetes-community-hangout/</link>
      <pubDate>Sat, 28 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/zh/blog/2015/03/28/weekly-kubernetes-community-hangout/</guid>
      <description>
        
        
        &lt;!--
---
title: &#34; Weekly Kubernetes Community Hangout Notes - March 27 2015 &#34;
date: 2015-03-28
slug: weekly-kubernetes-community-hangout
url: /blog/2015/03/Weekly-Kubernetes-Community-Hangout
---
--&gt;

&lt;!--
Every week the Kubernetes contributing community meet virtually over Google Hangouts. We want anyone who&#39;s interested to know what&#39;s discussed in this forum.
--&gt;

&lt;p&gt;每个星期，Kubernetes 贡献者社区几乎都会在谷歌 Hangouts 上聚会。我们希望任何对此感兴趣的人都能了解这个论坛的讨论内容。&lt;/p&gt;

&lt;!--
Agenda:
--&gt;

&lt;p&gt;日程安排：&lt;/p&gt;

&lt;!--

\- Andy - demo remote execution and port forwarding

\- Quinton - Cluster federation - Postponed

\- Clayton - UI code sharing and collaboration around Kubernetes

--&gt;

&lt;p&gt;- Andy - 演示远程执行和端口转发&lt;/p&gt;

&lt;p&gt;- Quinton - 联邦集群 - 延迟&lt;/p&gt;

&lt;p&gt;- Clayton - 围绕 Kubernetes 的 UI 代码共享和协作&lt;/p&gt;

&lt;!--
Notes from meeting:
--&gt;

&lt;p&gt;从会议指出：&lt;/p&gt;

&lt;!--

1\. Andy from RedHat:

--&gt;

&lt;p&gt;1. Andy 从 RedHat：&lt;/p&gt;

&lt;!--

* Demo remote execution

--&gt;

&lt;ul&gt;
&lt;li&gt;演示远程执行&lt;/li&gt;
&lt;/ul&gt;

&lt;!--

    * kubectl exec -p $POD -- $CMD

    * Makes a connection to the master as proxy, figures out which node the pod is on, proxies connection to kubelet, which does the interesting bit.  via nsenter.

    * Multiplexed streaming over HTTP using SPDY

    * Also interactive mode:

    * Assumes first container.  Can use -c $CONTAINER to pick a particular one.

    * If have gdb pre-installed in container, then can interactively attach it to running process

        * backtrace, symbol tbles, print, etc.  Most things you can do with gdb.

    * Can also with careful flag crafting run rsync over this or set up sshd inside container.

    * Some feedback via chat:

--&gt;

&lt;pre&gt;&lt;code&gt;* kubectl exec -p $POD -- $CMD

* 作为代理与主机建立连接，找出 pod 所在的节点，代理与 kubelet 的连接，这一点很有趣。通过 nsenter。

* 使用 SPDY 通过 HTTP 进行多路复用流式传输

* 还有互动模式：

* 假设第一个容器，可以使用 -c $CONTAINER 一个特定的。

* 如果在容器中预先安装了 gdb，则可以交互地将其附加到正在运行的进程中

    * backtrace、symbol tbles、print 等。  使用gdb可以做的大多数事情。

* 也可以用精心制作的参数在上面运行 rsync 或者在容器内设置 sshd。

* 一些聊天反馈：
&lt;/code&gt;&lt;/pre&gt;

&lt;!--

* Andy also demoed port forwarding
* nsenter vs. docker exec

--&gt;

&lt;ul&gt;
&lt;li&gt;Andy 还演示了端口转发&lt;/li&gt;
&lt;li&gt;nnsenter 与 docker exec&lt;/li&gt;
&lt;/ul&gt;

&lt;!--

    * want to inject a binary under control of the host, similar to pre-start hooks

    * socat, nsenter, whatever the pre-start hook needs

--&gt;

&lt;pre&gt;&lt;code&gt;* 想要在主机的控制下注入二进制文件，类似于预启动钩子

* socat、nsenter，任何预启动钩子需要的
&lt;/code&gt;&lt;/pre&gt;

&lt;!--

* would be nice to blog post on this
* version of nginx in wheezy is too old to support needed master-proxy functionality

--&gt;

&lt;ul&gt;
&lt;li&gt;如果能在博客上发表这方面的文章就太好了&lt;/li&gt;
&lt;li&gt;wheezy 中的 nginx 版本太旧，无法支持所需的主代理功能&lt;/li&gt;
&lt;/ul&gt;

&lt;!--

2\. Clayton: where are we wrt a community organization for e.g. kubernetes UI components?

* google-containers-ui IRC channel, mailing list.
* Tim: google-containers prefix is historical, should just do &#34;kubernetes-ui&#34;
* also want to put design resources in, and bower expects its own repo.
* General agreement

--&gt;

&lt;p&gt;2. Clayton: 我们的社区组织在哪里，例如 kubernetes UI 组件？&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;google-containers-ui IRC 频道，邮件列表。&lt;/li&gt;
&lt;li&gt;Tim: google-containers 前缀是历史的，应该只做 &amp;ldquo;kubernetes-ui&amp;rdquo;&lt;/li&gt;
&lt;li&gt;也希望将设计资源投入使用，并且 bower 期望自己的仓库。&lt;/li&gt;
&lt;li&gt;通用协议&lt;/li&gt;
&lt;/ul&gt;

&lt;!--

3\. Brian Grant:

* Testing v1beta3, getting that ready to go in.
* Paul working on changes to commandline stuff.
* Early to mid next week, try to enable v1beta3 by default?
* For any other changes, file issue and CC thockin.

--&gt;

&lt;p&gt;3. Brian Grant:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;测试 v1beta3，准备进入。&lt;/li&gt;
&lt;li&gt;Paul 力于改变命令行的内容。&lt;/li&gt;
&lt;li&gt;下周初至中旬，尝试默认启用v1beta3 ?&lt;/li&gt;
&lt;li&gt;对于任何其他更改，请发出文件并抄送 thockin。&lt;/li&gt;
&lt;/ul&gt;

&lt;!--

4\. General consensus that 30 minutes is better than 60

--&gt;

&lt;p&gt;4. 一般认为30分钟比60分钟好&lt;/p&gt;

&lt;!--

* Shouldn&#39;t artificially try to extend just to fill time.

--&gt;

&lt;ul&gt;
&lt;li&gt;不应该为了填满时间而人为地延长。&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog:  Kubernetes 采集视频 </title>
      <link>https://kubernetes.io/zh/blog/2015/03/23/kubernetes-gathering-videos/</link>
      <pubDate>Mon, 23 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/zh/blog/2015/03/23/kubernetes-gathering-videos/</guid>
      <description>
        
        
        &lt;!--
---

title: &#34; Kubernetes Gathering Videos &#34;
date: 2015-03-23
slug: kubernetes-gathering-videos
url: /blog/2015/03/Kubernetes-Gathering-Videos
---
--&gt;

&lt;!--
If you missed the Kubernetes Gathering in SF last month, fear not! Here are the videos from the evening presentations organized into a playlist on YouTube

[![Kubernetes Gathering](https://img.youtube.com/vi/q8lGZCKktYo/0.jpg)](https://www.youtube.com/playlist?list=PL69nYSiGNLP2FBVvSLHpJE8_6hRHW8Kxe)
--&gt;

&lt;p&gt;如果你错过了上个月在旧金山举行的 Kubernetes 大会，不要害怕!以下是在 YouTube 上组织成播放列表的晚间演示文稿中的视频。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PL69nYSiGNLP2FBVvSLHpJE8_6hRHW8Kxe&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/q8lGZCKktYo/0.jpg&#34; alt=&#34;Kubernetes Gathering&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: 欢迎来到 Kubernetes 博客!</title>
      <link>https://kubernetes.io/zh/blog/2015/03/20/welcome-to-kubernetes-blog/</link>
      <pubDate>Fri, 20 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/zh/blog/2015/03/20/welcome-to-kubernetes-blog/</guid>
      <description>
        
        
        &lt;!--
---
title: Welcome to the Kubernetes Blog!
date: 2015-03-20
slug: welcome-to-kubernetes-blog
url: /blog/2015/03/Welcome-To-Kubernetes-Blog
---
--&gt;

&lt;!--
Welcome to the new Kubernetes Blog. Follow this blog to learn about the Kubernetes Open Source project. We plan to post release notes, how-to articles, events, and maybe even some off topic fun here from time to time.
--&gt;

&lt;p&gt;欢迎来到新的 Kubernetes 博客。关注此博客，了解 Kubernetes 开源项目。我们计划不时发布发布说明，操作方法文章，活动，甚至一些非常有趣的话题。&lt;/p&gt;

&lt;!--
If you are using Kubernetes or contributing to the project and would like to do a guest post, [please let me know](mailto:kitm@google.com).
--&gt;

&lt;p&gt;如果您正在使用 Kubernetes 或为该项目做出贡献并想要发帖子，&lt;a href=&#34;mailto:kitm@google.com&#34; target=&#34;_blank&#34;&gt;请告诉我&lt;/a&gt;。&lt;/p&gt;

&lt;!--
To start things off, here&#39;s a roundup of recent Kubernetes posts from other sites:
--&gt;

&lt;p&gt;首先，以下是 Kubernetes 最近在其他网站上发布的文章摘要：&lt;/p&gt;

&lt;!--
- [Scaling MySQL in the cloud with Vitess and Kubernetes](http://googlecloudplatform.blogspot.com/2015/03/scaling-MySQL-in-the-cloud-with-Vitess-and-Kubernetes.html)
- [Container Clusters on VMs](http://googlecloudplatform.blogspot.com/2015/02/container-clusters-on-vms.html)
- [Everything you wanted to know about Kubernetes but were afraid to ask](http://googlecloudplatform.blogspot.com/2015/01/everything-you-wanted-to-know-about-Kubernetes-but-were-afraid-to-ask.html)
- [What makes a container cluster?](http://googlecloudplatform.blogspot.com/2015/01/what-makes-a-container-cluster.html)
- [Integrating OpenStack and Kubernetes with Murano](https://www.mirantis.com/blog/integrating-openstack-and-kubernetes-with-murano/)
- [An introduction to containers, Kubernetes, and the trajectory of modern cloud computing](http://googlecloudplatform.blogspot.com/2015/01/in-coming-weeks-we-will-be-publishing.html)
- [What is Kubernetes and how to use it?](http://www.centurylinklabs.com/what-is-kubernetes-and-how-to-use-it/)
- [OpenShift V3, Docker and Kubernetes Strategy](https://blog.openshift.com/v3-docker-kubernetes-interview/)
- [An Introduction to Kubernetes](https://www.digitalocean.com/community/tutorials/an-introduction-to-kubernetes)
--&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://googlecloudplatform.blogspot.com/2015/03/scaling-MySQL-in-the-cloud-with-Vitess-and-Kubernetes.html&#34; target=&#34;_blank&#34;&gt;使用 Vitess 和 Kubernetes 在云中扩展 MySQL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://googlecloudplatform.blogspot.com/2015/02/container-clusters-on-vms.html&#34; target=&#34;_blank&#34;&gt;虚拟机上的容器群集&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://googlecloudplatform.blogspot.com/2015/01/everything-you-wanted-to-know-about-Kubernetes-but-were-afraid-to-ask.html&#34; target=&#34;_blank&#34;&gt;想知道的关于 kubernetes 的一切，却又不敢问&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://googlecloudplatform.blogspot.com/2015/01/what-makes-a-container-cluster.html&#34; target=&#34;_blank&#34;&gt;什么构成容器集群？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.mirantis.com/blog/integrating-openstack-and-kubernetes-with-murano/&#34; target=&#34;_blank&#34;&gt;将 OpenStack 和 Kubernetes 与 Murano 集成&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://googlecloudplatform.blogspot.com/2015/01/in-coming-weeks-we-will-be-publishing.html&#34; target=&#34;_blank&#34;&gt;容器介绍，Kubernetes 以及现代云计算的发展轨迹&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.centurylinklabs.com/what-is-kubernetes-and-how-to-use-it/&#34; target=&#34;_blank&#34;&gt;什么是 Kubernetes 以及如何使用它？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.openshift.com/v3-docker-kubernetes-interview/&#34; target=&#34;_blank&#34;&gt;OpenShift V3，Docker 和 Kubernetes 策略&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.digitalocean.com/community/tutorials/an-introduction-to-kubernetes&#34; target=&#34;_blank&#34;&gt;Kubernetes 简介&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
Happy cloud computing!
--&gt;

&lt;p&gt;快乐的云计算！&lt;/p&gt;

&lt;!--
 - Kit Merker - Product Manager, Google Cloud Platform
--&gt;

&lt;ul&gt;
&lt;li&gt;Kit Merker - Google 云平台产品经理&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: </title>
      <link>https://kubernetes.io/zh/blog/1/01/01/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/zh/blog/1/01/01/</guid>
      <description>
        
        
        

&lt;hr /&gt;

&lt;p&gt;layout: blog
title: &amp;ldquo;基于 MIPS 架构的 Kubernetes 方案&amp;rdquo;
date: 2020-01-15&lt;/p&gt;

&lt;h2 id=&#34;slug-kubernetes-on-mips&#34;&gt;slug: Kubernetes-on-MIPS&lt;/h2&gt;

&lt;!-- 
**Authors:** TimYin Shi, Dominic Yin, Wang Zhan, Jessica Jiang, Will Cai, Jeffrey Gao, Simon Sun (Inspur)
--&gt;

&lt;p&gt;&lt;strong&gt;作者:&lt;/strong&gt; 石光银，尹东超，展望，江燕，蔡卫卫，高传集，孙思清（浪潮）&lt;/p&gt;

&lt;!-- 
## Background
--&gt;

&lt;h2 id=&#34;背景&#34;&gt;背景&lt;/h2&gt;

&lt;!-- 
[MIPS](https://en.wikipedia.org/wiki/MIPS_architecture) (Microprocessor without Interlocked Pipelined Stages) is a reduced instruction set computer (RISC) instruction set architecture (ISA), appeared in 1981 and developed by MIPS Technologies. Now MIPS architecture is widely used in many electronic products.
--&gt;

&lt;p&gt;&lt;a href=&#34;https://zh.wikipedia.org/wiki/MIPS%E6%9E%B6%E6%A7%8B&#34; target=&#34;_blank&#34;&gt;MIPS&lt;/a&gt; (Microprocessor without Interlocked Pipelined Stages) 是一种采取精简指令集（RISC）的处理器架构 (ISA)，出现于 1981 年，由 MIPS 科技公司开发。如今 MIPS 架构被广泛应用于许多电子产品上。&lt;/p&gt;

&lt;!-- 
[Kubernetes](https://kubernetes.io) has officially supported a variety of CPU architectures such as x86, arm/arm64, ppc64le, s390x. However, it&#39;s a pity that Kubernetes doesn&#39;t support MIPS. With the widespread use of cloud native technology, users under MIPS architecture also have an urgent demand for Kubernetes on MIPS.
--&gt;

&lt;p&gt;&lt;a href=&#34;https://kubernetes.io&#34; target=&#34;_blank&#34;&gt;Kubernetes&lt;/a&gt; 官方目前支持众多 CPU 架构诸如 x86, arm/arm64, ppc64le, s390x 等。然而目前还不支持 MIPS 架构，始终是一个遗憾。随着云原生技术的广泛应用，MIPS 架构下的用户始终对 Kubernetes on MIPS 有着迫切的需求。&lt;/p&gt;

&lt;!--
## Achievements
--&gt;

&lt;h2 id=&#34;成果&#34;&gt;成果&lt;/h2&gt;

&lt;!--
For many years, to enrich the ecology of the open-source community, we have been working on adjusting MIPS architecture for Kubernetes use cases. With the continuous iterative optimization and the performance improvement of the MIPS CPU, we have made some breakthrough progresses on the mips64el platform.
--&gt;

&lt;p&gt;多年来，为了丰富开源社区的生态，我们一直致力于在 MIPS 架构下适配 Kubernetes。随着 MIPS CPU 的不断迭代优化和性能的提升，我们在 mips64el 平台上取得了一些突破性的进展。&lt;/p&gt;

&lt;!-- 
Over the years, we have been actively participating in the Kubernetes community and have rich experience in the using and optimization of Kubernetes technology. Recently, we tried to adapt the MIPS architecture platform for Kubernetes and achieved a new a stage on that journey. The team has completed migration and adaptation of Kubernetes and related components, built not only a stable and highly available MIPS cluster but also completed the conformance test for Kubernetes v1.16.2.
--&gt;

&lt;p&gt;多年来，我们一直积极投入 Kubernetes 社区，在 Kubernetes 技术应用和优化方面具备了丰富的经验。最近，我们在研发过程中尝试将 Kubernetes 适配到 MIPS 架构平台，并取得了阶段性成果。成功完成了 Kubernetes 以及相关组件的迁移适配，不仅搭建出稳定高可用的 MIPS 集群，同时完成了 Kubernetes v1.16.2 版本的一致性测试。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2020-01-15-Kubernetes-on-MIPS/kubernetes-on-mips.png&#34; alt=&#34;Kubernetes on MIPS&#34; /&gt;&lt;/p&gt;

&lt;!--
_Figure 1 Kubernetes on MIPS_
--&gt;

&lt;p&gt;&lt;em&gt;图一 Kubernetes on MIPS&lt;/em&gt;&lt;/p&gt;

&lt;!--
## K8S-MIPS component build
--&gt;

&lt;h2 id=&#34;k8s-mips-组件构建&#34;&gt;K8S-MIPS 组件构建&lt;/h2&gt;

&lt;!--
Almost all native cloud components related to Kubernetes do not provide a MIPS version installation package or image. The prerequisite of deploying Kubernetes on the MIPS platform is to compile and build all required components on the mips64el platform. These components include:
--&gt;

&lt;p&gt;几乎所有的 Kubernetes 相关的云原生组件都没有提供 MIPS 版本的安装包或镜像，在 MIPS 平台上部署 Kubernetes 的前提是自行编译构建出全部所需组件。这些组件主要包括：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;golang&lt;/li&gt;
&lt;li&gt;docker-ce&lt;/li&gt;
&lt;li&gt;hyperkube&lt;/li&gt;
&lt;li&gt;pause&lt;/li&gt;
&lt;li&gt;etcd&lt;/li&gt;
&lt;li&gt;calico&lt;/li&gt;
&lt;li&gt;coredns&lt;/li&gt;
&lt;li&gt;metrics-server&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
Thanks to the excellent design of Golang and its good support for the MIPS platform, the compilation processes of the above cloud native components are greatly simplified. First of all, we compiled Golang on the latest stable version for the mips64el platform, and then we compiled most of the above components with source code.
--&gt;

&lt;p&gt;得益于 Golang 优秀的设计以及对于 MIPS 平台的良好支持，极大地简化了上述云原生组件的编译过程。首先，我们在 mips64el 平台编译出了最新稳定的 golang, 然后通过源码构建的方式编译完成了上述大部分组件。&lt;/p&gt;

&lt;!--
During the compilation processes, we inevitably encountered many platform compatibility problems, such as a Golang system call compatibility problem (syscall), typecasting of syscall. Stat_t from uint32 to uint64, patching for EpollEvent, and so on.
--&gt;

&lt;p&gt;在编译过程中，我们不可避免地遇到了很多平台兼容性的问题，比如关于 golang 系统调用 (syscall) 的兼容性问题, syscall.Stat_t 32 位 与 64 位类型转换，EpollEvent 修正位缺失等等。&lt;/p&gt;

&lt;!--
To build K8S-MIPS components, we used cross-compilation technology. Our process involved integrating a QEMU tool to translate MIPS CPU instructions and modifying the build script of Kubernetes and E2E image script of Kubernetes, Hyperkube, and E2E test images on MIPS architecture.
--&gt;

&lt;p&gt;构建 K8S-MIPS 组件主要使用了交叉编译技术。构建过程包括集成 QEMU 工具来实现 MIPS CPU 指令的转换。同时修改 Kubernetes 和 E2E 镜像的构建脚本，构建了 Hyperkube 和 MIPS 架构的 E2E 测试镜像。&lt;/p&gt;

&lt;!--
After successfully building the above components, we use tools such as kubespray and kubeadm to complete kubernetes cluster construction.
--&gt;

&lt;p&gt;成功构建出以上组件后，我们使用工具完成 Kubernetes 集群的搭建，比如 kubespray、kubeadm 等。&lt;/p&gt;

&lt;!--
| Name                           | Version | MIPS Repository                                                                                                                                                                                                                                                                   |
|--------------------------------|---------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| golang on MIPS                 | 1.12.5  | -                                                                                                                                                                                                                                                                                 |
| docker-ce on MIPS              | 18.09.8 | -                                                                                                                                                                                                                                                                                 |
| metrics-server for CKE on MIPS | 0.3.2   | `registry.inspurcloud.cn/library/cke/kubernetes/metrics-server-mips64el:v0.3.2`                                                                                                                                                                                                   |
| etcd for CKE on MIPS           | 3.2.26  | `registry.inspurcloud.cn/library/cke/etcd/etcd-mips64el:v3.2.26`                                                                                                                                                                                                                  |
| pause for CKE on MIPS          | 3.1     | `registry.inspurcloud.cn/library/cke/kubernetes/pause-mips64el:3.1`                                                                                                                                                                                                               |
| hyperkube for CKE on MIPS      | 1.14.3  | `registry.inspurcloud.cn/library/cke/kubernetes/hyperkube-mips64el:v1.14.3`                                                                                                                                                                                                       |
| coredns for CKE on MIPS        | 1.6.5   | `registry.inspurcloud.cn/library/cke/kubernetes/coredns-mips64el:v1.6.5`                                                                                                                                                                                                          |
| calico for CKE on MIPS         | 3.8.0   | `registry.inspurcloud.cn/library/cke/calico/cni-mips64el:v3.8.0` `registry.inspurcloud.cn/library/cke/calico/ctl-mips64el:v3.8.0` `registry.inspurcloud.cn/library/cke/calico/node-mips64el:v3.8.0` `registry.inspurcloud.cn/library/cke/calico/kube-controllers-mips64el:v3.8.0` |
--&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;名称&lt;/th&gt;
&lt;th&gt;版本&lt;/th&gt;
&lt;th&gt;MIPS 镜像仓库&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;MIPS 版本 golang&lt;/td&gt;
&lt;td&gt;1.12.5&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;MIPS 版本 docker-ce&lt;/td&gt;
&lt;td&gt;18.09.8&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;MIPS 版本 CKE 构建 metrics-server&lt;/td&gt;
&lt;td&gt;0.3.2&lt;/td&gt;
&lt;td&gt;&lt;code&gt;registry.inspurcloud.cn/library/cke/kubernetes/metrics-server-mips64el:v0.3.2&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;MIPS 版本 CKE 构建 etcd&lt;/td&gt;
&lt;td&gt;3.2.26&lt;/td&gt;
&lt;td&gt;&lt;code&gt;registry.inspurcloud.cn/library/cke/etcd/etcd-mips64el:v3.2.26&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;MIPS 版本 CKE 构建 pause&lt;/td&gt;
&lt;td&gt;3.1&lt;/td&gt;
&lt;td&gt;&lt;code&gt;registry.inspurcloud.cn/library/cke/kubernetes/pause-mips64el:3.1&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;MIPS 版本 CKE 构建 hyperkube&lt;/td&gt;
&lt;td&gt;1.14.3&lt;/td&gt;
&lt;td&gt;&lt;code&gt;registry.inspurcloud.cn/library/cke/kubernetes/hyperkube-mips64el:v1.14.3&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;MIPS 版本 CKE 构建 coredns&lt;/td&gt;
&lt;td&gt;1.6.5&lt;/td&gt;
&lt;td&gt;&lt;code&gt;registry.inspurcloud.cn/library/cke/kubernetes/coredns-mips64el:v1.6.5&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;MIPS 版本 CKE 构建 calico&lt;/td&gt;
&lt;td&gt;3.8.0&lt;/td&gt;
&lt;td&gt;&lt;code&gt;registry.inspurcloud.cn/library/cke/calico/cni-mips64el:v3.8.0&lt;/code&gt; &lt;code&gt;registry.inspurcloud.cn/library/cke/calico/ctl-mips64el:v3.8.0&lt;/code&gt; &lt;code&gt;registry.inspurcloud.cn/library/cke/calico/node-mips64el:v3.8.0&lt;/code&gt; &lt;code&gt;registry.inspurcloud.cn/library/cke/calico/kube-controllers-mips64el:v3.8.0&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;!--
**Note**: CKE is a Kubernetes-based cloud container engine launched by Inspur
--&gt;

&lt;p&gt;&lt;strong&gt;注&lt;/strong&gt;: CKE 是浪潮推出的一款基于 Kubernetes 的容器云服务引擎&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2020-01-15-Kubernetes-on-MIPS/k8s-mips-cluster-components.png&#34; alt=&#34;K8S-MIPS Cluster Components&#34; /&gt;&lt;/p&gt;

&lt;!--
_Figure 2 K8S-MIPS Cluster Components_
--&gt;

&lt;p&gt;&lt;em&gt;图二 K8S-MIPS 集群组件&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2020-01-15-Kubernetes-on-MIPS/cpu-architecture.png&#34; alt=&#34;CPU Architecture&#34; /&gt;&lt;/p&gt;

&lt;!--
_Figure 3 CPU Architecture_
--&gt;

&lt;p&gt;&lt;em&gt;图三 CPU 架构&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2020-01-15-Kubernetes-on-MIPS/cluster-node-information.png&#34; alt=&#34;Cluster Node Information&#34; /&gt;&lt;/p&gt;

&lt;!--
_Figure 4 Cluster Node Information_
--&gt;

&lt;p&gt;&lt;em&gt;图四 集群节点信息&lt;/em&gt;&lt;/p&gt;

&lt;!--
## Run K8S Conformance Test
--&gt;

&lt;h2 id=&#34;运行-k8s-一致性测试&#34;&gt;运行 K8S 一致性测试&lt;/h2&gt;

&lt;!--
The most straightforward way to verify the stability and availability of the K8S-MIPS cluster is to run a Kubernetes [conformance test](https://github.com/kubernetes/kubernetes/blob/v1.16.2/cluster/images/conformance/README.md).
--&gt;

&lt;p&gt;验证 K8S-MIP 集群稳定性和可用性最简单直接的方式是运行 Kubernetes 的 &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/v1.16.2/cluster/images/conformance/README.md&#34; target=&#34;_blank&#34;&gt;一致性测试&lt;/a&gt;。&lt;/p&gt;

&lt;!--
Conformance is a standalone container to launch Kubernetes end-to-end tests for conformance testing.
--&gt;

&lt;p&gt;一致性测试是一个独立的容器，用于启动 Kubernetes 端到端的一致性测试。&lt;/p&gt;

&lt;!--
Once the test has started, it launches several pods for various end-to-end tests. The source code of those images used by these pods is mostly from `kubernetes/test/images`, and the built images are at `gcr.io/kubernetes-e2e-test-images`. Since there are no MIPS images in the repository, we must first build all needed images to run the test.
--&gt;

&lt;p&gt;当执行一致性测试时，测试程序会启动许多 Pod 进行各种端到端的行为测试，这些 Pod 使用的镜像源码大部分来自于 &lt;code&gt;kubernetes/test/images&lt;/code&gt; 目录下，构建的镜像位于 &lt;code&gt;gcr.io/kubernetes-e2e-test-images/&lt;/code&gt;。由于镜像仓库中目前并不存在 MIPS 架构的镜像，我们要想运行 E2E 测试，必须首先构建出测试所需的全部镜像。&lt;/p&gt;

&lt;!--
### Build needed images for test
--&gt;

&lt;h3 id=&#34;构建测试所需镜像&#34;&gt;构建测试所需镜像&lt;/h3&gt;

&lt;!-- 
The first step is to find all needed images for the test. We can run `sonobuoy images-p e2e` command to list all images, or we can find those images in [/test/utils/image/manifest.go](https://github.com/kubernetes/kubernetes/blob/master/test/utils/image/manifest.go). Although Kubernetes officially has a complete Makefile and shell-script that provides commands for building test images, there are still a number of architecture-related issues that have not been resolved, such as the incompatibilities of base images and dependencies. So we cannot directly build mips64el architecture images by executing these commands.
--&gt;

&lt;p&gt;第一步是找到测试所需的所有镜像。我们可以执行 &lt;code&gt;sonobuoy images-p e2e&lt;/code&gt; 命令来列出所有镜像，或者我们可以在 &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/test/utils/image/manifest.go&#34; target=&#34;_blank&#34;&gt;/test/utils/image/manifest.go&lt;/a&gt; 中找到这些镜像。尽管 Kubernetes 官方提供了完整的 Makefile 和 shell 脚本，为构建测试映像提供了命令，但是仍然有许多与体系结构相关的问题未能解决，比如基础映像和依赖包的不兼容问题。因此，我们无法通过直接执行这些构建命令来制作 mips64el 架构镜像。&lt;/p&gt;

&lt;!--
Most test images are in golang, then compiled into binaries and built as Docker image based on the corresponding Dockerfile. These images are easy to build. But note that most images are using alpine as their base image, which does not officially support mips64el architecture for now. For this moment, we are unable to make mips64el version of [alpine](https://www.alpinelinux.org/), so we have to replace the alpine to existing MIPS images, such as Debian-stretch, fedora, ubuntu. Replacing the base image also requires replacing the command to install the dependencies, even the version of these dependencies.
--&gt;

&lt;p&gt;多数测试镜像都是使用 golang 编写，然后编译出二进制文件，并基于相应的 Dockerfile 制作出镜像。这些镜像对我们来说可以轻松地制作出来。但是需要注意一点：测试镜像默认使用的基础镜像大多是 alpine, 目前 &lt;a href=&#34;https://www.alpinelinux.org/&#34; target=&#34;_blank&#34;&gt;Alpine&lt;/a&gt; 官方并不支持 mips64el 架构，我们暂时未能自己制作出 mips64el 版本的 alpine 础镜像，只能将基础镜像替换为我们目前已有的 mips64el 基础镜像，比如 debian-stretch,fedora, ubuntu 等。替换基础镜像的同时也需要替换安装依赖包的命令，甚至依赖包的版本等。&lt;/p&gt;

&lt;!--
Some images are not in `kubernetes/test/images`, such as `gcr.io/google-samples/gb-frontend:v6`. There is no clear documentation explaining where these images are locaated, though we found the source code in repository [github.com/GoogleCloudPlatform/kubernetes-engine-samples](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples). We soon ran into new problems: to build these google sample images, we have to build the base image it uses, even the base image of the base images, such as `php:5-apache`, `redis`, and `perl`.
--&gt;

&lt;p&gt;有些测试所需镜像的源码并不在 &lt;code&gt;kubernetes/test/images&lt;/code&gt; 下,比如 &lt;code&gt;gcr.io/google-samples/gb-frontend:v6&lt;/code&gt; 等，没有明确的文档说明这类镜像来自于何方，最终还是在 &lt;a href=&#34;github.com/GoogleCloudPlatform/kubernetes-engine-samples&#34; target=&#34;_blank&#34;&gt;github.com/GoogleCloudPlatform/kubernetes-engine-samples&lt;/a&gt; 这个仓库找到了原始的镜像源代码。但是很快我们遇到了新的问题，为了制作这些镜像，还要制作它依赖的基础镜像，甚至基础镜像的基础镜像，比如 &lt;code&gt;php:5-apache&lt;/code&gt;、&lt;code&gt;redis&lt;/code&gt;、&lt;code&gt;perl&lt;/code&gt; 等等。&lt;/p&gt;

&lt;!--
After a long process of building an image, we finished with about four dozen images, including the images used by the test pod, and the base images. The last step before we run the tests is to place all those images into every node in the cluster and make sure the Pod image pull policy is `imagePullPolicy: ifNotPresent`.
--&gt;

&lt;p&gt;经过漫长庞杂的的镜像重制工作，我们完成了总计约 40 个镜像的制作 ，包括测试镜像以及直接和间接依赖的基础镜像。
最终我们将所有镜像在集群内准备妥当，并确保测试用例内所有 Pod 的镜像拉取策略设置为 &lt;code&gt;imagePullPolicy: ifNotPresent&lt;/code&gt;。&lt;/p&gt;

&lt;!--
Here are some of the images we built
--&gt;

&lt;p&gt;这是我们构建出的部分镜像列表：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;docker.io/library/busybox:1.29&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;docker.io/library/nginx:1.14-alpine&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;docker.io/library/nginx:1.15-alpine&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;docker.io/library/perl:5.26&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;docker.io/library/httpd:2.4.38-alpine&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;docker.io/library/redis:5.0.5-alpine&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gcr.io/google-containers/conformance:v1.16.2&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gcr.io/google-containers/hyperkube:v1.16.2&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gcr.io/google-samples/gb-frontend:v6&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gcr.io/kubernetes-e2e-test-images/agnhost:2.6&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gcr.io/kubernetes-e2e-test-images/apparmor-loader:1.0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gcr.io/kubernetes-e2e-test-images/dnsutils:1.1&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gcr.io/kubernetes-e2e-test-images/echoserver:2.2&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gcr.io/kubernetes-e2e-test-images/ipc-utils:1.0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gcr.io/kubernetes-e2e-test-images/kitten:1.0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gcr.io/kubernetes-e2e-test-images/metadata-concealment:1.2&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gcr.io/kubernetes-e2e-test-images/mounttest:1.0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gcr.io/kubernetes-e2e-test-images/nautilus:1.0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gcr.io/kubernetes-e2e-test-images/nonewprivs:1.0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gcr.io/kubernetes-e2e-test-images/nonroot:1.0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gcr.io/kubernetes-e2e-test-images/resource-consumer-controller:1.0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gcr.io/kubernetes-e2e-test-images/resource-consumer:1.5&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gcr.io/kubernetes-e2e-test-images/sample-apiserver:1.10&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gcr.io/kubernetes-e2e-test-images/test-webserver:1.0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gcr.io/kubernetes-e2e-test-images/volume/gluster:1.0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gcr.io/kubernetes-e2e-test-images/volume/iscsi:2.0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gcr.io/kubernetes-e2e-test-images/volume/nfs:1.0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gcr.io/kubernetes-e2e-test-images/volume/rbd:1.0.1&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;k8s.gcr.io/etcd:3.3.15&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;k8s.gcr.io/pause:3.1&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
Finally, we ran the tests and got the test result, include `e2e.log`, which showed that all test cases passed. Additionally, we submitted our test result to [k8s-conformance](https://github.com/cncf/k8s-conformance) as a [pull request](https://github.com/cncf/k8s-conformance/pull/779).
--&gt;

&lt;p&gt;最终我们执行一致性测试并且得到了测试报告，包括 &lt;code&gt;e2e.log&lt;/code&gt;，显示我们通过了全部的测试用例。此外，我们将测试结果以 &lt;a href=&#34;https://github.com/cncf/k8s-conformance/pull/779&#34; target=&#34;_blank&#34;&gt;pull request&lt;/a&gt; 的形式提交给了 &lt;a href=&#34;https://github.com/cncf/k8s-conformance&#34; target=&#34;_blank&#34;&gt;k8s-conformance&lt;/a&gt; 。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2020-01-15-Kubernetes-on-MIPS/pull-request-for-conformance-test-results.png&#34; alt=&#34;Pull request for conformance test results&#34; /&gt;&lt;/p&gt;

&lt;!--
_Figure 5 Pull request for conformance test results_
--&gt;

&lt;p&gt;&lt;em&gt;图五 一致性测试结果的 PR&lt;/em&gt;&lt;/p&gt;

&lt;!--
## What&#39;s next
--&gt;

&lt;h2 id=&#34;后续计划&#34;&gt;后续计划&lt;/h2&gt;

&lt;!--
We built the kubernetes-MIPS component manually and finished the conformance test, which verified the feasibility of Kubernetes On the MIPS platform and greatly enhanced our confidence in promoting the support of the MIPS architecture by Kubernetes.
--&gt;

&lt;p&gt;我们手动构建了 K8S-MIPS 组件以及执行了 E2E 测试，验证了 Kubernetes on MIPS 的可行性，极大的增强了我们对于推进 Kubernetes 支持 MIPS 架构的信心。&lt;/p&gt;

&lt;!--
In the future, we plan to actively contribute our experience and achievements to the community, submit PR, and patch for MIPS. We hope that more developers and companies in the community join us and promote Kubernetes on MIPS.
--&gt;

&lt;p&gt;后续，我们将积极地向社区贡献我们的工作经验以及成果，提交 PR 以及 Patch For MIPS 等， 希望能够有更多的来自社区的力量加入进来，共同推进 Kubernetes for MIPS 的进程。&lt;/p&gt;

&lt;!--
Contribution plan：
--&gt;

&lt;p&gt;后续开源贡献计划：&lt;/p&gt;

&lt;!--
- contribute the source of e2e test images for MIPS
- contribute the source of hyperkube for MIPS
- contribute the source of deploy tools like kubeadm for MIPS
--&gt;

&lt;ul&gt;
&lt;li&gt;贡献构建 E2E 测试镜像代码&lt;/li&gt;
&lt;li&gt;贡献构建 MIPS 版本 hyperkube 代码&lt;/li&gt;
&lt;li&gt;贡献构建 MIPS 版本 kubeadm 等集群部署工具&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

      </description>
    </item>
    
  </channel>
</rss>
